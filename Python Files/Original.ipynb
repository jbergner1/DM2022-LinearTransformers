{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from fast_transformers.masking import LengthMask, TriangularCausalMask\n",
    "from fast_transformers.builders import TransformerEncoderBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochStats(object):\n",
    "    def __init__(self, metric_names=[], freq=1, out=sys.stdout):\n",
    "        self._start = time.time()\n",
    "        self._samples = 0\n",
    "        self._loss = 0\n",
    "        self._metrics = [0]*len(metric_names)\n",
    "        self._metric_names = metric_names\n",
    "        self._out = out\n",
    "        self._freq = freq\n",
    "        self._max_line = 0\n",
    "\n",
    "    def update(self, n_samples, loss, metrics=[]):\n",
    "        self._samples += n_samples\n",
    "        self._loss += loss*n_samples\n",
    "        for i, m in enumerate(metrics):\n",
    "            self._metrics[i] += m*n_samples\n",
    "\n",
    "    def _get_progress_text(self):\n",
    "        time_per_sample = (time.time()-self._start) / self._samples\n",
    "        loss = self._loss / self._samples\n",
    "        metrics = [\n",
    "            m/self._samples\n",
    "            for m in self._metrics\n",
    "        ]\n",
    "        text = \"Loss: {} \".format(loss)\n",
    "        text += \" \".join(\n",
    "            \"{}: {}\".format(mn, m)\n",
    "            for mn, m in zip(self._metric_names, metrics)\n",
    "        )\n",
    "        if self._out.isatty():\n",
    "            to_add = \" [{} sec/sample]\".format(time_per_sample)\n",
    "            if len(text) + len(to_add) > self._max_line:\n",
    "                self._max_line = len(text) + len(to_add)\n",
    "            text += \" \" * (self._max_line-len(text)-len(to_add)) + to_add\n",
    "        else:\n",
    "            text += \" time: {}\".format(time_per_sample)\n",
    "        return text\n",
    "\n",
    "    def progress(self):\n",
    "        if self._samples < self._freq:\n",
    "            return\n",
    "        text = self._get_progress_text()\n",
    "        if self._out.isatty():\n",
    "            print(\"\\r\" + text, end=\"\", file=self._out)\n",
    "        else:\n",
    "            print(text, file=self._out, flush=True)\n",
    "        self._loss = 0\n",
    "        self._samples = 0\n",
    "        self._last_progress = 0\n",
    "        for i in range(len(self._metrics)):\n",
    "            self._metrics[i] = 0\n",
    "        self._start = time.time()\n",
    "\n",
    "    def finalize(self):\n",
    "        self._freq = 1\n",
    "        self.progress()\n",
    "        if self._out.isatty():\n",
    "            print(\"\", file=self._out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(saved_file, model, optimizer, device):\n",
    "    data = torch.load(saved_file, map_location=device)\n",
    "    model.load_state_dict(data[\"model_state\"])\n",
    "    optimizer.load_state_dict(data[\"optimizer_state\"])\n",
    "    epoch = data[\"epoch\"]\n",
    "\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(save_file, model, optimizer, epoch):\n",
    "    torch.save(\n",
    "        dict(\n",
    "            model_state=model.state_dict(),\n",
    "            optimizer_state=optimizer.state_dict(),\n",
    "            epoch=epoch\n",
    "        ),\n",
    "        save_file.format(epoch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_transformer_arguments(args):\n",
    "    print((\n",
    "        \"Transformer Config:\\n\"\n",
    "        \"    Attention type: {attention_type}\\n\"\n",
    "        \"    Number of layers: {n_layers}\\n\"\n",
    "        \"    Number of heads: {n_heads}\\n\"\n",
    "        \"    Key/Query/Value dimension: {d_query}\\n\"\n",
    "        \"    Transformer layer dropout: {dropout}\\n\"\n",
    "        \"    Softmax temperature: {softmax_temp}\\n\"\n",
    "        \"    Attention dropout: {attention_dropout}\\n\"\n",
    "        \"    Number of hashing planes: {bits}\\n\"\n",
    "        \"    Chunk Size: {chunk_size}\\n\"\n",
    "        \"    Rounds: {rounds}\\n\"\n",
    "        \"    Masked: {masked}\"\n",
    "    ).format(**vars(args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transformer_arguments(parser):\n",
    "    parser.add_argument(\n",
    "        \"--attention_type\",\n",
    "        type=str,\n",
    "        choices=[\"full\", \"causal-linear\", \"reformer\"],\n",
    "        default=\"causal-linear\",\n",
    "        help=\"Attention model to be used\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_layers\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of self-attention layers\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_heads\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of attention heads\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--d_query\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Dimension of the query, key, and value embedding\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Dropout to be used for transformer layers\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--softmax_temp\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "        help=(\"Softmax temperature to be used for training \"\n",
    "              \"(default: 1/sqrt(d_query))\")\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--attention_dropout\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Dropout to be used for attention layers\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--bits\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Number of planes to use for hashing for reformer\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chunk_size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Number of queries in each block for reformer\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--rounds\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of rounds of hashing for reformer\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--unmasked_reformer\",\n",
    "        action=\"store_false\",\n",
    "        dest=\"masked\",\n",
    "        help=\"If set the query can attend to itsself for reformer\"\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(params, args):\n",
    "    if args.optimizer == \"adam\":\n",
    "        return torch.optim.Adam(params, lr=args.lr)\n",
    "    elif args.optimizer == \"radam\":\n",
    "        return RAdam(params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        raise RuntimeError(\"Optimizer {} not available\".format(args.optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_optimizer_arguments(parser):\n",
    "    parser.add_argument(\n",
    "        \"--optimizer\",\n",
    "        choices=[\"radam\", \"adam\"],\n",
    "        default=\"radam\",\n",
    "        help=\"Choose the optimizer\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "        help=\"Set the learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\",\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "        help=\"Set the weight decay\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyTask(IterableDataset):\n",
    "    def __init__(self, max_sequence, n_classes):\n",
    "        self._max_sequence = max_sequence\n",
    "        self._n_classes = n_classes\n",
    "        self._i = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Make some local copies\n",
    "        max_seq = self._max_sequence\n",
    "        n_classes = self._n_classes\n",
    "\n",
    "        # Generate the random sequence\n",
    "        n = torch.randint(max_seq//4, (max_seq-1)//2, tuple())\n",
    "        random_sequence = (torch.rand(n)*n_classes).long() + 1\n",
    "\n",
    "        # Generate the input, target and loss mask\n",
    "        x = torch.zeros(max_seq, dtype=torch.long)\n",
    "        y = torch.zeros(max_seq, dtype=torch.long)\n",
    "        mask = torch.zeros(max_seq)\n",
    "        x[:n] = random_sequence\n",
    "        x[n+1:2*n+1] = random_sequence\n",
    "        y[:-1] = x[1:]\n",
    "        mask[n-1:2*n] = 1\n",
    "\n",
    "        return x, y, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos_embedding =  self.pe[:, :x.size(1), :]\n",
    "        pos_embedding = torch.repeat_interleave(pos_embedding, x.shape[0], dim=0)\n",
    "        x =  torch.cat([x, pos_embedding], dim=2)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePredictor(torch.nn.Module):\n",
    "    def __init__(self, d_model, sequence_length, n_classes,\n",
    "                 attention_type=\"full\", n_layers=4, n_heads=4,\n",
    "                 d_query=32, dropout=0.1, softmax_temp=None,\n",
    "                 attention_dropout=0.1,\n",
    "                 bits=32, rounds=4,\n",
    "                 chunk_size=32, masked=True):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "\n",
    "        self.pos_embedding = PositionalEncoding(\n",
    "            d_model//2,\n",
    "            max_len=sequence_length\n",
    "        )\n",
    "        self.value_embedding = torch.nn.Embedding(\n",
    "            n_classes+1,\n",
    "            d_model//2\n",
    "        )\n",
    "        self.builder_dict = OrderedDict({\n",
    "            \"attention_type\": attention_type,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"n_heads\": n_heads,\n",
    "            \"feed_forward_dimensions\": n_heads*d_query*4,\n",
    "            \"query_dimensions\": d_query,\n",
    "            \"value_dimensions\": d_query,\n",
    "            \"dropout\": dropout,\n",
    "            \"softmax_temp\": softmax_temp,\n",
    "            \"attention_dropout\": attention_dropout,\n",
    "            \"bits\": bits,\n",
    "            \"rounds\": rounds,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"masked\": masked\n",
    "        })\n",
    "\n",
    "        self.transformer = TransformerEncoderBuilder.from_dictionary(\n",
    "            self.builder_dict,\n",
    "            strict=True\n",
    "        ).get()\n",
    "\n",
    "        hidden_size = n_heads*d_query\n",
    "        self.predictor = torch.nn.Linear(\n",
    "            hidden_size,\n",
    "            n_classes+1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.value_embedding(x).transpose(1, 0)\n",
    "        x = self.pos_embedding(x)\n",
    "        triangular_mask = TriangularCausalMask(x.shape[1], device=x.device) \n",
    "        y_hat = self.transformer(x, attn_mask=triangular_mask)\n",
    "        y_hat = self.predictor(y_hat)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat, loss_mask):\n",
    "    y_hat = y_hat.transpose(1, 0).contiguous()\n",
    "    L, N, C = y_hat.shape\n",
    "    l = torch.nn.functional.cross_entropy(\n",
    "        y_hat.view(L*N, C),\n",
    "        y.contiguous().view(L*N),\n",
    "        reduction=\"none\"\n",
    "    ).view(L, N)\n",
    "    # this means longer sequences have higher weight but it sounds ok\n",
    "    l = (loss_mask * l).mean() / loss_mask.mean()\n",
    "    accuracy = ((y == y_hat.argmax(dim=-1)).float() * loss_mask).mean() / loss_mask.mean()\n",
    "\n",
    "    return l, accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    stats = EpochStats([\"accuracy\"])\n",
    "    for i, (x, y, m) in zip(range(100), dataloader):\n",
    "        x = x.to(device).t()\n",
    "        y = y.to(device).t()\n",
    "        m = m.to(device).t()\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        l, acc = loss(y, y_hat, m)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        stats.update(x.shape[1], l.item(), [acc])\n",
    "        stats.progress()\n",
    "    stats.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, m) in zip(range(20), dataloader):\n",
    "            x = x.to(device).t()\n",
    "            y = y.to(device).t()\n",
    "            m = m.to(device).t()\n",
    "            y_hat = model(x)\n",
    "            l, acc = loss(y, y_hat, m)\n",
    "            total_loss += x.shape[1] * l.item()\n",
    "            total_acc += x.shape[1] * acc\n",
    "            total_samples += x.shape[1]\n",
    "    print(\n",
    "        \"Testing =>\",\n",
    "        \"Loss:\",\n",
    "        total_loss/total_samples,\n",
    "        \"Accuracy:\",\n",
    "        total_acc/total_samples\n",
    "    )\n",
    "\n",
    "    return total_loss/total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--optimizer {radam,adam}] [--lr LR]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--attention_type {full,causal-linear,reformer}]\n",
      "                             [--n_layers N_LAYERS] [--n_heads N_HEADS]\n",
      "                             [--d_query D_QUERY] [--dropout DROPOUT]\n",
      "                             [--softmax_temp SOFTMAX_TEMP]\n",
      "                             [--attention_dropout ATTENTION_DROPOUT]\n",
      "                             [--bits BITS] [--chunk_size CHUNK_SIZE]\n",
      "                             [--rounds ROUNDS] [--unmasked_reformer]\n",
      "                             [--sequence_length SEQUENCE_LENGTH]\n",
      "                             [--n_classes N_CLASSES] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--reduce_lr_at REDUCE_LR_AT] [--save_to SAVE_TO]\n",
      "                             [--continue_from CONTINUE_FROM]\n",
      "                             [--save_frequency SAVE_FREQUENCY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/johannes/Library/Jupyter/runtime/kernel-caa12e36-f829-4ca3-849a-68d024f3d26e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train a transformer for a copy task\"\n",
    "    )\n",
    "\n",
    "    add_optimizer_arguments(parser)\n",
    "    add_transformer_arguments(parser)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--sequence_length\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Set the maximum sequence length\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_classes\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Set the number of classes\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"How many epochs to train for\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        help=\"How many samples to use together\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reduce_lr_at\",\n",
    "        type=int,\n",
    "        default=30,\n",
    "        help=\"At this epoch divide the lr by 10\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--save_to\",\n",
    "        default=None,\n",
    "        help=\"Set a file to save the models to.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--continue_from\",\n",
    "        default=None,\n",
    "        help=\"Load the model from a file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_frequency\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Save every that many epochs\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(argv)\n",
    "    print_transformer_arguments(args)\n",
    "\n",
    "    # Make the dataset and the model\n",
    "    train_set = CopyTask(args.sequence_length, args.n_classes)\n",
    "    test_set = CopyTask(args.sequence_length, args.n_classes)\n",
    "    model = SequencePredictor(\n",
    "        args.d_query*args.n_heads, args.sequence_length, args.n_classes,\n",
    "        attention_type=args.attention_type,\n",
    "        n_layers=args.n_layers,\n",
    "        n_heads=args.n_heads,\n",
    "        d_query=args.d_query,\n",
    "        dropout=args.dropout,\n",
    "        softmax_temp=None,\n",
    "        attention_dropout=args.attention_dropout,\n",
    "        bits=args.bits,\n",
    "        rounds=args.rounds,\n",
    "        chunk_size=args.chunk_size,\n",
    "        masked=args.masked\n",
    "    )\n",
    "\n",
    "    # Choose a device and move everything there\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Running on {}\".format(device))\n",
    "    model.to(device)\n",
    "    # Start training\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=device==\"cuda\"\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=device==\"cuda\"\n",
    "    )\n",
    "    optimizer = get_optimizer(model.parameters(), args)\n",
    "    start_epoch = 1\n",
    "    if args.continue_from:\n",
    "        start_epoch = load_model(\n",
    "            args.continue_from,\n",
    "            model,\n",
    "            optimizer,\n",
    "            device\n",
    "        )\n",
    "    lr_schedule = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda e: 1. if e < args.reduce_lr_at else 0.1\n",
    "    )\n",
    "    for e in range(start_epoch, args.epochs+1):\n",
    "        train(model, optimizer, train_loader, device)\n",
    "        evaluate(model, test_loader, device)\n",
    "        if (e % args.save_frequency) == 0 and args.save_to:\n",
    "            save_model(args.save_to, model, optimizer, e)\n",
    "        lr_schedule.step()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
