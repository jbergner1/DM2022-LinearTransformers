
@article{katharopoulos_transformers_2020,
	title = {Transformers are {RNNs}: Fast Autoregressive Transformers with Linear Attention},
	url = {http://arxiv.org/abs/2006.16236},
	shorttitle = {Transformers are {RNNs}},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	journaltitle = {{arXiv}:2006.16236 [cs, stat]},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran√ßois},
	urldate = {2022-03-27},
	date = {2020-08-31},
	eprinttype = {arxiv},
	eprint = {2006.16236},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/johannes/Zotero/storage/EG7RS9UT/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf:application/pdf;arXiv.org Snapshot:/Users/johannes/Zotero/storage/U7TMPS4S/2006.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2022-03-27},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/johannes/Zotero/storage/EYDEVMA7/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/johannes/Zotero/storage/4NRKFCTN/1706.html:text/html},
}

@article{choromanski_rethinking_2021,
	title = {Rethinking Attention with Performers},
	url = {http://arxiv.org/abs/2009.14794},
	abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach ({FAVOR}+), which may be of independent interest for scalable kernel methods. {FAVOR}+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
	journaltitle = {{arXiv}:2009.14794 [cs, stat]},
	author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
	urldate = {2022-03-27},
	date = {2021-03-09},
	eprinttype = {arxiv},
	eprint = {2009.14794},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/johannes/Zotero/storage/D7XKYEDG/Choromanski et al. - 2021 - Rethinking Attention with Performers.pdf:application/pdf;arXiv.org Snapshot:/Users/johannes/Zotero/storage/MSM9C58K/2009.html:text/html},
}

@article{wang_linformer_2020,
	title = {Linformer: Self-Attention with Linear Complexity},
	url = {http://arxiv.org/abs/2006.04768},
	shorttitle = {Linformer},
	abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n{\textasciicircum}2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n{\textasciicircum}2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the {\textbackslash}textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
	journaltitle = {{arXiv}:2006.04768 [cs, stat]},
	author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	urldate = {2022-03-27},
	date = {2020-06-14},
	eprinttype = {arxiv},
	eprint = {2006.04768},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/johannes/Zotero/storage/V3Z5VJHX/Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf:application/pdf;arXiv.org Snapshot:/Users/johannes/Zotero/storage/E3MSIMGP/2006.html:text/html},
}

@article{zaheer_big_2021,
	title = {Big Bird: Transformers for Longer Sequences},
	url = {http://arxiv.org/abs/2007.14062},
	shorttitle = {Big Bird},
	abstract = {Transformers-based models, such as {BERT}, have been one of the most successful deep learning models for {NLP}. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, {BigBird}, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that {BigBird} is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as {CLS}), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, {BigBird} drastically improves performance on various {NLP} tasks such as question answering and summarization. We also propose novel applications to genomics data.},
	journaltitle = {{arXiv}:2007.14062 [cs, stat]},
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	urldate = {2022-03-27},
	date = {2021-01-08},
	eprinttype = {arxiv},
	eprint = {2007.14062},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/johannes/Zotero/storage/LGYYZDJ9/Zaheer et al. - 2021 - Big Bird Transformers for Longer Sequences.pdf:application/pdf;arXiv.org Snapshot:/Users/johannes/Zotero/storage/67VPLGQP/2007.html:text/html},
}

@article{tay_long_2020,
	title = {Long Range Arena: A Benchmark for Efficient Transformers},
	url = {http://arxiv.org/abs/2011.04006},
	shorttitle = {Long Range Arena},
	abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, {LRA}, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from \$1K\$ to \$16K\$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. {LRA} paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.},
	journaltitle = {{arXiv}:2011.04006 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
	urldate = {2022-03-27},
	date = {2020-11-08},
	eprinttype = {arxiv},
	eprint = {2011.04006},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/johannes/Zotero/storage/WNTRLX2N/Tay et al. - 2020 - Long Range Arena A Benchmark for Efficient Transf.pdf:application/pdf;arXiv.org Snapshot:/Users/johannes/Zotero/storage/3C2YD837/2011.html:text/html},
}