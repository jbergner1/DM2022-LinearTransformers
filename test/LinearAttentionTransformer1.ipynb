{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\tusing Flux\n",
    "\tusing Flux: onehot\n",
    "\tusing Flux: gradient\n",
    "\tusing Flux.Optimise: update!\n",
    "\tusing Flux: onecold\n",
    "\tusing CUDA\n",
    "\tusing Transformers\n",
    "\tusing Transformers.Basic \n",
    "    using Transformers.Datasets: batched\n",
    "\tusing Flux: @functor\n",
    "\tusing ..Transformers: Abstract3DTensor, Container, epsilon, batchedmul, batched_triu!\n",
    "\tusing Einsum\n",
    "end\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary{String}(13, unk=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Labeling: vielleicht ändern\n",
    "\n",
    "begin\n",
    "\tlabels = map(string, 1:10)\n",
    "\tstartsym = \"11\"\n",
    "\tendsym = \"12\"\n",
    "\tunksym = \"0\"\n",
    "\tlabels = [unksym, startsym, endsym, labels...]\n",
    "\tvocab = Vocabulary(labels, unksym)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for generate training datas \n",
    "\n",
    "#nichts ändern\n",
    "sample_data() = (d = map(string, rand(1:10, 10)); (d,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"2\", \"5\", \"8\", \"8\", \"10\", \"4\", \"4\", \"2\", \"9\", \"1\"], [\"2\", \"5\", \"8\", \"8\", \"10\", \"4\", \"4\", \"2\", \"9\", \"1\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "\n",
    "sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for adding start & end symbol\n",
    "\n",
    "#nichts ändern\n",
    "preprocess(x) = [startsym, x..., endsym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_ex = preprocess.(sample_data()) = ([\"11\", \"7\", \"4\", \"2\", \"10\", \"7\", \"8\", \"4\", \"9\", \"7\", \"9\", \"12\"], [\"11\", \"7\", \"4\", \"2\", \"10\", \"7\", \"8\", \"4\", \"9\", \"7\", \"9\", \"12\"])\n",
      "encoded_sample_ex = vocab(sample_ex[1]) = [2, 10, 7, 5, 13, 10, 11, 7, 12, 10, 12, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       " 10\n",
       "  7\n",
       "  5\n",
       " 13\n",
       " 10\n",
       " 11\n",
       "  7\n",
       " 12\n",
       " 10\n",
       " 12\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "\n",
    "begin\n",
    "    @show sample_ex = preprocess.(sample_data())\n",
    "    @show encoded_sample_ex = vocab(sample_ex[1]) #use Vocabulary to encode the training data\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"11\", \"5\", \"10\", \"2\", \"2\", \"6\", \"9\", \"2\", \"3\", \"7\", \"9\", \"12\"], [\"11\", \"5\", \"10\", \"2\", \"2\", \"6\", \"9\", \"2\", \"3\", \"7\", \"9\", \"12\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "sample = preprocess.(sample_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       "  8\n",
       " 13\n",
       "  5\n",
       "  5\n",
       "  9\n",
       " 12\n",
       "  5\n",
       "  6\n",
       " 10\n",
       " 12\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "encoded_sample = vocab(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embed(512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a Word embedding layer which turn word index to word vector\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "embed = Embed(512, length(vocab)) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionEmbedding(512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a position embedding layer metioned above\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "pe = PositionEmbedding(512) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wrapper for get embedding\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "function embedding(x)\n",
    "    we = embed(x, inv(sqrt(512)))\n",
    "    e = we .+ pe(we)\n",
    "    return e\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractAttention end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MultiheadAttention_1{Q<:Dense, K<:Dense, V<:Dense, O<:Dense, DP<:Dropout} <: AbstractAttention\n",
    "    head::Int\n",
    "    future::Bool\n",
    "    iqproj::Q\n",
    "    ikproj::K\n",
    "    ivproj::V\n",
    "    oproj::O\n",
    "    drop::DP\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.functor(la::LinearAttention) = (la.iqproj, la.ikproj, la.ivproj, la.oproj), m -> LinearAttention(la.head, la.future, m..., la.drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearAttention(head::Int,\n",
    "                   is::Int,\n",
    "                   hs::Int,\n",
    "                   os::Int;\n",
    "                   future::Bool=true, pdrop = 0.1) = LinearAttention(head,\n",
    "                                                                        future,\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(hs*head, os),\n",
    "                                                                        Dropout(pdrop),\n",
    "                                                                        )\n",
    "\n",
    "\n",
    "function Base.show(io::IO, la::LinearAttention)\n",
    "    hs = div(size(la.iqproj.weight)[1], la.head)\n",
    "    is = size(la.iqproj.weight)[end]\n",
    "    os = size(la.oproj.weight)[1]\n",
    "\n",
    "    print(io, \"LinearAttention(\")\n",
    "    print(io, \"head=$(la.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"$(is)=>$(os)\")\n",
    "\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(la.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nelu (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# of type float (to allow for integer inputs)\n",
    "oftf(x, y) = oftype(float(x), y)\n",
    "nelu(x, α=1) = ifelse(x ≥ 0, float(x)+1, @fastmath oftf(x, α) * (exp(x) - 1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splitHeads (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function splitHeads(x, batch_size, head, depth)\n",
    "    x = reshape(x, (batch_size, :, head, depth))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (la::LinearAttention)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: Abstract3DTensor{T},\n",
    "                         A2 <: Abstract3DTensor{T},\n",
    "                         A3 <: Abstract3DTensor{T}}\n",
    "\n",
    "#size(query) == (dims, {q,k}_seq_len, batch) == size(key) == size(value)\n",
    "#size(ipq) == (hs, q_seq_len, head * batch)\n",
    "#size(score) == (k_seq_len, q_seq_len, batch)\n",
    "#=\n",
    "dk = size(key, 1)\n",
    "score = batchedmul(key, query; transA = true) \n",
    "score = score ./ convert(T, sqrt(dk))\n",
    "\n",
    "score = apply_mask1(score, mask, future)\n",
    "score = softmax(score; dims=1)\n",
    "dropout !== nothing && (score = dropout(score))\n",
    "batchedmul(value, score) #size(return) == (dims, q_seq_len, batch)\n",
    "=#\n",
    "#size(query) == (batch, {q,k}_seq_len, dims) == size(key) == size(value)\n",
    "query = permutedims(query,(3,2,1))\n",
    "key = permutedims(key,(3,2,1))\n",
    "value = permutedims(value,(3,2,1))\n",
    "\n",
    "qs = size(query)\n",
    "ks = size(key)\n",
    "vs = size(value)\n",
    "\n",
    "#depth = div(la.size, la.head)\n",
    "depth = div(512, 8)   \n",
    "\n",
    "#query = nelu(splitHeads(query, qs[1], la.head, depth))\n",
    "#key = nelu(splitHeads(key, qs[1], la.head, depth))\n",
    "#value = splitHeads(value, qs[1], la.head, depth)\n",
    "\n",
    "query = nelu(splitHeads(query, qs[1], 8, depth))\n",
    "key = nelu(splitHeads(key, ks[1], 8, depth))\n",
    "value = splitHeads(value, vs[1], 8, depth)\n",
    "# size(k_v) == (batch_size, depth_k, depth_v, seq_len_v)\n",
    "@einsum k_v[m,d,e,h] := key[m,j,h,d]*value[m,j,h,e]\n",
    "\n",
    "k_reduced = sum(key, dims=2) .+ 1e-8\n",
    "\n",
    "@einsum z_1[m,l,h] := query[m,l,h,d]*k_reduced[m,h,d]\n",
    "# size(z) == (batch_size, num_heads, seq_len_q)\n",
    "z = 1/z_1 # ...\n",
    "# size(output) == (batch_size,len_q, heads, depth_v)\n",
    "@einsum output[m,l,h,e] := query[m,l,h,d]*k_v[m,d,e,h]*z[m,l,h]\n",
    "\n",
    "#output = reshape(output, (qs[1], qs[2], la.head*depth))\n",
    "# size(output) == (batch_size,len_q, d_model)\n",
    "output = reshape(output, (qs[1], :, 8*depth))\n",
    "\n",
    "output = permutedims(output,(3,2,1))\n",
    "\n",
    "out = @toNd la.oproj(output)\n",
    "#(dims, q_seq_len, batch)\n",
    "end\n",
    "\n",
    "function (la::LinearAttention)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: AbstractMatrix{T},\n",
    "                         A2 <: AbstractMatrix{T},\n",
    "                         A3 <: AbstractMatrix{T}}\n",
    "\n",
    "#size(query) == (dims, {q,k}_seq_len, batch) == size(key) == size(value)\n",
    "#size(ipq) == (hs, q_seq_len, head * batch)\n",
    "#size(score) == (k_seq_len, q_seq_len, batch)\n",
    "#=\n",
    "dk = size(key, 1)\n",
    "score = batchedmul(key, query; transA = true) \n",
    "score = score ./ convert(T, sqrt(dk))\n",
    "\n",
    "score = apply_mask1(score, mask, future)\n",
    "score = softmax(score; dims=1)\n",
    "dropout !== nothing && (score = dropout(score))\n",
    "batchedmul(value, score) #size(return) == (dims, q_seq_len, batch)\n",
    "=#\n",
    "#size(query) == (batch, {q,k}_seq_len, dims) == size(key) == size(value)\n",
    "query = permutedims(query,(3,2,1))\n",
    "key = permutedims(key,(3,2,1))\n",
    "value = permutedims(value,(3,2,1))\n",
    "\n",
    "qs = size(query)\n",
    "ks = size(key)\n",
    "vs = size(value)\n",
    "\n",
    "#depth = div(la.size, la.head)\n",
    "depth = div(512, 8)   \n",
    "\n",
    "#query = nelu(splitHeads(query, qs[1], la.head, depth))\n",
    "#key = nelu(splitHeads(key, qs[1], la.head, depth))\n",
    "#value = splitHeads(value, qs[1], la.head, depth)\n",
    "\n",
    "query = nelu(splitHeads(query, qs[1], 8, depth))\n",
    "key = nelu(splitHeads(key, ks[1], 8, depth))\n",
    "value = splitHeads(value, vs[1], 8, depth)\n",
    "# size(k_v) == (batch_size, depth_k, depth_v, seq_len_v)\n",
    "@einsum k_v[m,d,e,h] := key[m,j,h,d]*value[m,j,h,e]\n",
    "\n",
    "k_reduced = sum(key, dims=2) .+ 1e-8\n",
    "\n",
    "@einsum z_1[m,l,h] := query[m,l,h,d]*k_reduced[m,h,d]\n",
    "# size(z) == (batch_size, num_heads, seq_len_q)\n",
    "z = 1/z_1 # ...\n",
    "# size(output) == (batch_size,len_q, heads, depth_v)\n",
    "@einsum output[m,l,h,e] := query[m,l,h,d]*k_v[m,d,e,h]*z[m,l,h]\n",
    "\n",
    "#output = reshape(output, (qs[1], qs[2], la.head*depth))\n",
    "# size(output) == (batch_size,len_q, d_model)\n",
    "output = reshape(output, (qs[1], :, 8*depth))\n",
    "\n",
    "output = permutedims(output,(3,2,1))\n",
    "#(dims, q_seq_len, batch)\n",
    "\n",
    "la.oproj(output)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractTransformer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PwFFN{Di<:Dense, Do<:Dense}\n",
    "    din::Di\n",
    "    dout::Do\n",
    "end\n",
    "\n",
    "@functor PwFFN\n",
    "\n",
    "\n",
    "\"just a wrapper for two dense layer.\"\n",
    "PwFFN(size::Int, h::Int, act = nelu) = PwFFN(  #relu vielleicht auf elu + 1 oder nur elu ändern\n",
    "    Dense(size, h, act),\n",
    "    Dense(h, size)\n",
    ")\n",
    "\n",
    "function (pw::PwFFN)(x::AbstractMatrix)\n",
    "  # size(x) == (dims, seq_len)\n",
    "  pw.dout(pw.din(x))\n",
    "end\n",
    "\n",
    "function (pw::PwFFN)(x::A) where {T, N, A<:AbstractArray{T, N}}\n",
    "  new_x = reshape(x, size(x, 1), :)\n",
    "  y = pw(new_x)\n",
    "  return reshape(y, Base.setindex(size(x), size(y, 1), 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "invalid redefinition of constant Transformer1",
     "output_type": "error",
     "traceback": [
      "invalid redefinition of constant Transformer1\n",
      "\n",
      "Stacktrace:\n",
      "  [1] top-level scope\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:1\n",
      "  [2] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      "  [4] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      "  [5] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      "  [6] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      "  [7] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      "  [8] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      "  [9] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [10] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [11] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [12] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [13] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [14] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "struct Transformer1{MA<:LinearAttention, LA<:LayerNorm, P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "    la::MA\n",
    "    mhn::LA\n",
    "    pw::P\n",
    "    pwn::LP\n",
    "    drop::DP\n",
    "end\n",
    "\n",
    "@functor Transformer1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Transformer(size::Int, head::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)\n",
    "    Transformer(size::Int, head::Int, hs::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)  \n",
    "\n",
    "Transformer layer.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "When `future` is `false`, the k-th token can't see the j-th tokens where j > k. `pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "\n",
    "########\n",
    "#ganze Funktion von Transformer ändern\n",
    "########\n",
    "function Transformer1(size::Int, head::Int, ps::Int; future::Bool = true, act = nelu, pdrop = 0.1)  #relu vielleicht wieder auf elu ändern\n",
    "    rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "    Transformer1(size, head, div(size, head), ps;future=future, act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "Transformer1(size::Int, head::Int, hs::Int, ps::Int; future::Bool = true, act = nelu, pdrop = 0.1) = Transformer1(\n",
    "    LinearAttention(head, size, hs, size; future=future, pdrop=pdrop),\n",
    "    LayerNorm(size),\n",
    "    PwFFN(size, ps, act),\n",
    "    LayerNorm(size),\n",
    "    Dropout(pdrop),     #braucht man das? vielleicht nicht\n",
    ")\n",
    "\n",
    "function (t::Transformer1)(x::A, mask=nothing) where {T, N, A<:AbstractArray{T, N}}\n",
    "    dropout = t.drop\n",
    "    a = t.la(x, x, x; mask=mask)\n",
    "    a = dropout(a)\n",
    "    res_a = x + a\n",
    "    res_a = t.mhn(res_a)\n",
    "    pwffn = t.pw(res_a)\n",
    "    pwffn = dropout(pwffn)\n",
    "    res_pwffn = res_a + pwffn\n",
    "    res_pwffn = t.pwn(res_pwffn)\n",
    "    res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, t::Transformer1) #zum visualisieren --> nice to have\n",
    "    hs = div(size(t.la.iqproj.weight)[1], t.la.head)\n",
    "    h, ps = size(t.pw.dout.weight)\n",
    "\n",
    "    print(io, \"Transformer(\")\n",
    "    print(io, \"head=$(t.la.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"pwffn_size=$(ps), \")\n",
    "    print(io, \"size=$(h)\")\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(t.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "invalid redefinition of constant TransformerDecoder1",
     "output_type": "error",
     "traceback": [
      "invalid redefinition of constant TransformerDecoder1\n",
      "\n",
      "Stacktrace:\n",
      "  [1] top-level scope\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:4\n",
      "  [2] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      "  [4] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      "  [5] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      "  [6] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      "  [7] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      "  [8] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      "  [9] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [10] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [11] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [12] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [13] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [14] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "###########\n",
    "    #auch diese Funktion ändern\n",
    "    ###############\n",
    "    struct TransformerDecoder1{MA<:LinearAttention, LA<:LayerNorm,\n",
    "        IMA<:LinearAttention, ILA<:LayerNorm,\n",
    "        P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "la::MA\n",
    "mhn::LA\n",
    "ila::IMA\n",
    "imhn::ILA\n",
    "pw::P\n",
    "pwn::LP\n",
    "drop::DP\n",
    "end\n",
    "\n",
    "@functor TransformerDecoder1\n",
    "\n",
    "\"\"\"\n",
    "TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "TransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "\n",
    "TransformerDecoder layer. Decode the value from a Encoder.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "`pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "function TransformerDecoder1(size::Int, head::Int, ps::Int; act = nelu, pdrop = 0.1)\n",
    "rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "TransformerDecoder1(size, head, div(size, head), ps; act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "TransformerDecoder1(size::Int, head::Int, hs::Int, ps::Int; act = nelu, pdrop = 0.1) = TransformerDecoder1(\n",
    "LinearAttention(head, size, hs, size; future=false, pdrop=pdrop),\n",
    "LayerNorm(size),\n",
    "LinearAttention(head, size, hs, size; future=true, pdrop=pdrop), #future = true --> Unterschied zu oben??\n",
    "LayerNorm(size),\n",
    "PwFFN(size, ps, act),\n",
    "LayerNorm(size),\n",
    "Dropout(pdrop),\n",
    ")\n",
    "\n",
    "function (td::TransformerDecoder1)(x::AbstractArray{T,N}, m, mask=nothing) where {T,N}\n",
    "dropout = td.drop\n",
    "a = td.la(x,x,x)\n",
    "a = dropout(a)\n",
    "res_a = x + a\n",
    "res_a = td.mhn(res_a)\n",
    "\n",
    "ia = td.ila(res_a, m, m, mask=mask)\n",
    "ia = dropout(ia)\n",
    "res_ia = res_a + ia\n",
    "res_ia = td.imhn(res_ia)\n",
    "\n",
    "pwffn = td.pw(res_ia)\n",
    "pwffn = dropout(pwffn)\n",
    "res_pwffn = res_ia + pwffn\n",
    "res_pwffn = td.pwn(res_pwffn)\n",
    "res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, td::TransformerDecoder1)\n",
    "hs = div(size(td.ila.iqproj.weight)[1], td.ila.head)\n",
    "h, ps = size(td.pw.dout.weight)\n",
    "\n",
    "print(io, \"TransformerDecoder(\")\n",
    "print(io, \"head=$(td.la.head), \")\n",
    "print(io, \"head_size=$(hs), \")\n",
    "print(io, \"pwffn_size=$(ps), \")\n",
    "print(io, \"size=$(h)\")\n",
    "if Flux.istraining()\n",
    "print(io, \", dropout=$(td.drop.p))\")\n",
    "else\n",
    "print(io, \")\")\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer\n",
    "\n",
    "#layer auf jeden Fall ändern\n",
    "encode_t1 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer decoder\n",
    "\n",
    "#layer auf jeden Fall ändern\n",
    "decode_t1 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positionwise(Dense(512, 13), logsoftmax)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define the layer to get the final output probabilities\n",
    "\n",
    "#ÄNDERN\n",
    "linear = Positionwise(Dense(512, length(vocab)), logsoftmax) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neue Layer einfügen\n",
    "function encoder_forward(x)\n",
    "    e = embedding(x)\n",
    "    t1 = encode_t1(e)\n",
    "    return t1\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neue Layer einfügen\n",
    "function decoder_forward(x, m)\n",
    "    e = embedding(x)\n",
    "    t1 = decode_t1(e, m)\n",
    "    p = linear(t4)\n",
    "    return p\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: no valid permutation of dimensions",
     "output_type": "error",
     "traceback": [
      "ArgumentError: no valid permutation of dimensions\n",
      "\n",
      "Stacktrace:\n",
      "  [1] permutedims(B::Matrix{Float32}, perm::Tuple{Int64, Int64, Int64})\n",
      "    @ Base ./multidimensional.jl:1491\n",
      "  [2] (::LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}})(query::Matrix{Float32}, key::Matrix{Float32}, value::Matrix{Float32}; mask::Nothing)\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:84\n",
      "  [3] (::Transformer1{LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, PwFFN{Dense{typeof(nelu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Dropout{Float64, Colon}})(x::Matrix{Float32}, mask::Nothing)\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:43\n",
      "  [4] (::Transformer1{LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, PwFFN{Dense{typeof(nelu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Dropout{Float64, Colon}})(x::Matrix{Float32})\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:42\n",
      "  [5] encoder_forward(x::Vector{Int64})\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:4\n",
      "  [6] top-level scope\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:2\n",
      "  [7] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      "  [9] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      " [10] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      " [11] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      " [12] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      " [13] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      " [14] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [15] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [16] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [17] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [18] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [19] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "#bleibt so\n",
    "enc = encoder_forward(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: enc not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: enc not defined\n",
      "\n",
      "Stacktrace:\n",
      "  [1] top-level scope\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:2\n",
      "  [2] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      "  [4] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      "  [5] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      "  [6] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      "  [7] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      "  [8] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      "  [9] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [10] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [11] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [12] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [13] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [14] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "#bleibt so\n",
    "probs = decoder_forward(encoded_sample, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smooth (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hier vielleicht etwas ändern --> herausfinden, was diese funktion macht\n",
    "function smooth(et)\n",
    "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
    "    p = sm .* (1 .+ -et)\n",
    "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
    "    label\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define loss function\n",
    "\n",
    "#ÄNDERN\n",
    "function loss(x, y)\n",
    "    label = onehot(vocab, y) #turn the index to one-hot encoding\n",
    "    label = smooth(label) #perform label smoothing\n",
    "    enc = encoder_forward(x)\n",
    "    probs = decoder_forward(y, enc)\n",
    "    l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])  #erstmal so lassen\n",
    "    return l\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.51304907 -0.6683715 … -0.80315036 0.5276025; -0.39588508 -0.55599225 … 0.14842303 -1.3252602; … ; -0.5312266 0.25819048 … 0.32759124 1.7346628; 0.7313382 -2.0601692 … -0.47881743 -1.3610063], Float32[0.5403023 -0.41614684 … 0.4000682 0.9873536; 0.8218562 0.9364147 … 0.37902638 0.97646356; … ; 1.0 1.0 … 0.9943822 0.99437124; 0.0001 0.0002 … 0.10212166 0.10222114], Float32[-0.046199363 0.045024894 … -0.063788846 -0.042573355; 0.056413334 0.04221744 … -0.011564555 0.06920331; … ; -0.066903815 -0.05477294 … 0.05219281 0.06633255; -0.02792442 0.05107262 … 0.06759633 0.0073322235], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.05767657 -0.027813094 … -0.05032909 0.06377225; -0.0108496975 0.064189985 … -0.0142692225 0.03808293; … ; -0.016464567 0.024370208 … 0.015878372 -0.0021333296; 0.06784048 0.024988431 … 0.028356856 -0.07620384], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0074377093 0.03191406 … 0.040185653 0.04981575; 0.06596873 0.039984245 … 0.061487142 0.057325732; … ; 0.040955115 0.014760133 … -0.044261456 0.048925236; 0.074031815 0.00051520095 … 0.04512996 0.05070486], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.048705667 0.008372681 … 0.05546732 -0.041912135; 0.011833944 -0.04265493 … 0.04910522 0.013602436; … ; -0.029047003 0.050131388 … -0.009921661 0.07522894; 0.06557343 0.060563356 … 0.011262259 0.073048666], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.032974347 0.017370893 … 0.0334183 0.029289484; -0.027498314 -0.032482468 … -0.033157375 -0.04680333; … ; -0.02220599 -0.008335937 … 0.011128434 0.0018879541; -0.04689478 0.047017373 … -0.045050997 0.027926814], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.01348846 -0.03581464 … 0.0070941723 0.03077484; -0.03724046 -0.0145618105 … -0.0037748464 -0.008336295; … ; -0.017560696 -0.005548935 … -0.03437579 0.035178814; 0.044240445 -0.010629791 … 0.04026154 0.02173642], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.065892264 -0.047913942 … 0.072615534 -0.007749768; 0.024854494 0.040836032 … 0.042549282 0.0072166272; … ; 0.075325914 -0.059583124 … 0.03844379 0.022745619; 0.0197528 -0.0493966 … -0.071169704 0.073677726], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.026086066 0.04496839 … -0.0061709136 -0.022046657; 0.00047808018 -0.016670719 … -0.042103887 0.0027797124; … ; 0.043807995 -0.005820274 … -0.025015423 -0.044071596; 0.0028838476 -0.053365745 … 0.06780375 0.068754464], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.05080394 -0.0053529614 … 0.063841805 0.06988382; -0.025894277 0.021351108 … 0.0544392 0.01547906; … ; 0.01161018 0.02269817 … 0.029763667 -0.051570408; -0.009476193 0.04130068 … -0.04153027 0.021478787], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.049043916 0.039688084 … -0.0147792585 -0.0198842; -0.0041180667 -0.07440251 … 0.06495501 -0.006113754; … ; 0.03389595 -0.033504322 … 0.060159974 0.020188266; -0.07132653 0.04007732 … 0.0212102 0.02455167], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.037784122 -0.061031964 … 0.013787785 -0.06838649; -0.052385386 -0.035393536 … -0.0047173095 0.040336143; … ; -0.03606912 -0.028759435 … 0.0013693066 0.045487132; 0.06974257 0.044279505 … -0.019640397 0.04537827], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.050460562 0.042140424 … 0.007238071 0.0428301; -0.073796846 0.01291499 … -0.075498305 0.008339357; … ; -0.0308828 -0.065644465 … -0.043836042 0.019667316; -0.01130876 -0.06179484 … -0.048395142 -0.057154253], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.041257538 -0.03283854 … -0.01570443 -0.063682996; 0.067010194 -0.029262135 … 0.010248574 -0.060870998; … ; -0.06780773 -0.02668301 … -0.04738007 -0.072033085; 0.036360905 0.011065906 … 0.0644436 -0.009544065], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.05887011 0.018865243 … -0.07524505 -0.062104397; 0.024171172 0.05606023 … 0.019665072 -0.05120869; … ; -0.007868905 0.033212483 … 0.01575345 0.04886512; 0.02465073 0.0522745 … 0.03146124 -0.002782158], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.020247234 0.017234104 … 0.011719531 0.027535977; -0.04137447 0.03777199 … -0.0048405435 -0.013268324; … ; 0.003141827 0.032758318 … 0.004749405 0.025215367; 0.03599717 -0.042774886 … -0.032035593 0.04013244], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.032750726 0.048189893 … 0.028062806 -0.043146644; 0.040589254 -0.047458302 … 0.032178268 -0.02704696; … ; -0.036240622 0.03844657 … 0.009096015 0.037512466; -0.038805563 -0.042733785 … 0.042420372 0.026629874], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.03317393 -0.010990332 … 0.07963777 0.048128568; -0.034535322 -0.084405586 … 0.09380366 -0.044065878; … ; -0.067204185 -0.07596508 … 0.01643712 0.033320412; 0.083790585 -0.051664826 … -0.04538185 -0.07409153], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collect all the parameters\n",
    "\n",
    "#Layer neu einfügen\n",
    "ps = params(embed, pe, encode_t1, decode_t1, linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = ADAM(1e-4) #bleibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define training loop\n",
    "\n",
    "#ÄNDERN siehe Paper Kapitel 4.1\n",
    "function train!()\n",
    "    @info \"start training\"\n",
    "    for i = 1:100\n",
    "      data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
    "      x, y = preprocess.(data[1]), preprocess.(data[2])\n",
    "      x, y = vocab(x), vocab(y) #encode the data\n",
    "      x, y = todevice(x, y) #move to gpu\n",
    "      grad = gradient(()->loss(x, y), ps)\n",
    "      if i % 8 == 0\n",
    "          l = loss(x, y)\n",
    "          println(\"loss = $l\")\n",
    "      end\n",
    "      update!(opt, ps, grad)\n",
    "    end\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching isless(::Int64, ::Array{Float32, 4})\nClosest candidates are:\n  isless(::Integer, !Matched::ForwardDiff.Dual{Ty, V, N} where {V, N}) where Ty at /Users/johannes/.julia/packages/ForwardDiff/PBzup/src/dual.jl:145\n  isless(::Real, !Matched::ColorTypes.AbstractGray{T} where T) at /Users/johannes/.julia/packages/ColorTypes/6m8P7/src/operations.jl:32\n  isless(::Real, !Matched::ForwardDiff.Dual{Ty, V, N} where {V, N}) where Ty at /Users/johannes/.julia/packages/ForwardDiff/PBzup/src/dual.jl:145\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching isless(::Int64, ::Array{Float32, 4})\n",
      "Closest candidates are:\n",
      "  isless(::Integer, !Matched::ForwardDiff.Dual{Ty, V, N} where {V, N}) where Ty at /Users/johannes/.julia/packages/ForwardDiff/PBzup/src/dual.jl:145\n",
      "  isless(::Real, !Matched::ColorTypes.AbstractGray{T} where T) at /Users/johannes/.julia/packages/ColorTypes/6m8P7/src/operations.jl:32\n",
      "  isless(::Real, !Matched::ForwardDiff.Dual{Ty, V, N} where {V, N}) where Ty at /Users/johannes/.julia/packages/ForwardDiff/PBzup/src/dual.jl:145\n",
      "  ...\n",
      "\n",
      "Stacktrace:\n",
      "  [1] <(x::Int64, y::Array{Float32, 4})\n",
      "    @ Base ./operators.jl:279\n",
      "  [2] <=(x::Int64, y::Array{Float32, 4})\n",
      "    @ Base ./operators.jl:328\n",
      "  [3] >=(x::Array{Float32, 4}, y::Int64)\n",
      "    @ Base ./operators.jl:352\n",
      "  [4] rrule(#unused#::typeof(>=), 330::Array{Float32, 4}, 331::Int64)\n",
      "    @ ChainRules ~/.julia/packages/ChainRules/K2jy1/src/rulesets/Base/nondiff.jl:15\n",
      "  [5] rrule(::Zygote.ZygoteRuleConfig{Zygote.Context}, ::Function, ::Array{Float32, 4}, ::Int64)\n",
      "    @ ChainRulesCore ~/.julia/packages/ChainRulesCore/IzITE/src/rules.jl:134\n",
      "  [6] chain_rrule\n",
      "    @ ~/.julia/packages/Zygote/cCyLF/src/compiler/chainrules.jl:216 [inlined]\n",
      "  [7] macro expansion\n",
      "    @ ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0 [inlined]\n",
      "  [8] _pullback(::Zygote.Context, ::typeof(>=), ::Array{Float32, 4}, ::Int64)\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:9\n",
      "  [9] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:3 [inlined]\n",
      " [10] _pullback(::Zygote.Context, ::typeof(nelu), ::Array{Float32, 4}, ::Int64)\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [11] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:3 [inlined]\n",
      " [12] _pullback(ctx::Zygote.Context, f::typeof(nelu), args::Array{Float32, 4})\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [13] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:38 [inlined]\n",
      " [14] _pullback(::Zygote.Context, ::var\"##_#176\", ::Nothing, ::LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}}, ::Array{Float32, 3}, ::Array{Float32, 3}, ::Array{Float32, 3})\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [15] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:23 [inlined]\n",
      " [16] _pullback(::Zygote.Context, ::Core.var\"#Any##kw\", ::NamedTuple{(:mask,), Tuple{Nothing}}, ::LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}}, ::Array{Float32, 3}, ::Array{Float32, 3}, ::Array{Float32, 3})\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [17] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:43 [inlined]\n",
      " [18] _pullback(::Zygote.Context, ::Transformer1{LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, PwFFN{Dense{typeof(nelu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Dropout{Float64, Colon}}, ::Array{Float32, 3}, ::Nothing)\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [19] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:42 [inlined]\n",
      " [20] _pullback(ctx::Zygote.Context, f::Transformer1{LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, PwFFN{Dense{typeof(nelu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Dropout{Float64, Colon}}, args::Array{Float32, 3})\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [21] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:4 [inlined]\n",
      " [22] _pullback(ctx::Zygote.Context, f::typeof(encoder_forward), args::Matrix{Int64})\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [23] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:7 [inlined]\n",
      " [24] _pullback(::Zygote.Context, ::typeof(loss), ::Matrix{Int64}, ::Matrix{Int64})\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [25] _pullback\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:11 [inlined]\n",
      " [26] _pullback(::Zygote.Context, ::var\"#183#185\")\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface2.jl:0\n",
      " [27] pullback(f::Function, ps::Zygote.Params)\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface.jl:352\n",
      " [28] gradient(f::Function, args::Zygote.Params)\n",
      "    @ Zygote ~/.julia/packages/Zygote/cCyLF/src/compiler/interface.jl:75\n",
      " [29] train!()\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:11\n",
      " [30] top-level scope\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:1\n",
      " [31] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      " [32] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      " [33] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      " [34] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      " [35] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      " [36] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      " [37] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      " [38] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [39] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [40] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [41] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [42] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [43] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: start training\n",
      "└ @ Main /Users/johannes/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "translate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#erst einmal so lassen, vielleicht einzelne Änderungen\n",
    "function translate(x)\n",
    "    ix = todevice(vocab(preprocess(x)))\n",
    "    seq = [startsym]\n",
    "\n",
    "    enc = encoder_forward(ix)\n",
    "\n",
    "    len = length(ix)\n",
    "    for i = 1:2len\n",
    "        trg = todevice(vocab(seq))\n",
    "        dec = decoder_forward(trg, enc)\n",
    "        #move back to gpu due to argmax wrong result on CuArrays\n",
    "        ntok = onecold(collect(dec), labels)\n",
    "        push!(seq, ntok[end])\n",
    "        ntok[end] == endsym && break\n",
    "    end\n",
    "  seq[2:end-1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: no valid permutation of dimensions",
     "output_type": "error",
     "traceback": [
      "ArgumentError: no valid permutation of dimensions\n",
      "\n",
      "Stacktrace:\n",
      "  [1] permutedims(B::Matrix{Float32}, perm::Tuple{Int64, Int64, Int64})\n",
      "    @ Base ./multidimensional.jl:1491\n",
      "  [2] (::LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}})(query::Matrix{Float32}, key::Matrix{Float32}, value::Matrix{Float32}; mask::Nothing)\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:84\n",
      "  [3] (::Transformer1{LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, PwFFN{Dense{typeof(nelu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Dropout{Float64, Colon}})(x::Matrix{Float32}, mask::Nothing)\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:43\n",
      "  [4] (::Transformer1{LinearAttention{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dropout{Float64, Colon}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, PwFFN{Dense{typeof(nelu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Dropout{Float64, Colon}})(x::Matrix{Float32})\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:42\n",
      "  [5] encoder_forward(x::Vector{Int64})\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:4\n",
      "  [6] translate(x::Vector{String})\n",
      "    @ Main ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:6\n",
      "  [7] top-level scope\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/LinearAttentionTransformer.ipynb:1\n",
      "  [8] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [9] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      " [10] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      " [11] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      " [12] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      " [13] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      " [14] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      " [15] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [16] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [17] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [18] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [19] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [20] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "translate(map(string, [5,5,6,6,1,12,3,4,6]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
