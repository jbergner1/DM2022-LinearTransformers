{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyCall.PyError",
     "evalue": "PyCall.PyError(\"PyImport_ImportModule\\n\\nThe Python package flax could not be imported by pyimport. Usually this means\\nthat you did not install flax in the Python version being used by PyCall.\\n\\nPyCall is currently configured to use the Julia-specific Python distribution\\ninstalled by the Conda.jl package.  To install the flax module, you can\\nuse `pyimport_conda(\\\"flax\\\", PKG)`, where PKG is the Anaconda\\npackage that contains the module flax, or alternatively you can use the\\nConda package directly (via `using Conda` followed by `Conda.add` etcetera).\\n\\nAlternatively, if you want to use a different Python distribution on your\\nsystem, such as a system-wide Python (as opposed to the Julia-specific Python),\\nyou can re-configure PyCall with that Python.   As explained in the PyCall\\ndocumentation, set ENV[\\\"PYTHON\\\"] to the path/name of the python executable\\nyou want to use, run Pkg.build(\\\"PyCall\\\"), and re-launch Julia.\\n\\n\", PyObject(Ptr{PyCall.PyObject_struct} @0x00000001ad3e30e0), PyObject(Ptr{PyCall.PyObject_struct} @0x00000001a66b8740), PyObject(Ptr{PyCall.PyObject_struct} @0x0000000000000000))",
     "output_type": "error",
     "traceback": [
      "PyCall.PyError(\"PyImport_ImportModule\\n\\nThe Python package flax could not be imported by pyimport. Usually this means\\nthat you did not install flax in the Python version being used by PyCall.\\n\\nPyCall is currently configured to use the Julia-specific Python distribution\\ninstalled by the Conda.jl package.  To install the flax module, you can\\nuse `pyimport_conda(\\\"flax\\\", PKG)`, where PKG is the Anaconda\\npackage that contains the module flax, or alternatively you can use the\\nConda package directly (via `using Conda` followed by `Conda.add` etcetera).\\n\\nAlternatively, if you want to use a different Python distribution on your\\nsystem, such as a system-wide Python (as opposed to the Julia-specific Python),\\nyou can re-configure PyCall with that Python.   As explained in the PyCall\\ndocumentation, set ENV[\\\"PYTHON\\\"] to the path/name of the python executable\\nyou want to use, run Pkg.build(\\\"PyCall\\\"), and re-launch Julia.\\n\\n\", PyObject(Ptr{PyCall.PyObject_struct} @0x00000001ad3e30e0), PyObject(Ptr{PyCall.PyObject_struct} @0x00000001a66b8740), PyObject(Ptr{PyCall.PyObject_struct} @0x0000000000000000))\n",
      "\n",
      "Stacktrace:\n",
      "  [1] pyimport(name::String)\n",
      "    @ PyCall ~/.julia/packages/PyCall/bd0X6/src/PyCall.jl:550\n",
      "  [2] top-level scope\n",
      "    @ Untitled-1.ipynb:15\n",
      "  [3] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      "  [5] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      "  [6] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      "  [7] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      "  [8] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      "  [9] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      " [10] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [11] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [12] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [13] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [14] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [15] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "begin\n",
    "\tusing Flux\n",
    "\tusing Flux: onehot\n",
    "\tusing Flux: gradient\n",
    "\tusing Flux.Optimise: update!\n",
    "\tusing Flux: onecold\n",
    "\tusing Transformers\n",
    "\tusing Transformers.Basic \n",
    "    using Transformers.Datasets: batched\n",
    "    enable_gpu(false) \n",
    "    using NNlib\n",
    "\tusing Einsum\n",
    "\tusing PyCall\n",
    "\t\n",
    "\tnn = pyimport(\"flax\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elu_feature_map (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function elu_feature_map(x)\n",
    "  return NN.elu(x) + 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_attention (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function linear_attention(query, key, value, dropout_rng, dropout_rate, broadcast_dropout,deterministic)\n",
    "\n",
    "    broadcast_dropout=True\n",
    "    dropout_rng=None\n",
    "    dropout_rate=0.\n",
    "    deterministic=False\n",
    "    feature_map = elu_feature_map()\n",
    "    eps=1e-6\n",
    "\n",
    "    Base.delete!(broadcast_dropout)\n",
    "    Base.delete!(dropout_rng)\n",
    "    Base.delete!(dropout_rate)\n",
    "    Base.delete!(deterministic)\n",
    "\n",
    "   # Base.@assert key.ndim == query.ndim == value.ndim == 4\n",
    "   # Base.@assert key.shape[:-1] == value.shape[:-1]\n",
    "   # Base.@assert (query.shape[0:1] == key.shape[0:1] and query.shape[-1] == key.shape[-1])\n",
    "\n",
    "    query_mapped = feature_map(query)\n",
    "    key_mapped = feature_map(key)\n",
    "    \n",
    "    @einsum kv[n,h,m,d] = key_mapped[n,s,h,d]*value[n,s,h,m]\n",
    "\n",
    "    sum_1 = sum(key_mapped, axis=1)+eps\n",
    "\n",
    "    @einsum z_1[n,l,h] = query_mapped[n,l,h,d]*sum_1[n,h,d]\n",
    "\n",
    "    z = 1/z_1\n",
    "    \n",
    "    @einsum y[n,l,h,m] = query_mapped[n,l,h,d]*kv[n,h,m,d]*z[n,l,h]\n",
    "\n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function LinearAttention\n",
    "    function apply(\n",
    "        inputs_q,\n",
    "        inputs_kv,\n",
    "        num_heads,\n",
    "        dtype,\n",
    "        qkv_features,\n",
    "        out_features,\n",
    "        causal_mask,\n",
    "        padding_mask,\n",
    "        key_padding_mask,\n",
    "        segmentation,\n",
    "        key_segmentation,\n",
    "        cache,\n",
    "        broadcast_dropout,\n",
    "        dropout_rng,\n",
    "        dropout_rate,\n",
    "        deterministic,\n",
    "        precision,\n",
    "        kernel_init,\n",
    "        bias_init,\n",
    "        bias=\n",
    "    )\n",
    "    dtype=jnp.float32,\n",
    "    qkv_features=None,\n",
    "    out_features=None,\n",
    "    causal_mask=False,\n",
    "    padding_mask=None,\n",
    "    key_padding_mask=None,\n",
    "    segmentation=None,\n",
    "    key_segmentation=None,\n",
    "    cache=None,\n",
    "    broadcast_dropout=True,\n",
    "    dropout_rng=None,\n",
    "    dropout_rate=0.,\n",
    "    deterministic=False,\n",
    "    precision=None,\n",
    "    kernel_init=nn.linear.default_kernel_init, #diese Funktionen gibt es f√ºr Julia nichts\n",
    "    bias_init=nn.initializers.zeros,            # diese auch nicht\n",
    "    bias=True\n",
    "\n",
    "    if padding_mask == nothing\n",
    "        NotImplementedError(\n",
    "            'Currently, we do not support autoregresive decoding.')\n",
    "  \n",
    "       # Base.@assert causal_mask or not cache, (\n",
    "            'Caching is only support for causal attention.')\n",
    "    end\n",
    "\n",
    "    Base.@assert inputs_q.ndim == 3\n",
    "        \n",
    "    if inputs_kv == nothing\n",
    "        inputs_kv = inputs_q\n",
    "    end\n",
    "  \n",
    "      features = out_features || inputs_q.shape[-1]\n",
    "      qkv_features = qkv_features || inputs_q.shape[-1]\n",
    "\n",
    "      Base.@assert qkv_features % num_heads == 0, (\n",
    "        'Memory dimension must be divisible by number of heads.')\n",
    "    \n",
    "    head_dim = qkv_features // num_heads\n",
    "\n",
    "    dense = nn.DenseGeneral.partial(\n",
    "        axis=-1,\n",
    "        features=(num_heads, head_dim),\n",
    "        kernel_init=kernel_init,\n",
    "        bias_init=bias_init,\n",
    "        bias=bias,\n",
    "        precision=precision)\n",
    "\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
