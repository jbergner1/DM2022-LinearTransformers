{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyCall.PyError",
     "evalue": "PyCall.PyError(\"PyImport_ImportModule\\n\\nThe Python package flax.linen could not be imported by pyimport. Usually this means\\nthat you did not install flax.linen in the Python version being used by PyCall.\\n\\nPyCall is currently configured to use the Julia-specific Python distribution\\ninstalled by the Conda.jl package.  To install the flax.linen module, you can\\nuse `pyimport_conda(\\\"flax.linen\\\", PKG)`, where PKG is the Anaconda\\npackage that contains the module flax.linen, or alternatively you can use the\\nConda package directly (via `using Conda` followed by `Conda.add` etcetera).\\n\\nAlternatively, if you want to use a different Python distribution on your\\nsystem, such as a system-wide Python (as opposed to the Julia-specific Python),\\nyou can re-configure PyCall with that Python.   As explained in the PyCall\\ndocumentation, set ENV[\\\"PYTHON\\\"] to the path/name of the python executable\\nyou want to use, run Pkg.build(\\\"PyCall\\\"), and re-launch Julia.\\n\\n\", PyObject(Ptr{PyCall.PyObject_struct} @0x00000001a68190e0), PyObject(Ptr{PyCall.PyObject_struct} @0x00000001a71d7120), PyObject(Ptr{PyCall.PyObject_struct} @0x0000000000000000))",
     "output_type": "error",
     "traceback": [
      "PyCall.PyError(\"PyImport_ImportModule\\n\\nThe Python package flax.linen could not be imported by pyimport. Usually this means\\nthat you did not install flax.linen in the Python version being used by PyCall.\\n\\nPyCall is currently configured to use the Julia-specific Python distribution\\ninstalled by the Conda.jl package.  To install the flax.linen module, you can\\nuse `pyimport_conda(\\\"flax.linen\\\", PKG)`, where PKG is the Anaconda\\npackage that contains the module flax.linen, or alternatively you can use the\\nConda package directly (via `using Conda` followed by `Conda.add` etcetera).\\n\\nAlternatively, if you want to use a different Python distribution on your\\nsystem, such as a system-wide Python (as opposed to the Julia-specific Python),\\nyou can re-configure PyCall with that Python.   As explained in the PyCall\\ndocumentation, set ENV[\\\"PYTHON\\\"] to the path/name of the python executable\\nyou want to use, run Pkg.build(\\\"PyCall\\\"), and re-launch Julia.\\n\\n\", PyObject(Ptr{PyCall.PyObject_struct} @0x00000001a68190e0), PyObject(Ptr{PyCall.PyObject_struct} @0x00000001a71d7120), PyObject(Ptr{PyCall.PyObject_struct} @0x0000000000000000))\n",
      "\n",
      "Stacktrace:\n",
      "  [1] pyimport(name::String)\n",
      "    @ PyCall ~/.julia/packages/PyCall/bd0X6/src/PyCall.jl:550\n",
      "  [2] top-level scope\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/linearAttention.ipynb:15\n",
      "  [3] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      "  [5] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      "  [6] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      "  [7] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      "  [8] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      "  [9] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      " [10] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [11] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [12] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [13] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [14] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [15] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "begin\n",
    "\tusing Flux\n",
    "\tusing Flux: onehot\n",
    "\tusing Flux: gradient\n",
    "\tusing Flux.Optimise: update!\n",
    "\tusing Flux: onecold\n",
    "\tusing Transformers\n",
    "\tusing Transformers.Basic \n",
    "    using Transformers.Datasets: batched\n",
    "    enable_gpu(false) \n",
    "    using NNlib\n",
    "\tusing Einsum\n",
    "\tusing PyCall\n",
    "\t\n",
    "\tnn = pyimport(\"flax.linen\") #das funktioniert noch nicht; dieses Python-Package muss aber importiert werden, da es nichts Ã¤quivalentes zu Julia gibt\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elu_feature_map (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function elu_feature_map(x)\n",
    "  return NN.elu(x) + 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_attention (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function linear_attention(query, key, value, dropout_rng, dropout_rate, broadcast_dropout,deterministic)\n",
    "\n",
    "    broadcast_dropout=True\n",
    "    dropout_rng=None\n",
    "    dropout_rate=0.\n",
    "    deterministic=False\n",
    "    feature_map = elu_feature_map()\n",
    "    eps=1e-6\n",
    "\n",
    "    Base.delete!(broadcast_dropout)\n",
    "    Base.delete!(dropout_rng)\n",
    "    Base.delete!(dropout_rate)\n",
    "    Base.delete!(deterministic)\n",
    "\n",
    "   # Base.@assert key.ndim == query.ndim == value.ndim == 4\n",
    "   # Base.@assert key.shape[:-1] == value.shape[:-1]\n",
    "   # Base.@assert (query.shape[0:1] == key.shape[0:1] and query.shape[-1] == key.shape[-1])\n",
    "\n",
    "    query_mapped = feature_map(query)\n",
    "    key_mapped = feature_map(key)\n",
    "    \n",
    "    @einsum kv[n,h,m,d] = key_mapped[n,s,h,d]*value[n,s,h,m]\n",
    "\n",
    "    sum_1 = sum(key_mapped, axis=1)+eps\n",
    "\n",
    "    @einsum z_1[n,l,h] = query_mapped[n,l,h,d]*sum_1[n,h,d]\n",
    "\n",
    "    z = 1/z_1\n",
    "    \n",
    "    @einsum y[n,l,h,m] = query_mapped[n,l,h,d]*kv[n,h,m,d]*z[n,l,h]\n",
    "\n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: character literal contains multiple characters",
     "output_type": "error",
     "traceback": [
      "syntax: character literal contains multiple characters\n",
      "\n",
      "Stacktrace:\n",
      "  [1] top-level scope\n",
      "    @ ~/Documents/GitHub/DM2022-LinearTransformers/test/linearAttention.ipynb:43\n",
      "  [2] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      "  [4] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      "  [5] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      "  [6] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      "  [7] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      "  [8] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      "  [9] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [10] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [11] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [12] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [13] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [14] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "function apply(\n",
    "        inputs_q,\n",
    "        inputs_kv,\n",
    "        num_heads,\n",
    "        dtype,\n",
    "        qkv_features,\n",
    "        out_features,\n",
    "        causal_mask,\n",
    "        padding_mask,\n",
    "        key_padding_mask,\n",
    "        segmentation,\n",
    "        key_segmentation,\n",
    "        cache,\n",
    "        broadcast_dropout,\n",
    "        dropout_rng,\n",
    "        dropout_rate,\n",
    "        deterministic,\n",
    "        precision,\n",
    "        kernel_init,\n",
    "        bias_init,\n",
    "        bias)\n",
    "\n",
    "    dtype=jnp.float32,\n",
    "    qkv_features=None,\n",
    "    out_features=None,\n",
    "    causal_mask=False,\n",
    "    padding_mask=None,\n",
    "    key_padding_mask=None,\n",
    "    segmentation=None,\n",
    "    key_segmentation=None,\n",
    "    cache=None,\n",
    "    broadcast_dropout=True,\n",
    "    dropout_rng=None,\n",
    "    dropout_rate=0.,\n",
    "    deterministic=False,\n",
    "    precision=None,\n",
    "    kernel_init=nn.linear.default_kernel_init, #diese Funktionen gibt es fÃ¼r Julia nichts\n",
    "    bias_init=nn.initializers.zeros,            # diese auch nicht\n",
    "    bias=True\n",
    "\n",
    "    if padding_mask == nothing\n",
    "        NotImplementedError(\n",
    "            'Currently, we do not support autoregresive decoding.')\n",
    "  \n",
    "       # Base.@assert causal_mask or not cache, (\n",
    "            'Caching is only support for causal attention.')\n",
    "    end\n",
    "\n",
    "    Base.@assert inputs_q.ndim == 3\n",
    "        \n",
    "    if inputs_kv == nothing\n",
    "       inputs_kv = inputs_q\n",
    "    end\n",
    "  \n",
    "      features = out_features || inputs_q.shape[-1]\n",
    "      qkv_features = qkv_features || inputs_q.shape[-1]\n",
    "\n",
    "      #Base.@assert qkv_features % num_heads == 0, (\n",
    "       # 'Memory dimension must be divisible by number of heads.')\n",
    "    \n",
    "    head_dim = qkv_features // num_heads\n",
    "\n",
    "    dense = nn.DenseGeneral.partial(    #diese funktion gibt es in keinem Julia package -> PyCall wird notwendig sein\n",
    "        axis=-1,\n",
    "        features=(num_heads, head_dim),\n",
    "        kernel_init=kernel_init,\n",
    "        bias_init=bias_init,\n",
    "        bias=bias,\n",
    "        precision=precision)\n",
    "\n",
    "        query, key, value = (dense(inputs_q, dtype=dtype, name='query'),\n",
    "        dense(inputs_kv, dtype=dtype, name='key'),\n",
    "        dense(inputs_kv, dtype=dtype, name='value'))\n",
    "\n",
    "\n",
    "    # apply regular dot product attention\n",
    "    x = linear_attention(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            dropout_rng,\n",
    "            dropout_rate,\n",
    "            broadcast_dropout,\n",
    "            deterministic)\n",
    "\n",
    "    # back to the original inputs dimensions\n",
    "    out = nn.DenseGeneral(\n",
    "                x,\n",
    "                features,\n",
    "                (-2, -1),\n",
    "                kernel_init,\n",
    "                bias_init,\n",
    "                bias,\n",
    "                dtype,\n",
    "                precision,\n",
    "                'out')\n",
    "\n",
    "        return out\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTransformerBlock(nn.Module):\n",
    "  \"\"\"FastTransformer layer (https://arxiv.org/abs/2006.16236).\"\"\"\n",
    "\n",
    "function apply(self,\n",
    "            inputs,\n",
    "            qkv_dim,\n",
    "            mlp_dim,\n",
    "            num_heads,\n",
    "            dtype,\n",
    "            inputs_segmentation,\n",
    "            causal_mask,\n",
    "            padding_mask,\n",
    "            dropout_rate,\n",
    "            attention_dropout_rate,\n",
    "            deterministic,\n",
    "            cache)\n",
    "\n",
    "    dtype=jnp.float32,\n",
    "    inputs_segmentation=None,\n",
    "    causal_mask=False,\n",
    "    padding_mask=None,\n",
    "    dropout_rate=0.1,\n",
    "    attention_dropout_rate=0.1,\n",
    "    deterministic=False,\n",
    "    cache=None\n",
    "    \"\"\"Applies LinearTransformer module.\n",
    "    Args:\n",
    "      inputs: input data\n",
    "      qkv_dim: dimension of the query/key/value\n",
    "      mlp_dim: dimension of the mlp on top of attention block\n",
    "      num_heads: number of heads\n",
    "      dtype: the dtype of the computation (default: float32).\n",
    "      inputs_segmentation: input segmentation info for packed examples.\n",
    "      causal_mask: bool, mask future or not\n",
    "      padding_mask: bool, mask padding tokens\n",
    "      dropout_rate: dropout rate\n",
    "      attention_dropout_rate: dropout rate for attention weights\n",
    "      deterministic: bool, deterministic or not (to apply dropout)\n",
    "      cache: flax autoregressive cache for fast decoding.\n",
    "    Returns:\n",
    "      output after transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    # Attention block.\n",
    "    assert inputs.ndim == 3\n",
    "    x = nn.LayerNorm(inputs)\n",
    "    x = linear_attention.LinearSelfAttention(\n",
    "        x,\n",
    "        num_heads=num_heads,\n",
    "        dtype=dtype,\n",
    "        qkv_features=qkv_dim,\n",
    "        causal_mask=causal_mask,\n",
    "        segmentation=inputs_segmentation,\n",
    "        padding_mask=padding_mask,\n",
    "        kernel_init=nn.initializers.xavier_uniform(),\n",
    "        bias_init=nn.initializers.normal(stddev=1e-6),\n",
    "        bias=False,\n",
    "        broadcast_dropout=False,\n",
    "        dropout_rate=attention_dropout_rate,\n",
    "        deterministic=deterministic,\n",
    "        cache=cache)\n",
    "    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)\n",
    "    x = x + inputs\n",
    "\n",
    "    # MLP block.\n",
    "    y = nn.LayerNorm(x)\n",
    "    y = common_layers.MlpBlock(\n",
    "        y,\n",
    "        mlp_dim=mlp_dim,\n",
    "        dtype=dtype,\n",
    "        dropout_rate=dropout_rate,\n",
    "        deterministic=deterministic)\n",
    "\n",
    "    return x + y\n",
    "\n",
    "\n",
    "class LinearTransformerEncoder(nn.Module):\n",
    "  \"\"\"Linear Transformer Model Encoder.\"\"\"\n",
    "\n",
    "  def apply(self,\n",
    "            inputs,\n",
    "            vocab_size,\n",
    "            inputs_positions=None,\n",
    "            inputs_segmentation=None,\n",
    "            shared_embedding=None,\n",
    "            use_bfloat16=False,\n",
    "            emb_dim=512,\n",
    "            num_heads=8,\n",
    "            dtype=jnp.float32,\n",
    "            num_layers=6,\n",
    "            qkv_dim=512,\n",
    "            mlp_dim=2048,\n",
    "            max_len=512,\n",
    "            train=True,\n",
    "            dropout_rate=0.1,\n",
    "            attention_dropout_rate=0.1,\n",
    "            learn_pos_emb=False,\n",
    "            classifier=False,\n",
    "            classifier_pool='CLS',\n",
    "            num_classes=10):\n",
    "    \"\"\"Applies Transformer model on the inputs.\n",
    "    Args:\n",
    "      inputs: input data\n",
    "      vocab_size: size of the vocabulary\n",
    "      inputs_positions: input subsequence positions for packed examples.\n",
    "      inputs_segmentation: input segmentation info for packed examples.\n",
    "      shared_embedding: a shared embedding layer to use.\n",
    "      use_bfloat16: bool: whether use bfloat16.\n",
    "      emb_dim: dimension of embedding\n",
    "      num_heads: number of heads\n",
    "      dtype: the dtype of the computation (default: float32)\n",
    "      num_layers: number of layers\n",
    "      qkv_dim: dimension of the query/key/value\n",
    "      mlp_dim: dimension of the mlp on top of attention block\n",
    "      max_len: maximum length.\n",
    "      train: if it is training,\n",
    "      dropout_rate: dropout rate\n",
    "      attention_dropout_rate: dropout rate for attention weights\n",
    "      learn_pos_emb: boolean, if learn the positional embedding or use the\n",
    "        sinusoidal positional embedding.\n",
    "      classifier: boolean, for classification mode (output N-class logits)\n",
    "      classifier_pool: str, supports \"MEAN\", \"MAX\" pooling.\n",
    "      num_classes: int, number of classification classes.\n",
    "    Returns:\n",
    "      output of a transformer encoder or logits if classifier_mode is true.\n",
    "    \"\"\"\n",
    "    assert inputs.ndim == 2  # (batch, len)\n",
    "\n",
    "    # Padding Masks\n",
    "    src_padding_mask = (inputs > 0)[..., None]\n",
    "\n",
    "    # Input Embedding\n",
    "    if shared_embedding is None:\n",
    "      input_embed = nn.Embed.partial(\n",
    "          num_embeddings=vocab_size,\n",
    "          features=emb_dim,\n",
    "          embedding_init=nn.initializers.normal(stddev=1.0))\n",
    "    else:\n",
    "      input_embed = shared_embedding\n",
    "    x = inputs.astype('int32')\n",
    "    x = input_embed(x)\n",
    "\n",
    "    if classifier and classifier_pool == 'CLS':\n",
    "      cls = self.param('cls', (1, 1, emb_dim), nn.initializers.zeros)\n",
    "      cls = jnp.tile(cls, [x.shape[0], 1, 1])\n",
    "      x = jnp.concatenate([cls, x], axis=1)\n",
    "      max_len += 1\n",
    "      src_padding_mask = jnp.concatenate(\n",
    "          [src_padding_mask[:, :1], src_padding_mask], axis=1)\n",
    "    pe_init = nn.initializers.normal(stddev=0.02) if learn_pos_emb else None\n",
    "    x = common_layers.AddPositionEmbs(\n",
    "        x,\n",
    "        inputs_positions=inputs_positions,\n",
    "        posemb_init=pe_init,\n",
    "        max_len=max_len,\n",
    "        name='posembed_input')\n",
    "    x = nn.dropout(x, rate=dropout_rate, deterministic=not train)\n",
    "\n",
    "    if use_bfloat16:\n",
    "      x = x.astype(jnp.bfloat16)\n",
    "      dtype = jnp.bfloat16\n",
    "    else:\n",
    "      dtype = jnp.float32\n",
    "\n",
    "    # Input Encoder\n",
    "    for lyr in range(num_layers):\n",
    "      x = LinearTransformerBlock(\n",
    "          x,\n",
    "          qkv_dim=qkv_dim,\n",
    "          mlp_dim=mlp_dim,\n",
    "          num_heads=num_heads,\n",
    "          dtype=dtype,\n",
    "          padding_mask=src_padding_mask,\n",
    "          inputs_segmentation=inputs_segmentation,\n",
    "          dropout_rate=dropout_rate,\n",
    "          attention_dropout_rate=attention_dropout_rate,\n",
    "          deterministic=not train,\n",
    "          name=f'encoderblock_{lyr}')\n",
    "    encoded = nn.LayerNorm(x, dtype=dtype, name='encoder_norm')\n",
    "\n",
    "    if classifier:\n",
    "      encoded = common_layers.classifier_head(\n",
    "          encoded, num_classes, mlp_dim, pooling_mode=classifier_pool)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "class LinearTransformerDualEncoder(nn.Module):\n",
    "  \"\"\"Linear Transformer Model for Matching (dual encoding) tasks.\"\"\"\n",
    "\n",
    "  def apply(self,\n",
    "            inputs1,\n",
    "            inputs2,\n",
    "            vocab_size=None,\n",
    "            inputs1_positions=None,\n",
    "            inputs2_positions=None,\n",
    "            inputs1_segmentation=None,\n",
    "            inputs2_segmentation=None,\n",
    "            use_bfloat16=False,\n",
    "            emb_dim=512,\n",
    "            num_heads=8,\n",
    "            num_layers=6,\n",
    "            qkv_dim=512,\n",
    "            mlp_dim=2048,\n",
    "            max_len=2048,\n",
    "            train=False,\n",
    "            dropout_rate=0.1,\n",
    "            attention_dropout_rate=0.1,\n",
    "            classifier=True,\n",
    "            classifier_pool='CLS',\n",
    "            num_classes=2,\n",
    "            interaction=None):\n",
    "    \"\"\"Applies Transformer model on text similarity.\n",
    "    A deliberate choice to distinguish this from NLI because\n",
    "    we may want to do different things to the model later. Dual Encoding\n",
    "    mode enforces that we do not do cross attention between pairs.\n",
    "    Args:\n",
    "      inputs1: input data.\n",
    "      inputs2: target data.\n",
    "      vocab_size: size of the input vocabulary.\n",
    "      inputs1_positions: input subsequence positions for packed examples.\n",
    "      inputs2_positions: target subsequence positions for packed examples.\n",
    "      inputs1_segmentation: input segmentation info for packed examples.\n",
    "      inputs2_segmentation: target segmentation info for packed examples.\n",
    "      use_bfloat16: bool: whether use bfloat16.\n",
    "      emb_dim: dimension of embedding.\n",
    "      num_heads: number of heads.\n",
    "      num_layers: number of layers.\n",
    "      qkv_dim: dimension of the query/key/value.\n",
    "      mlp_dim: dimension of the mlp on top of attention block.\n",
    "      max_len: maximum length.\n",
    "      train: whether it is training.\n",
    "      dropout_rate: dropout rate.\n",
    "      attention_dropout_rate: dropout rate for attention weights.\n",
    "      classifier: boolean, to use classifier.\n",
    "      classifier_pool: str, supports \"MEAN\", \"MAX\" pooling.\n",
    "      num_classes: int, number of classification classes.\n",
    "      interaction: str\n",
    "    Returns:\n",
    "      output of a transformer decoder.\n",
    "    \"\"\"\n",
    "    encoder = LinearTransformerEncoder.shared(\n",
    "        inputs_positions=inputs1_positions,\n",
    "        inputs_segmentation=inputs1_segmentation,\n",
    "        vocab_size=vocab_size,\n",
    "        use_bfloat16=use_bfloat16,\n",
    "        emb_dim=emb_dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        qkv_dim=qkv_dim,\n",
    "        mlp_dim=mlp_dim,\n",
    "        max_len=max_len,\n",
    "        train=train,\n",
    "        dropout_rate=dropout_rate,\n",
    "        attention_dropout_rate=attention_dropout_rate,\n",
    "        name='encoder')\n",
    "    inputs1_encoded = encoder(inputs1)\n",
    "    inputs2_encoded = encoder(inputs2)\n",
    "\n",
    "    encoded = common_layers.classifier_head_dual(\n",
    "        inputs1_encoded,\n",
    "        inputs2_encoded,\n",
    "        num_classes,\n",
    "        mlp_dim,\n",
    "        pooling_mode=classifier_pool,\n",
    "        interaction=interaction)\n",
    "\n",
    "    return encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
