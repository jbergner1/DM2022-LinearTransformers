{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\tusing Flux\n",
    "\tusing Flux: onehot\n",
    "\tusing Flux: gradient\n",
    "\tusing Flux.Optimise: update!\n",
    "\tusing Flux: onecold\n",
    "\tusing CUDA\n",
    "\tusing Transformers\n",
    "\tusing Transformers.Basic \n",
    "    using Transformers.Datasets: batched\n",
    "    enable_gpu(false) \n",
    "\tusing Flux: @functor\n",
    "\tusing ..Transformers: Abstract3DTensor, Container, epsilon, batchedmul, batched_triu!\n",
    "\tusing Plots\n",
    "end\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary{String}(13, unk=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Labeling: vielleicht ändern\n",
    "\n",
    "begin\n",
    "\tlabels = map(string, 1:10)\n",
    "\tstartsym = \"11\"\n",
    "\tendsym = \"12\"\n",
    "\tunksym = \"0\"\n",
    "\tlabels = [unksym, startsym, endsym, labels...]\n",
    "\tvocab = Vocabulary(labels, unksym)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for generate training datas \n",
    "\n",
    "#nichts ändern\n",
    "sample_data() = (d = map(string, rand(1:10, 10)); (d,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"8\", \"6\", \"10\", \"10\", \"3\", \"8\", \"4\", \"4\", \"7\", \"5\"], [\"8\", \"6\", \"10\", \"10\", \"3\", \"8\", \"4\", \"4\", \"7\", \"5\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "\n",
    "sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for adding start & end symbol\n",
    "\n",
    "#nichts ändern\n",
    "preprocess(x) = [startsym, x..., endsym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       " 10\n",
       " 12\n",
       "  8\n",
       "  5\n",
       "  5\n",
       " 11\n",
       "  5\n",
       " 12\n",
       "  5\n",
       "  4\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "\n",
    "begin\n",
    "    @show sample_ex = preprocess.(sample_data())\n",
    "    @show encoded_sample_ex = vocab(sample_ex[1]) #use Vocabulary to encode the training data\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"11\", \"1\", \"1\", \"2\", \"6\", \"10\", \"3\", \"10\", \"1\", \"2\", \"7\", \"12\"], [\"11\", \"1\", \"1\", \"2\", \"6\", \"10\", \"3\", \"10\", \"1\", \"2\", \"7\", \"12\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "sample = preprocess.(sample_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       "  4\n",
       "  4\n",
       "  5\n",
       "  9\n",
       " 13\n",
       "  6\n",
       " 13\n",
       "  4\n",
       "  5\n",
       " 10\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "encoded_sample = vocab(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embed(128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a Word embedding layer which turn word index to word vector\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "embed = Embed(128, length(vocab)) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionEmbedding(128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a position embedding layer metioned above\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "pe = PositionEmbedding(128) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wrapper for get embedding\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "function embedding(x)\n",
    "    we = embed(x, inv(sqrt(128)))\n",
    "    e = we .+ pe(we)\n",
    "    return e\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractAttention end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_atten_mask1 (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_atten_mask1(T::Type, score::AbstractArray, ::Nothing, future) = create_atten_mask1(T, score, fill!(similar(score, size(score,1), size(score, 2), 1), one(T)), future)\n",
    "function create_atten_mask1(T::Type, score::AbstractArray, _mask::AbstractArray, future::Bool=false)\n",
    "  #size(mask) == (q, k, n, b)\n",
    "\n",
    "  # ql, kl = size(mask)\n",
    "  mask = copy(_mask)\n",
    "\n",
    "  maskval = convert(T, -1e9)\n",
    "  !future && batched_triu!(mask, 0)\n",
    "  mask .= (1 .- mask) .* maskval\n",
    "  return mask\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd create_atten_mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MultiheadAttention_1{Q<:Dense, K<:Dense, V<:Dense, O<:Dense, DP<:Dropout} <: AbstractAttention\n",
    "    head::Int\n",
    "    future::Bool\n",
    "    iqproj::Q\n",
    "    ikproj::K\n",
    "    ivproj::V\n",
    "    oproj::O\n",
    "    drop::DP\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.functor(mh::MultiheadAttention_1) = (mh.iqproj, mh.ikproj, mh.ivproj, mh.oproj), m -> MultiheadAttention_1(mh.head, mh.future, m..., mh.drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;\\n                       future::Bool=true, pdrop = 0.1)\\nMultihead dot product Attention Layer, `head` is the number of head, \\n`is` is the input size, `hs` is the hidden size of input projection layer of each head, \\n`os` is the output size. When `future` is `false`, the k-th token can't see tokens at > k. \\n`pdrop` is the dropout rate.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;\n",
    "                       future::Bool=true, pdrop = 0.1)\n",
    "Multihead dot product Attention Layer, `head` is the number of head, \n",
    "`is` is the input size, `hs` is the hidden size of input projection layer of each head, \n",
    "`os` is the output size. When `future` is `false`, the k-th token can't see tokens at > k. \n",
    "`pdrop` is the dropout rate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiheadAttention_1(head::Int,\n",
    "                   is::Int,\n",
    "                   hs::Int,\n",
    "                   os::Int;\n",
    "                   future::Bool=true, pdrop = 0.1) = MultiheadAttention_1(head,\n",
    "                                                                        future,\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(hs*head, os),\n",
    "                                                                        Dropout(pdrop),\n",
    "                                                                        )\n",
    "\n",
    "\n",
    "function Base.show(io::IO, mh::MultiheadAttention_1)\n",
    "    hs = div(size(mh.iqproj.weight)[1], mh.head)\n",
    "    is = size(mh.iqproj.weight)[end]\n",
    "    os = size(mh.oproj.weight)[1]\n",
    "\n",
    "    print(io, \"MultiheadAttention(\")\n",
    "    print(io, \"head=$(mh.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"$(is)=>$(os)\")\n",
    "\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(mh.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (mh::MultiheadAttention_1)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: Abstract3DTensor{T},\n",
    "                         A2 <: Abstract3DTensor{T},\n",
    "                         A3 <: Abstract3DTensor{T}}\n",
    "qs = size(query)\n",
    "ks = size(key)\n",
    "vs = size(value)\n",
    "\n",
    "#size(ipq) == (h, q_seq_len, batch)\n",
    "ipq = @toNd mh.iqproj(query)\n",
    "ipk = @toNd mh.ikproj(key)\n",
    "ipv = @toNd mh.ivproj(value)\n",
    "@show(ipq)\n",
    "\n",
    "h = size(ipq, 1)\n",
    "hs = div(h, mh.head)\n",
    "\n",
    "#size(ipq) == (hs, q_seq_len, head, batch)\n",
    "ipq = permutedims(reshape(ipq, hs, mh.head, qs[2], qs[3]), [1, 3, 2, 4])\n",
    "ipk = permutedims(reshape(ipk, hs, mh.head, ks[2], ks[3]), [1, 3, 2, 4])\n",
    "ipv = permutedims(reshape(ipv, hs, mh.head, vs[2], vs[3]), [1, 3, 2, 4])\n",
    "\n",
    "#size(ipq) == (hs, q_seq_len, head * batch)\n",
    "ipq = reshape(ipq, hs, qs[2], :)\n",
    "ipk = reshape(ipk, hs, ks[2], :)\n",
    "ipv = reshape(ipv, hs, vs[2], :)\n",
    "\n",
    "atten = attention1(ipq,ipk,ipv,\n",
    "mask,\n",
    "mh.future,\n",
    "mh.drop)\n",
    "\n",
    "atten = permutedims(reshape(atten, hs, qs[2], mh.head, qs[3]), [1, 3, 2, 4]) #size(atten) == (hs, head, ql, b)\n",
    "atten = reshape(atten, h, qs[2], qs[3]) #size(atten) == (h, ql, b)\n",
    "\n",
    "out = @toNd mh.oproj(atten)\n",
    "out #size(out) == (h, q_seq_len, batch)\n",
    "end\n",
    "\n",
    "function (mh::MultiheadAttention_1)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: AbstractMatrix{T},\n",
    "                         A2 <: AbstractMatrix{T},\n",
    "                         A3 <: AbstractMatrix{T}}\n",
    "\n",
    "# size(query) == (dims, seq_len)\n",
    "ipq = mh.iqproj(query)\n",
    "ipk = mh.ikproj(key)\n",
    "ipv = mh.ivproj(value)\n",
    "\n",
    "h = size(ipq)[1] #h == hs * head\n",
    "hs = div(h, mh.head)\n",
    "\n",
    "#size(hq) == (hs, seq_len, head)\n",
    "hq = permutedims(reshape(ipq, hs, mh.head, :), [1, 3, 2])\n",
    "hk = permutedims(reshape(ipk, hs, mh.head, :), [1, 3, 2])\n",
    "hv = permutedims(reshape(ipv, hs, mh.head, :), [1, 3, 2])\n",
    "\n",
    "atten = attention1(hq, hk, hv,\n",
    "mask,\n",
    "mh.future,\n",
    "mh.drop)\n",
    "\n",
    "# size(atten) == (head*hs, seq_len)\n",
    "atten = reshape(permutedims(atten, [1, 3, 2]), h, :)\n",
    "\n",
    "mh.oproj(atten)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLinearAttention (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function CausalLinearAttention(queryDimensions)\n",
    "    feature_map = nelu(queryDimensions)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_sizes_compatible (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function make_sizes_compatible(Q, K)\n",
    "    N, L, H, E = Q.shape\n",
    "    _, S, _, _ = K.shape\n",
    "        if L == S\n",
    "            return Q, K\n",
    "        end\n",
    "        if L < S\n",
    "            return Q, K[:, :L, :, :]\n",
    "        end\n",
    "        if L > S\n",
    "            return Q, torch.cat([K, K.new_zeros(N, L-S, H, E)], dim=1)\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward(queries, keys, values, attn_mask, query_lengths, key_lengths)\n",
    "    Q = nelu(queries)\n",
    "    K = nelu(keys)\n",
    "    K = K * key_lengths.float_matrix[:, :, None, None]\n",
    "\n",
    "    Q, K = make_sizes_compatible(Q, K)\n",
    "\n",
    "    Z = 1/(torch.einsum(\"nlhi,nlhi->nlh\", Q, K.cumsum(1)) + self.eps)  # diese Funktionen und Packages gehen nur mit Torch und CUDA GPU!!\n",
    "\n",
    "    # Compute the unnormalized result\n",
    "    V = causal_linear(\n",
    "        Q,\n",
    "        K,\n",
    "        values\n",
    "    )\n",
    "\n",
    "    return V * Z[:, :, :, None]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_mask1 (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apply_mask1(score, mask)\n",
    "    s = size(score)\n",
    "    ms = size(mask)\n",
    "    bxn = s[end]\n",
    "    b = ms[end]\n",
    "    if bxn == b || b == 1\n",
    "      return score .+ mask\n",
    "    else\n",
    "      return reshape(reshape(score, s[1:end-1]..., :, b) .+\n",
    "                     reshape(mask, ms[1:end-1]..., 1, b), s)\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  apply_mask1(score::AbstractArray{T}, ::Nothing, future) where T = future ? score : apply_mask1(score, create_atten_mask1(T, score, nothing, future))\n",
    "  apply_mask1(score::AbstractArray{T}, mask, future) where T = apply_mask(score, create_atten_mask1(T, score, mask, future))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention1 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hier anstatt softmax linear\n",
    "function attention1(query::A1,\n",
    "    key::A2,\n",
    "    value::A3,\n",
    "    mask, future::Bool,\n",
    "    dropout) where {T,\n",
    "                    A1 <: Abstract3DTensor{T},\n",
    "                    A2 <: Abstract3DTensor{T},\n",
    "                    A3 <: Abstract3DTensor{T}}\n",
    "#size(query) == (dims, {q,k}_seq_len, batch) == size(key) == size(value)\n",
    "#size(ipq) == (hs, q_seq_len, head * batch)\n",
    "#size(score) == (k_seq_len, q_seq_len, batch)\n",
    "\n",
    "dk = size(key, 1)\n",
    "score = batchedmul(key, query; transA = true) \n",
    "score = score ./ convert(T, sqrt(dk))\n",
    "\n",
    "# score = apply_mask1(score, mask, future)\n",
    "score = softmax(score; dims=1)\n",
    "dropout !== nothing && (score = dropout(score))\n",
    "batchedmul(value, score) #size(return) == (dims, q_seq_len, batch)\n",
    "\n",
    "#size(query) == (batch, {q,k}_seq_len, dims) == size(key) == size(value)\n",
    "#=\n",
    "query = permutedims(query,(3,2,1))\n",
    "key = permutedims(key,(3,2,1))\n",
    "value = permutedims(value,(3,2,1))\n",
    "\n",
    "qs = size(query)\n",
    "ks = size(key)\n",
    "vs = size(value)\n",
    "\n",
    "#depth = div(mh.size, mh.head)\n",
    "depth = div(512, 8)   \n",
    "\n",
    "#query = nelu(splitHeads(query, qs[1], mh.head, depth))\n",
    "#key = nelu(splitHeads(key, qs[1], mh.head, depth))\n",
    "#value = splitHeads(value, qs[1], mh.head, depth)\n",
    "\n",
    "query = nelu(splitHeads(query, qs[1], 8, depth))\n",
    "key = nelu(splitHeads(key, ks[1], 8, depth))\n",
    "value = splitHeads(value, vs[1], 8, depth)\n",
    "# size(k_v) == (batch_size, depth_k, depth_v, seq_len_v)\n",
    "@einsum k_v[m,d,e,h] := key[m,j,h,d]*value[m,j,h,e]\n",
    "\n",
    "k_reduced = sum(key, dims=2) .+ 1e-8\n",
    "\n",
    "@einsum z_1[m,l,h] := query[m,l,h,d]*k_reduced[m,h,d]\n",
    "# size(z) == (batch_size, num_heads, seq_len_q)\n",
    "z = 1/z_1 # ...\n",
    "# size(output) == (batch_size,len_q, heads, depth_v)\n",
    "@einsum output[m,l,h,e] := query[m,l,h,d]*k_v[m,d,e,h]*z[m,l,h]\n",
    "\n",
    "#output = reshape(output, (qs[1], qs[2], mh.head*depth))\n",
    "# size(output) == (batch_size,len_q, d_model)\n",
    "output = reshape(output, (qs[1], :, 8*depth))\n",
    "\n",
    "output = permutedims(output,(3,2,1))\n",
    "#(dims, q_seq_len, batch)\n",
    "=#\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function attention(query::A1,\n",
    "    key::A2,\n",
    "    value::A3,\n",
    "    mask, future::Bool,\n",
    "    dropout) where {T,\n",
    "                    A1 <: Abstract3DTensor{T},\n",
    "                    A2 <: Abstract3DTensor{T},\n",
    "                    A3 <: Abstract3DTensor{T}}\n",
    "#size(query) == (dims, {q,k}_seq_len, batch) == size(key) == size(value)\n",
    "#size(score) == (k_seq_len, q_seq_len, batch)\n",
    "dk = size(key, 1)\n",
    "score = batchedmul(key, query; transA = true)\n",
    "score = score ./ convert(T, sqrt(dk))\n",
    "\n",
    "score = apply_mask1(score, mask, future)\n",
    "score = softmax(score; dims=1)\n",
    "dropout !== nothing && (score = dropout(score))\n",
    "batchedmul(value, score) #size(return) == (dims, q_seq_len, batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "causal_linear (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function causal_linear(query, key, value)\n",
    "    query = query.permute(0,2,1,3).contiguous()\n",
    "    key = key.permute(0,2,1,3).contiguous()\n",
    "    value = value.permute(0,2,1,3).contiguous()\n",
    "    value_new = causal_dot_product(Q, K, V)\n",
    "    return value_new.permute(0,2,1,3).contiguous()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractTransformer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nelu (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oftf(x, y) = oftype(float(x), y)\n",
    "\n",
    "\n",
    "nelu(x, α=1) = ifelse(x ≥ 0, float(x)+1, @fastmath oftf(x, α) * (exp(x) - 1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PwFFN{Di<:Dense, Do<:Dense}\n",
    "    din::Di\n",
    "    dout::Do\n",
    "end\n",
    "\n",
    "@functor PwFFN\n",
    "\n",
    "\n",
    "\"just a wrapper for two dense layer.\"\n",
    "PwFFN(size::Int, h::Int, act = elu) = PwFFN(  #relu vielleicht auf elu + 1 oder nur elu ändern\n",
    "    Dense(size, h, act),\n",
    "    Dense(h, size)\n",
    ")\n",
    "\n",
    "function (pw::PwFFN)(x::AbstractMatrix)\n",
    "  # size(x) == (dims, seq_len)\n",
    "  pw.dout(pw.din(x))\n",
    "end\n",
    "\n",
    "function (pw::PwFFN)(x::A) where {T, N, A<:AbstractArray{T, N}}\n",
    "  new_x = reshape(x, size(x, 1), :)\n",
    "  y = pw(new_x)\n",
    "  return reshape(y, Base.setindex(size(x), size(y, 1), 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Transformer1{MA<:MultiheadAttention_1, LA<:LayerNorm, P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "    mh::MA\n",
    "    mhn::LA\n",
    "    pw::P\n",
    "    pwn::LP\n",
    "    drop::DP\n",
    "end\n",
    "\n",
    "@functor Transformer1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Transformer(size::Int, head::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)\n",
    "    Transformer(size::Int, head::Int, hs::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)  \n",
    "\n",
    "Transformer layer.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "When `future` is `false`, the k-th token can't see the j-th tokens where j > k. `pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "\n",
    "########\n",
    "#ganze Funktion von Transformer ändern\n",
    "########\n",
    "function Transformer1(size::Int, head::Int, ps::Int; future::Bool = true, act = elu, pdrop = 0.1)  #relu vielleicht wieder auf elu ändern\n",
    "    rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "    Transformer1(size, head, div(size, head), ps;future=future, act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "Transformer1(size::Int, head::Int, hs::Int, ps::Int; future::Bool = true, act = elu, pdrop = 0.1) = Transformer1(\n",
    "    MultiheadAttention_1(head, size, hs, size; future=future, pdrop=pdrop),\n",
    "    LayerNorm(size),\n",
    "    PwFFN(size, ps, act),\n",
    "    LayerNorm(size),\n",
    "    Dropout(pdrop),     #braucht man das? vielleicht nicht\n",
    ")\n",
    "\n",
    "function (t::Transformer1)(x::A, mask=nothing) where {T, N, A<:AbstractArray{T, N}}\n",
    "    dropout = t.drop\n",
    "    a = t.mh(x, x, x; mask=mask)\n",
    "    a = dropout(a)\n",
    "    res_a = x + a\n",
    "    res_a = t.mhn(res_a)\n",
    "    pwffn = t.pw(res_a)\n",
    "    pwffn = dropout(pwffn)\n",
    "    res_pwffn = res_a + pwffn\n",
    "    res_pwffn = t.pwn(res_pwffn)\n",
    "    res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, t::Transformer1) #zum visualisieren --> nice to have\n",
    "    hs = div(size(t.mh.iqproj.weight)[1], t.mh.head)\n",
    "    h, ps = size(t.pw.dout.weight)\n",
    "\n",
    "    print(io, \"Transformer(\")\n",
    "    print(io, \"head=$(t.mh.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"pwffn_size=$(ps), \")\n",
    "    print(io, \"size=$(h)\")\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(t.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "    #auch diese Funktion ändern\n",
    "    ###############\n",
    "    struct TransformerDecoder1{MA<:MultiheadAttention_1, LA<:LayerNorm,\n",
    "        IMA<:MultiheadAttention_1, ILA<:LayerNorm,\n",
    "        P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "mh::MA\n",
    "mhn::LA\n",
    "imh::IMA\n",
    "imhn::ILA\n",
    "pw::P\n",
    "pwn::LP\n",
    "drop::DP\n",
    "end\n",
    "\n",
    "@functor TransformerDecoder1\n",
    "\n",
    "\"\"\"\n",
    "TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "TransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "\n",
    "TransformerDecoder layer. Decode the value from a Encoder.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "`pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "function TransformerDecoder1(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "TransformerDecoder1(size, head, div(size, head), ps; act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "TransformerDecoder1(size::Int, head::Int, hs::Int, ps::Int; act = elu, pdrop = 0.1) = TransformerDecoder1(\n",
    "MultiheadAttention_1(head, size, hs, size; future=false, pdrop=pdrop),\n",
    "LayerNorm(size),\n",
    "MultiheadAttention_1(head, size, hs, size; future=true, pdrop=pdrop), #future = true --> Unterschied zu oben??\n",
    "LayerNorm(size),\n",
    "PwFFN(size, ps, act),\n",
    "LayerNorm(size),\n",
    "Dropout(pdrop),\n",
    ")\n",
    "\n",
    "function (td::TransformerDecoder1)(x::AbstractArray{T,N}, m, mask=nothing) where {T,N}\n",
    "dropout = td.drop\n",
    "a = td.mh(x,x,x)\n",
    "a = dropout(a)\n",
    "res_a = x + a\n",
    "res_a = td.mhn(res_a)\n",
    "\n",
    "ia = td.imh(res_a, m, m, mask=mask)\n",
    "ia = dropout(ia)\n",
    "res_ia = res_a + ia\n",
    "res_ia = td.imhn(res_ia)\n",
    "\n",
    "pwffn = td.pw(res_ia)\n",
    "pwffn = dropout(pwffn)\n",
    "res_pwffn = res_ia + pwffn\n",
    "res_pwffn = td.pwn(res_pwffn)\n",
    "res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, td::TransformerDecoder1)\n",
    "hs = div(size(td.imh.iqproj.weight)[1], td.imh.head)\n",
    "h, ps = size(td.pw.dout.weight)\n",
    "\n",
    "print(io, \"TransformerDecoder(\")\n",
    "print(io, \"head=$(td.mh.head), \")\n",
    "print(io, \"head_size=$(hs), \")\n",
    "print(io, \"pwffn_size=$(ps), \")\n",
    "print(io, \"size=$(h)\")\n",
    "if Flux.istraining()\n",
    "print(io, \", dropout=$(td.drop.p))\")\n",
    "else\n",
    "print(io, \")\")\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer\n",
    "\n",
    "#layer auf jeden Fall ändern\n",
    "encode_t1 = Transformer1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t2 = Transformer1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t3 = Transformer1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t4 = Transformer1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer decoder\n",
    "\n",
    "#layer auf jeden Fall ändern\n",
    "decode_t1 = TransformerDecoder1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t2 = TransformerDecoder1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t3 = TransformerDecoder1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t4 = TransformerDecoder1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positionwise(Dense(128, 13), logsoftmax)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define the layer to get the final output probabilities\n",
    "\n",
    "#ÄNDERN\n",
    "linear = Positionwise(Dense(128, length(vocab)), logsoftmax) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neue Layer einfügen\n",
    "function encoder_forward(x)\n",
    "    e = embedding(x)\n",
    "    t1 = encode_t1(e)\n",
    "    t2 = encode_t2(t1)\n",
    "    t3 = encode_t2(t2)\n",
    "    t4 = encode_t2(t3)\n",
    "    return t2\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neue Layer einfügen\n",
    "function decoder_forward(x, m)\n",
    "    e = embedding(x)\n",
    "    t1 = decode_t1(e, m)\n",
    "    t2 = decode_t2(t1, m)\n",
    "    t3 = decode_t3(t2, m)\n",
    "    t4 = decode_t4(t3, m)\n",
    "    p = linear(t4)\n",
    "    return p\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×12 Matrix{Float32}:\n",
       "  1.40429     0.48135    0.0802765   …   0.64429     1.125        1.77723\n",
       "  1.30857     1.20231    0.666678        1.51042     0.717665     0.0233496\n",
       " -0.217263   -1.09486   -1.7194         -1.36385    -1.68779     -1.59444\n",
       "  0.22041     0.650414   0.619975       -0.0188124   0.308183    -0.346813\n",
       " -1.33163    -1.83223   -2.22717        -1.25673    -1.97198     -2.45477\n",
       "  0.129447    0.327889   0.196147    …  -0.177327    0.455111     0.819691\n",
       "  0.0698552  -0.481439  -1.02815         0.837254    0.474574    -0.0920821\n",
       " -0.382835    0.105016   0.326304       -0.727393   -0.263912     0.163892\n",
       "  0.664174    0.296703   0.00105876      0.677907    0.844249     0.636208\n",
       "  0.190935    0.70502    0.774361       -1.08557    -1.01217     -0.37921\n",
       "  ⋮                                  ⋱               ⋮           \n",
       " -0.761083   -0.580932  -0.501562       -0.437616   -0.5409      -0.621816\n",
       " -0.805079   -0.717244  -0.537191    …   0.0591887   0.00292158   0.121722\n",
       " -1.53901    -1.29082   -1.32215        -1.50627    -1.44815     -1.20742\n",
       "  1.55349     1.40004    1.26629         1.98005     1.71436      1.49331\n",
       "  1.17082     1.31564    1.42019         0.925039    0.945798     1.12047\n",
       "  0.154361    0.295138   0.284821       -0.162558   -0.293314    -0.0983384\n",
       " -1.88098    -1.65571   -1.45426     …  -1.20721    -1.06656     -1.39308\n",
       "  0.812443    0.499652   0.279688        0.528965    0.230331     0.241263\n",
       " -0.613607   -0.669043  -0.624302       -1.03175    -0.847067    -0.601821"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bleibt so\n",
    "enc = encoder_forward(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×12 Matrix{Float32}:\n",
       " -0.9885   -1.07028  -1.0814   -1.00559  …  -0.864755  -0.887681  -0.8494\n",
       " -2.91202  -2.93347  -2.91755  -2.91998     -3.27027   -3.26334   -3.2233\n",
       " -2.2474   -2.18657  -2.14223  -2.18536     -2.28324   -2.31471   -2.49593\n",
       " -2.86054  -2.7723   -2.69142  -2.60912     -2.70761   -2.74835   -2.66267\n",
       " -5.44363  -5.7986   -5.93537  -5.87527     -5.66618   -5.84295   -5.86192\n",
       " -4.98369  -4.87515  -4.84387  -4.98327  …  -5.21094   -5.25964   -5.28072\n",
       " -2.15605  -2.20097  -2.33089  -2.44396     -2.67377   -2.53275   -2.41645\n",
       " -1.82379  -1.73893  -1.73102  -1.82218     -1.77971   -1.7285    -1.84109\n",
       " -6.40014  -6.51995  -6.57264  -6.56387     -6.71331   -6.73925   -6.72285\n",
       " -3.23982  -3.12139  -3.03003  -3.00417     -3.16238   -3.17116   -3.21617\n",
       " -6.74592  -6.75522  -6.75422  -6.72352  …  -7.01639   -7.03988   -6.9207\n",
       " -2.70411  -2.64151  -2.64199  -2.67683     -2.70285   -2.73525   -2.65025\n",
       " -4.32122  -4.26884  -4.21946  -4.19459     -4.25787   -4.31835   -4.45902"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bleibt so\n",
    "probs = decoder_forward(encoded_sample, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smooth (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hier vielleicht etwas ändern --> herausfinden, was diese funktion macht\n",
    "function smooth(et)\n",
    "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
    "    p = sm .* (1 .+ -et)\n",
    "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
    "    label\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define loss function\n",
    "\n",
    "#ÄNDERN\n",
    "function loss(x, y)\n",
    "    label = onehot(vocab, y) #turn the index to one-hot encoding\n",
    "    label = smooth(label) #perform label smoothing\n",
    "    enc = encoder_forward(x)\n",
    "    probs = decoder_forward(y, enc)\n",
    "    l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])  #erstmal so lassen\n",
    "    return l\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-2.3301287 0.1544132 … -0.38240856 -0.37233922; -0.11870715 0.14345613 … -0.73466337 -0.25698587; … ; 0.6212508 -0.44518754 … 1.1487536 0.8756521; -0.8398909 -0.5842247 … 0.67612064 0.80920833], Float32[0.5403023 -0.41614684 … 0.4000682 0.9873536; 0.7617204 0.98704624 … -0.04760751 0.7300115; … ; 1.0 1.0 … 0.99303025 0.99301666; 0.0001 0.0002 … 0.10212166 0.10222114], Float32[0.039384805 0.009802005 … 0.050029453 -0.091087416; -0.082121685 -0.013584285 … 5.201001f-5 -0.08037476; … ; -0.086514026 -0.020177113 … -0.040689997 -0.077759884; -0.004750663 -0.06702606 … -0.08146825 0.0810674], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.038417783 -0.021766594 … -0.041272957 -0.021055259; 0.016781643 0.007645264 … -0.040413324 -0.07214453; … ; -0.08673716 -0.037880715 … 0.09213138 0.0022301283; 0.03261972 0.07066895 … -0.06126906 0.029178308], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.07720935 -0.024700392 … -0.059960432 -0.09288678; -0.043746352 0.07560577 … 0.035228577 0.044135768; … ; 0.035564896 0.008977509 … -0.057339825 0.07214294; 0.08060746 -0.06898748 … -0.019235302 -0.048481084], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0021600197 0.089166254 … -0.08121313 0.02514288; 0.010098368 -0.0476236 … 0.06728796 -0.08918689; … ; 0.06610168 -0.03438584 … 0.0036155384 0.020359622; 0.08087227 0.04798915 … -0.042743616 -0.0066592204], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.016532302 0.010680034 … 0.0559612 -0.013714843; 0.055821415 0.03820279 … 0.04335013 -0.055744417; … ; 0.016684905 -0.030599032 … -0.048768364 -0.045824975; -0.05880144 0.06601585 … 0.05687818 0.026442349], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.027459178 -0.047554646 … -0.06309021 -0.06805004; 0.052046645 0.02734312 … 0.004215098 0.021723036; … ; -0.030558854 0.047894213 … 0.053932395 -0.055348877; -0.015137002 0.034761496 … 0.02815058 0.04647947], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.025967538 0.020832606 … 0.014618807 -0.021760637; -0.0003432707 -0.07705986 … 0.007862907 0.054651555; … ; 0.06062292 -0.049089625 … -0.08795107 -0.067329995; 0.04172433 -0.042158302 … 0.024498655 -0.03411104], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.054132264 -0.054805417 … 0.011415054 0.07756592; 0.082583286 0.06838596 … -0.024841579 0.05308022; … ; 0.049134523 0.093286656 … 0.049310498 -0.009994255; 0.04360992 0.012808498 … -0.0902282 -0.0118208155], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.05844397 0.05268554 … -0.03145606 0.024040353; -0.012208294 -0.062358778 … -0.0095201405 0.055222858; … ; 0.079375654 -0.07699079 … 0.09648937 0.017893821; 0.08251747 -0.029589633 … -0.07654223 0.06074686], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.07630556 0.0902915 … 0.055213463 -0.091072574; -0.087266676 -0.08867933 … -0.028046738 -0.0023914678; … ; 0.0020951978 0.067075 … -0.044333212 0.08718422; 0.027388984 0.06154467 … 0.0419697 -0.016677737], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.0446563 -0.032334417 … -0.014749411 0.060482606; 0.045362227 -0.019682359 … 0.009608833 -0.07071352; … ; 0.018058937 0.028367847 … 0.015402565 0.036682397; -0.0086743375 0.038668342 … -0.04625694 0.07140004], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.014661435 -0.04369935 … 0.04418304 -0.027717497; 0.06666324 0.010059073 … 0.031944536 0.047375783; … ; -0.057335217 -0.056950256 … 0.010721105 0.0031190002; -0.009273825 -0.040887002 … -0.05766095 0.057184573], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.03149279 -0.030409165 … 0.060789105 0.048230756; -0.03124345 0.0331756 … 0.003073877 0.05821721; … ; -0.06537804 0.001838772 … -0.00058284454 -0.078565404; 0.09414068 0.029883524 … 0.03097599 0.037015107], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.057255566 -0.02575442 … -0.0106415525 -0.0104576135; -0.0142240105 -0.07624248 … -0.055514093 -0.045521524; … ; -0.041155685 0.019124795 … -0.008776626 -0.09533072; 0.085559234 -0.0057190005 … 0.062881485 0.030262507], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.09381306 -0.059199534 … -0.090688355 -0.055050783; 0.08617463 -0.061790824 … 0.060923528 0.08873009; … ; -0.06144926 0.029957304 … -0.0011008439 -0.06330022; 0.036577094 -0.0058160718 … 0.052643526 -0.0707751], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.021387933 -0.079900466 … -0.07018191 -0.052046623; -0.06866136 -0.03592359 … 0.08887465 0.023341667; … ; -0.06765394 0.09488512 … 0.07233992 0.029140403; -0.061549705 -0.03121831 … -0.03617011 0.08837662], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.034030724 -0.019072685 … 0.07187409 0.0071022767; 0.07116116 -0.04701875 … -0.0068722446 0.04016872; … ; 0.002192024 0.06446022 … -0.03436451 0.014674047; -0.063272 -0.037484113 … -0.022440128 -0.04177382], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.06726698 0.030848783 … 0.01628878 0.02294641; -0.006296313 0.007767493 … -0.0067074075 0.059767097; … ; 0.0011295471 0.053023726 … -0.063570425 -0.059994925; -0.042854447 0.063847244 … -0.048925925 0.005456349], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.078147106 0.084369704 … -0.066542506 -0.00027367007; 0.016834645 0.067040235 … 0.012993707 -0.08225183; … ; 0.0031475406 -0.01019193 … -0.01836512 -0.08275554; 0.0154032735 -0.013479849 … 0.086896904 -0.07155056], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.03335363 -0.041408833 … -0.08549093 -0.078178436; -0.0309186 -0.06412373 … 0.07581044 -0.00508964; … ; -0.014603779 0.094445236 … -8.5552194f-5 -0.063821614; 0.021738015 -0.05762388 … 0.08442511 0.08360629], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.013469185 0.032431807 … -0.046118613 0.08685884; 0.026139127 -0.0087041855 … 0.07879657 0.09440835; … ; -0.012255502 -0.057390634 … 0.018477451 0.08190704; 0.0871817 0.09504237 … 0.020957056 0.04147236], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.05853663 0.06773421 … -0.0023484377 -0.03839994; -0.06580478 -0.020089438 … 0.042915553 0.06692515; … ; -0.059252053 -0.064606614 … -0.02062175 -0.07265221; 0.031483255 0.00023696526 … -0.065850355 -0.05263152], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.012582095 -0.03113852 … -0.01672861 -0.041758623; 0.0075188777 0.05252839 … 0.027586505 -0.016538031; … ; 0.046278432 0.0016172103 … 0.049517773 -0.05569937; 0.008854248 -0.045148917 … -0.06908529 -0.054700006], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.021288438 0.06290884 … -0.052099865 -0.07126368; -0.058552325 0.00869015 … -0.029887754 0.024678525; … ; -0.06004892 -0.03394385 … -0.04855867 0.035461985; 0.051595166 -0.009881124 … -0.07028762 0.065631114], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.09353909 -0.046309084 … -0.040691126 -0.04714953; 0.009295317 -0.023173494 … -0.0346913 -0.033775598; … ; -0.025037244 -0.046928912 … -0.05514102 0.02447476; 0.03789969 -0.07832179 … 0.0017755197 -0.011923058], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.03957918 -0.026421316 … 0.023377264 0.022072166; 0.06826708 -0.04252182 … -0.026782084 0.043554403; … ; 0.028733002 0.08911353 … -0.04128115 -0.03990218; -0.069433086 0.028721506 … -0.04801789 0.08276454], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.079093814 -0.09521952 … 0.018286908 0.09533606; -0.09650204 0.033173844 … 0.0629489 0.016820563; … ; -0.003815314 0.07155878 … -0.045629192 0.030902717; 0.066240415 -0.013925779 … 0.06139956 0.03208609], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.033227447 0.07506245 … 0.039962802 0.020121226; -0.0631164 -0.030313225 … -0.07535773 0.028173288; … ; -0.0021032775 -0.0062883403 … -0.07148036 0.017717315; 0.040273014 0.046124224 … 0.051032808 -0.053449392], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.032968484 0.03186032 … 0.06664165 0.03604908; 0.03220352 0.070360266 … 0.03732347 0.08453787; … ; 0.078729674 0.08822294 … 0.06927899 0.070506595; 0.004169873 0.0070817643 … 0.043459963 0.06533235], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.07309246 0.013701002 … -0.08795336 -0.06664322; 0.037906595 -0.04202538 … 0.038162652 0.006207382; … ; 0.054855924 0.02153212 … -0.08024551 0.038439713; -0.07086573 -0.08236391 … -0.018555615 -0.05125112], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.092522345 -0.049643565 … 0.029299803 -0.006121945; 0.0539272 0.08698675 … -0.03723589 -0.026400147; … ; -0.062080167 0.02308665 … -0.007455576 -0.06096672; 0.046389326 0.073700145 … 0.018881897 0.079539694], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.038211454 0.07962495 … 0.088174835 -0.052736603; 0.06817007 -0.045210063 … 0.04998453 0.00727069; … ; -0.01311114 -0.001734752 … 0.061620064 -0.077689335; -0.04775297 0.033695813 … -0.095929936 0.090723604], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.0199982 -0.023614379 … -0.0194961 -0.048557587; -0.030835189 0.035431236 … -0.050486647 -0.0073024733; … ; -0.0367127 0.06891504 … 0.053630956 0.022217324; -0.011594638 -0.046962213 … 0.004479027 -0.05869605], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.02360543 -0.049519632 … 0.04300134 0.02856135; -0.021865986 0.011206652 … -0.011489835 0.01620514; … ; 0.036163367 -0.007934515 … 0.06967971 0.005279846; 0.025542697 0.0034518233 … -0.034123413 -0.030086454], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.07493611 -0.095935866 … 0.001037984 -0.09102297; -0.022820713 -0.05810811 … -0.007602234 0.036158707; … ; -0.029240822 -0.08611771 … 0.03191316 -0.05423605; 0.09658483 0.06621211 … -0.042595528 -0.09133018], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.006348638 -0.013809847 … -0.054464385 0.053816073; -0.049570594 0.023878874 … -0.087264694 -0.04450951; … ; 0.039093196 0.037504435 … 0.038856324 -0.043999176; 0.07919169 -0.06999668 … -0.028400512 -0.045656294], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.0046385406 -0.020084914 … -0.058228288 -0.00075634976; 0.07013602 0.07040791 … 0.06549586 -0.05427373; … ; -0.02896018 0.022044556 … -0.036771286 -0.06330833; 0.08377737 -0.016675292 … 0.035862483 0.058379676], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.08160982 -0.081818394 … 0.04659215 -0.018369552; 0.020194428 -0.065370515 … -0.07042275 0.049653076; … ; -0.068491414 -0.082100764 … -0.016056873 -0.07255659; 0.010416037 0.010601316 … -0.07905727 0.08619312], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.015593099 -0.017321156 … -0.02457398 -0.0324949; 0.046687514 -0.0076084896 … -0.06417832 0.082551084; … ; -0.013422877 0.05695943 … 0.05442597 -0.07310689; 0.08628539 -0.052322738 … -0.091673404 -0.07215871], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0762277 -0.076310135 … 0.009591218 -0.0716161; 0.05735206 0.021730673 … -0.0946934 -0.0018876656; … ; 0.07367538 -0.024995945 … -0.08658406 -0.013836163; -0.063577056 -0.094429635 … 0.06895925 -0.04356509], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0098680975 -0.014992665 … 0.055448536 -0.014756807; 0.012394519 -0.035840277 … -0.00765459 -0.051546536; … ; 0.0061509167 -0.08618749 … 0.09188431 -0.036445096; -0.005808131 -0.008916289 … -0.0032768154 -0.09648143], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.008626806 0.048259772 … 0.016498739 -0.051900174; -0.05298149 0.050778784 … 0.056777246 0.055480853; … ; -0.032536544 0.05768868 … -0.033636417 0.05474835; -0.04516558 -0.046453595 … -0.05115518 -0.06548656], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.03325673 0.04706433 … 0.0075924178 -0.019995326; -0.01819929 0.017359463 … 0.04052892 0.037704304; … ; 0.06461224 0.047297824 … -0.003675317 -0.056902714; 0.03476411 -0.0672066 … 0.020236233 -0.0020952378], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.06644439 -0.05640091 … -0.0041122558 0.020357778; 0.05909806 0.06039604 … 0.039245445 -0.014323811; … ; 0.006311764 0.0019688916 … 0.0671664 -0.044846945; 0.053721994 0.039348647 … 0.04616511 -0.053600468], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.07661202 0.021916114 … -0.085036665 0.039769374; -0.021028364 -0.06548737 … 0.05950298 0.005032159; … ; -0.010871777 0.06116234 … -0.08146991 -0.061800726; 0.012090539 -0.016992869 … 0.015461701 -0.01578371], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.04822722 0.04851204 … 0.00068280165 0.034713715; -0.024315361 0.03987536 … 0.08857607 -0.063522756; … ; -0.051404774 0.033520762 … 0.0316575 -0.053607017; -0.024747208 0.080154374 … -0.016536528 0.05472012], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.047233354 -0.03606764 … -0.05252905 0.00046375016; 0.08645232 0.06473349 … -0.087845504 0.08753668; … ; 0.05897395 -0.023205075 … -0.024710549 -0.016744314; 0.041393157 0.051653672 … 0.0146989105 -0.007877358], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.081711374 -0.060040973 … 0.0029351606 0.010540972; -0.007095223 0.02788376 … 0.052798126 -0.047156483; … ; -0.083738334 0.024043398 … -0.008504064 -0.04001509; 0.092677824 0.003450967 … 0.07938627 -0.030198378], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.070405394 -0.032603834 … -0.062588036 0.08720135; -0.059099946 0.065874636 … 0.05645519 -0.042067025; … ; 0.036246613 0.054465815 … 0.017263422 -0.06678743; 0.025358954 -0.017777843 … 0.09057992 0.008696776], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.023130188 0.094866514 … 0.05398565 -0.09222019; -0.008383284 0.068596266 … -0.032034796 0.08055155; … ; 0.07259183 -0.08759908 … -0.0752827 -0.09114806; -0.059107378 0.070102476 … 0.03538816 -0.059165277], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.034241863 0.038481336 … 0.028291829 0.07621881; 0.007159283 -0.034519225 … -0.070297055 -0.012666804; … ; -0.034584787 0.026819598 … -0.0795487 0.080497205; -0.043487802 -0.07007156 … -0.040663633 0.056581467], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.086769246 0.08119508 … -0.030953875 -0.061735075; -0.06587939 0.09227998 … -0.016013335 -0.05053875; … ; 4.563861f-5 -0.05072144 … 0.021072365 -0.030950388; 0.08930322 -0.016734296 … -0.07128739 0.0602519], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.00062263 -0.03919398 … 0.050570853 -0.034678217; -0.06839513 0.059333734 … 0.05953999 0.038478073; … ; 0.06534764 0.056548744 … 0.051317472 0.043902334; -0.027823333 -0.054048024 … 0.0375033 0.059877887], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.05562363 0.0680382 … 0.07172142 -0.015971614; 0.039573602 0.0370698 … -0.06727677 0.012362972; … ; 0.04043714 0.017762506 … -0.013842721 0.03325308; -0.06116427 0.050177637 … -0.039014433 -0.06150045], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.02756452 0.07122075 … 0.030383795 -0.053119026; 0.024859214 0.074409425 … 0.07914398 -0.025525535; … ; 0.036743607 -0.09519316 … 0.05388117 -0.06974826; -0.086002305 0.0038671624 … -0.0793402 0.026546644], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.016153852 -0.06411936 … -0.027906844 0.0033025548; -0.0229423 -0.068453 … -0.08194603 -0.039736386; … ; 0.035161328 0.048113924 … 0.06529833 -0.051089965; 0.028407231 0.04539518 … 0.02218055 -0.087427676], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.021851638 0.0084192045 … 0.037983283 0.03590727; -0.03755686 0.030373685 … -0.05203434 0.029444821; … ; 0.012568878 0.015317006 … 0.03900781 0.00319371; -0.023103757 0.02202812 … 0.05469816 0.09303896], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.029452695 -0.0067265816 … 0.0745975 -0.012255063; 0.015323631 0.012387662 … -0.018863544 0.016674645; … ; -0.06752125 -0.067166135 … -0.03215373 0.0594389; 0.015877757 0.0208149 … -0.057385232 0.06884539], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.05239211 -0.08429569 … 0.08371513 -0.06200482; -0.08284606 -0.054276403 … 0.08043695 -0.09050547; … ; 0.034782346 -0.08636178 … -0.03419454 -0.059882704; 0.051635344 -0.09582382 … 0.0018825408 -0.042849924], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0576493 -0.047073122 … 0.07914241 -0.0055886176; -0.07411147 -0.05843972 … 0.008389056 0.0386219; … ; 0.061172914 -0.092684425 … -0.007667656 0.052454185; 0.077903934 0.008011873 … -0.0015674335 -0.009465452], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.05127776 0.017137425 … -0.016088452 0.0018089465; 0.054499704 3.254954f-6 … 0.007061796 -0.014646393; … ; 0.03816565 0.07214463 … -0.0202984 0.019693742; 0.046515256 -0.03715567 … 0.07578606 -0.024039475], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.07541738 0.066986375 … -0.09496548 -0.06240601; -0.048083868 -0.061606538 … 0.065960765 0.085502885; … ; 0.055239387 0.0754898 … 0.036180384 -0.024865402; -0.05723571 0.090957016 … 0.06761848 0.081717424], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.027100528 0.06975058 … 0.0076484075 0.017582664; 0.024233894 0.014841173 … -0.04120482 -0.019552657; … ; 0.0669713 0.02510557 … -0.057492826 -0.048722837; -0.07024461 0.05497925 … -0.037407596 0.06291237], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0032272455 0.005621203 … -0.05520023 0.04350877; -0.041636873 0.003973383 … 0.013170811 -0.0120272245; … ; 0.04079438 0.041280217 … 0.05300449 0.022123342; -0.050198577 -0.03246804 … -0.021839729 0.007758253], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.084598735 -0.105287194 … 0.18900901 0.036905676; 0.17993906 -0.177523 … 0.12156245 0.12802625; … ; 0.16052353 -0.04815591 … -0.030232664 -0.17728944; -0.13353512 0.05606895 … -0.118190385 0.18586461], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collect all the parameters\n",
    "\n",
    "#Layer neu einfügen\n",
    "ps = params(embed, pe, encode_t1, encode_t2, encode_t3, encode_t4, decode_t1, decode_t2, decode_t3, decode_t4, linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = ADAM(1e-4) #bleibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define training loop\n",
    "\n",
    "#ÄNDERN siehe Paper Kapitel 4.1\n",
    "function train!()\n",
    "    @info \"start training\"\n",
    "    losses =[]\n",
    "    for i = 1:100\n",
    "      data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
    "      x, y = preprocess.(data[1]), preprocess.(data[2])\n",
    "      x, y = vocab(x), vocab(y) #encode the data\n",
    "      x, y = todevice(x, y) #move to gpu\n",
    "      grad = gradient(()->loss(x, y), ps)\n",
    "      if i % 8 == 0\n",
    "          l = loss(x, y)\n",
    "          println(\"loss = $l\")\n",
    "      end\n",
    "      update!(opt, ps, grad)\n",
    "      push!(losses, loss(x,y))\n",
    "    end\n",
    "    return losses\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Vector{Any}:\n",
       " 99.00599f0\n",
       " 80.52509f0\n",
       " 83.8895f0\n",
       " 81.69281f0\n",
       " 80.88852f0\n",
       " 82.875854f0\n",
       " 81.38764f0\n",
       " 78.958565f0\n",
       " 77.14153f0\n",
       " 77.25515f0\n",
       "  ⋮\n",
       " 75.74791f0\n",
       " 76.29076f0\n",
       " 75.29016f0\n",
       " 75.379684f0\n",
       " 75.38267f0\n",
       " 74.92147f0\n",
       " 75.086075f0\n",
       " 74.31642f0\n",
       " 74.1831f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Vector{Any}:\n",
       " 74.87053f0\n",
       " 73.12973f0\n",
       " 72.77963f0\n",
       " 72.07547f0\n",
       " 72.844315f0\n",
       " 71.283485f0\n",
       " 70.81355f0\n",
       " 70.199326f0\n",
       " 70.12751f0\n",
       " 68.63194f0\n",
       "  ⋮\n",
       " 60.346947f0\n",
       " 58.115494f0\n",
       " 58.716503f0\n",
       " 59.116623f0\n",
       " 57.617023f0\n",
       " 58.764233f0\n",
       " 57.859787f0\n",
       " 58.396076f0\n",
       " 58.11123f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip890\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip890)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip891\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip890)\" d=\"\nM141.02 1486.45 L2352.76 1486.45 L2352.76 47.2441 L141.02 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip892\">\n    <rect x=\"141\" y=\"47\" width=\"2213\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  182.325,1486.45 182.325,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  714.606,1486.45 714.606,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1246.89,1486.45 1246.89,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1779.17,1486.45 1779.17,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2311.45,1486.45 2311.45,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  141.02,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  182.325,1486.45 182.325,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  714.606,1486.45 714.606,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1246.89,1486.45 1246.89,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1779.17,1486.45 1779.17,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2311.45,1486.45 2311.45,1467.55 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"M182.325 1517.37 Q178.714 1517.37 176.885 1520.93 Q175.079 1524.47 175.079 1531.6 Q175.079 1538.71 176.885 1542.27 Q178.714 1545.82 182.325 1545.82 Q185.959 1545.82 187.764 1542.27 Q189.593 1538.71 189.593 1531.6 Q189.593 1524.47 187.764 1520.93 Q185.959 1517.37 182.325 1517.37 M182.325 1513.66 Q188.135 1513.66 191.19 1518.27 Q194.269 1522.85 194.269 1531.6 Q194.269 1540.33 191.19 1544.94 Q188.135 1549.52 182.325 1549.52 Q176.515 1549.52 173.436 1544.94 Q170.38 1540.33 170.38 1531.6 Q170.38 1522.85 173.436 1518.27 Q176.515 1513.66 182.325 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M693.877 1544.91 L710.197 1544.91 L710.197 1548.85 L688.252 1548.85 L688.252 1544.91 Q690.914 1542.16 695.498 1537.53 Q700.104 1532.88 701.285 1531.53 Q703.53 1529.01 704.41 1527.27 Q705.312 1525.51 705.312 1523.82 Q705.312 1521.07 703.368 1519.33 Q701.447 1517.6 698.345 1517.6 Q696.146 1517.6 693.692 1518.36 Q691.261 1519.13 688.484 1520.68 L688.484 1515.95 Q691.308 1514.82 693.761 1514.24 Q696.215 1513.66 698.252 1513.66 Q703.622 1513.66 706.817 1516.35 Q710.011 1519.03 710.011 1523.52 Q710.011 1525.65 709.201 1527.57 Q708.414 1529.47 706.308 1532.07 Q705.729 1532.74 702.627 1535.95 Q699.525 1539.15 693.877 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M720.058 1514.29 L738.414 1514.29 L738.414 1518.22 L724.34 1518.22 L724.34 1526.7 Q725.358 1526.35 726.377 1526.19 Q727.395 1526 728.414 1526 Q734.201 1526 737.581 1529.17 Q740.96 1532.34 740.96 1537.76 Q740.96 1543.34 737.488 1546.44 Q734.016 1549.52 727.696 1549.52 Q725.521 1549.52 723.252 1549.15 Q721.007 1548.78 718.599 1548.04 L718.599 1543.34 Q720.683 1544.47 722.905 1545.03 Q725.127 1545.58 727.604 1545.58 Q731.608 1545.58 733.946 1543.48 Q736.284 1541.37 736.284 1537.76 Q736.284 1534.15 733.946 1532.04 Q731.608 1529.94 727.604 1529.94 Q725.729 1529.94 723.854 1530.35 Q722.002 1530.77 720.058 1531.65 L720.058 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1221.59 1514.29 L1239.94 1514.29 L1239.94 1518.22 L1225.87 1518.22 L1225.87 1526.7 Q1226.89 1526.35 1227.91 1526.19 Q1228.92 1526 1229.94 1526 Q1235.73 1526 1239.11 1529.17 Q1242.49 1532.34 1242.49 1537.76 Q1242.49 1543.34 1239.02 1546.44 Q1235.55 1549.52 1229.23 1549.52 Q1227.05 1549.52 1224.78 1549.15 Q1222.54 1548.78 1220.13 1548.04 L1220.13 1543.34 Q1222.21 1544.47 1224.43 1545.03 Q1226.66 1545.58 1229.13 1545.58 Q1233.14 1545.58 1235.48 1543.48 Q1237.81 1541.37 1237.81 1537.76 Q1237.81 1534.15 1235.48 1532.04 Q1233.14 1529.94 1229.13 1529.94 Q1227.26 1529.94 1225.38 1530.35 Q1223.53 1530.77 1221.59 1531.65 L1221.59 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1261.7 1517.37 Q1258.09 1517.37 1256.26 1520.93 Q1254.46 1524.47 1254.46 1531.6 Q1254.46 1538.71 1256.26 1542.27 Q1258.09 1545.82 1261.7 1545.82 Q1265.34 1545.82 1267.14 1542.27 Q1268.97 1538.71 1268.97 1531.6 Q1268.97 1524.47 1267.14 1520.93 Q1265.34 1517.37 1261.7 1517.37 M1261.7 1513.66 Q1267.51 1513.66 1270.57 1518.27 Q1273.65 1522.85 1273.65 1531.6 Q1273.65 1540.33 1270.57 1544.94 Q1267.51 1549.52 1261.7 1549.52 Q1255.89 1549.52 1252.81 1544.94 Q1249.76 1540.33 1249.76 1531.6 Q1249.76 1522.85 1252.81 1518.27 Q1255.89 1513.66 1261.7 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1753.02 1514.29 L1775.25 1514.29 L1775.25 1516.28 L1762.7 1548.85 L1757.82 1548.85 L1769.62 1518.22 L1753.02 1518.22 L1753.02 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1784.41 1514.29 L1802.77 1514.29 L1802.77 1518.22 L1788.69 1518.22 L1788.69 1526.7 Q1789.71 1526.35 1790.73 1526.19 Q1791.75 1526 1792.77 1526 Q1798.56 1526 1801.94 1529.17 Q1805.32 1532.34 1805.32 1537.76 Q1805.32 1543.34 1801.84 1546.44 Q1798.37 1549.52 1792.05 1549.52 Q1789.88 1549.52 1787.61 1549.15 Q1785.36 1548.78 1782.95 1548.04 L1782.95 1543.34 Q1785.04 1544.47 1787.26 1545.03 Q1789.48 1545.58 1791.96 1545.58 Q1795.96 1545.58 1798.3 1543.48 Q1800.64 1541.37 1800.64 1537.76 Q1800.64 1534.15 1798.3 1532.04 Q1795.96 1529.94 1791.96 1529.94 Q1790.08 1529.94 1788.21 1530.35 Q1786.36 1530.77 1784.41 1531.65 L1784.41 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2271.06 1544.91 L2278.7 1544.91 L2278.7 1518.55 L2270.39 1520.21 L2270.39 1515.95 L2278.65 1514.29 L2283.33 1514.29 L2283.33 1544.91 L2290.96 1544.91 L2290.96 1548.85 L2271.06 1548.85 L2271.06 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2310.41 1517.37 Q2306.8 1517.37 2304.97 1520.93 Q2303.16 1524.47 2303.16 1531.6 Q2303.16 1538.71 2304.97 1542.27 Q2306.8 1545.82 2310.41 1545.82 Q2314.04 1545.82 2315.85 1542.27 Q2317.68 1538.71 2317.68 1531.6 Q2317.68 1524.47 2315.85 1520.93 Q2314.04 1517.37 2310.41 1517.37 M2310.41 1513.66 Q2316.22 1513.66 2319.27 1518.27 Q2322.35 1522.85 2322.35 1531.6 Q2322.35 1540.33 2319.27 1544.94 Q2316.22 1549.52 2310.41 1549.52 Q2304.6 1549.52 2301.52 1544.94 Q2298.46 1540.33 2298.46 1531.6 Q2298.46 1522.85 2301.52 1518.27 Q2304.6 1513.66 2310.41 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2340.57 1517.37 Q2336.96 1517.37 2335.13 1520.93 Q2333.33 1524.47 2333.33 1531.6 Q2333.33 1538.71 2335.13 1542.27 Q2336.96 1545.82 2340.57 1545.82 Q2344.21 1545.82 2346.01 1542.27 Q2347.84 1538.71 2347.84 1531.6 Q2347.84 1524.47 2346.01 1520.93 Q2344.21 1517.37 2340.57 1517.37 M2340.57 1513.66 Q2346.38 1513.66 2349.44 1518.27 Q2352.52 1522.85 2352.52 1531.6 Q2352.52 1540.33 2349.44 1544.94 Q2346.38 1549.52 2340.57 1549.52 Q2334.76 1549.52 2331.68 1544.94 Q2328.63 1540.33 2328.63 1531.6 Q2328.63 1522.85 2331.68 1518.27 Q2334.76 1513.66 2340.57 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  141.02,1237.15 2352.76,1237.15 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  141.02,974.574 2352.76,974.574 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  141.02,712.001 2352.76,712.001 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  141.02,449.428 2352.76,449.428 \n  \"/>\n<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  141.02,186.855 2352.76,186.855 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  141.02,1486.45 141.02,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  141.02,1237.15 159.917,1237.15 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  141.02,974.574 159.917,974.574 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  141.02,712.001 159.917,712.001 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  141.02,449.428 159.917,449.428 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  141.02,186.855 159.917,186.855 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"M63.4921 1235.28 Q60.3439 1235.28 58.4921 1237.44 Q56.6634 1239.59 56.6634 1243.34 Q56.6634 1247.07 58.4921 1249.24 Q60.3439 1251.39 63.4921 1251.39 Q66.6402 1251.39 68.4689 1249.24 Q70.3207 1247.07 70.3207 1243.34 Q70.3207 1239.59 68.4689 1237.44 Q66.6402 1235.28 63.4921 1235.28 M72.7744 1220.63 L72.7744 1224.89 Q71.0152 1224.06 69.2096 1223.62 Q67.4272 1223.18 65.668 1223.18 Q61.0384 1223.18 58.5847 1226.3 Q56.1541 1229.43 55.8069 1235.75 Q57.1726 1233.73 59.2328 1232.67 Q61.293 1231.58 63.7698 1231.58 Q68.9781 1231.58 71.9874 1234.75 Q75.0198 1237.9 75.0198 1243.34 Q75.0198 1248.66 71.8716 1251.88 Q68.7235 1255.1 63.4921 1255.1 Q57.4967 1255.1 54.3254 1250.52 Q51.1542 1245.91 51.1542 1237.18 Q51.1542 1228.99 55.043 1224.13 Q58.9319 1219.24 65.4828 1219.24 Q67.242 1219.24 69.0244 1219.59 Q70.83 1219.94 72.7744 1220.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M93.0752 1222.95 Q89.4641 1222.95 87.6354 1226.51 Q85.8299 1230.05 85.8299 1237.18 Q85.8299 1244.29 87.6354 1247.85 Q89.4641 1251.39 93.0752 1251.39 Q96.7095 1251.39 98.515 1247.85 Q100.344 1244.29 100.344 1237.18 Q100.344 1230.05 98.515 1226.51 Q96.7095 1222.95 93.0752 1222.95 M93.0752 1219.24 Q98.8854 1219.24 101.941 1223.85 Q105.02 1228.43 105.02 1237.18 Q105.02 1245.91 101.941 1250.52 Q98.8854 1255.1 93.0752 1255.1 Q87.2651 1255.1 84.1864 1250.52 Q81.1309 1245.91 81.1309 1237.18 Q81.1309 1228.43 84.1864 1223.85 Q87.2651 1219.24 93.0752 1219.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M64.1402 972.711 Q60.9921 972.711 59.1402 974.864 Q57.3115 977.016 57.3115 980.766 Q57.3115 984.493 59.1402 986.669 Q60.9921 988.822 64.1402 988.822 Q67.2883 988.822 69.117 986.669 Q70.9689 984.493 70.9689 980.766 Q70.9689 977.016 69.117 974.864 Q67.2883 972.711 64.1402 972.711 M73.4226 958.058 L73.4226 962.317 Q71.6633 961.484 69.8578 961.044 Q68.0754 960.604 66.3161 960.604 Q61.6865 960.604 59.2328 963.729 Q56.8023 966.854 56.4551 973.174 Q57.8208 971.16 59.881 970.095 Q61.9411 969.007 64.418 969.007 Q69.6263 969.007 72.6355 972.178 Q75.6679 975.327 75.6679 980.766 Q75.6679 986.09 72.5198 989.308 Q69.3717 992.526 64.1402 992.526 Q58.1449 992.526 54.9736 987.942 Q51.8023 983.336 51.8023 974.609 Q51.8023 966.415 55.6912 961.553 Q59.58 956.669 66.1309 956.669 Q67.8902 956.669 69.6726 957.016 Q71.4781 957.364 73.4226 958.058 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M97.89 973.22 Q101.246 973.938 103.121 976.206 Q105.02 978.475 105.02 981.808 Q105.02 986.924 101.501 989.725 Q97.9826 992.526 91.5012 992.526 Q89.3253 992.526 87.0105 992.086 Q84.7188 991.669 82.2651 990.813 L82.2651 986.299 Q84.2095 987.433 86.5243 988.012 Q88.8391 988.59 91.3623 988.59 Q95.7604 988.59 98.0521 986.854 Q100.367 985.118 100.367 981.808 Q100.367 978.752 98.2141 977.039 Q96.0845 975.303 92.2651 975.303 L88.2373 975.303 L88.2373 971.461 L92.4502 971.461 Q95.8993 971.461 97.728 970.095 Q99.5567 968.706 99.5567 966.114 Q99.5567 963.452 97.6585 962.04 Q95.7836 960.604 92.2651 960.604 Q90.3438 960.604 88.1447 961.021 Q85.9456 961.438 83.3068 962.317 L83.3068 958.151 Q85.9688 957.41 88.2836 957.04 Q90.6215 956.669 92.6817 956.669 Q98.0058 956.669 101.108 959.1 Q104.209 961.507 104.209 965.628 Q104.209 968.498 102.566 970.489 Q100.922 972.456 97.89 973.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M63.33 710.138 Q60.1819 710.138 58.33 712.291 Q56.5014 714.443 56.5014 718.193 Q56.5014 721.92 58.33 724.096 Q60.1819 726.249 63.33 726.249 Q66.4782 726.249 68.3068 724.096 Q70.1587 721.92 70.1587 718.193 Q70.1587 714.443 68.3068 712.291 Q66.4782 710.138 63.33 710.138 M72.6124 695.485 L72.6124 699.744 Q70.8531 698.911 69.0476 698.471 Q67.2652 698.031 65.5059 698.031 Q60.8763 698.031 58.4226 701.156 Q55.9921 704.281 55.6449 710.601 Q57.0106 708.587 59.0708 707.522 Q61.131 706.434 63.6078 706.434 Q68.8161 706.434 71.8253 709.605 Q74.8577 712.754 74.8577 718.193 Q74.8577 723.517 71.7096 726.735 Q68.5615 729.952 63.33 729.952 Q57.3347 729.952 54.1634 725.369 Q50.9921 720.763 50.9921 712.036 Q50.9921 703.842 54.881 698.98 Q58.7699 694.096 65.3208 694.096 Q67.08 694.096 68.8624 694.443 Q70.6679 694.791 72.6124 695.485 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M93.4919 710.138 Q90.3438 710.138 88.4919 712.291 Q86.6632 714.443 86.6632 718.193 Q86.6632 721.92 88.4919 724.096 Q90.3438 726.249 93.4919 726.249 Q96.64 726.249 98.4687 724.096 Q100.321 721.92 100.321 718.193 Q100.321 714.443 98.4687 712.291 Q96.64 710.138 93.4919 710.138 M102.774 695.485 L102.774 699.744 Q101.015 698.911 99.2095 698.471 Q97.4271 698.031 95.6678 698.031 Q91.0382 698.031 88.5845 701.156 Q86.154 704.281 85.8068 710.601 Q87.1725 708.587 89.2327 707.522 Q91.2928 706.434 93.7697 706.434 Q98.978 706.434 101.987 709.605 Q105.02 712.754 105.02 718.193 Q105.02 723.517 101.871 726.735 Q98.7234 729.952 93.4919 729.952 Q87.4966 729.952 84.3253 725.369 Q81.154 720.763 81.154 712.036 Q81.154 703.842 85.0429 698.98 Q88.9317 694.096 95.4826 694.096 Q97.2419 694.096 99.0243 694.443 Q100.83 694.791 102.774 695.485 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M63.6772 447.565 Q60.5291 447.565 58.6773 449.718 Q56.8486 451.87 56.8486 455.62 Q56.8486 459.347 58.6773 461.523 Q60.5291 463.676 63.6772 463.676 Q66.8254 463.676 68.6541 461.523 Q70.5059 459.347 70.5059 455.62 Q70.5059 451.87 68.6541 449.718 Q66.8254 447.565 63.6772 447.565 M72.9596 432.912 L72.9596 437.171 Q71.2004 436.338 69.3948 435.898 Q67.6124 435.458 65.8532 435.458 Q61.2236 435.458 58.7699 438.583 Q56.3393 441.708 55.9921 448.028 Q57.3578 446.014 59.418 444.949 Q61.4782 443.861 63.955 443.861 Q69.1633 443.861 72.1726 447.032 Q75.205 450.18 75.205 455.62 Q75.205 460.944 72.0568 464.162 Q68.9087 467.379 63.6772 467.379 Q57.6819 467.379 54.5106 462.796 Q51.3393 458.19 51.3393 449.463 Q51.3393 441.269 55.2282 436.407 Q59.1171 431.523 65.668 431.523 Q67.4272 431.523 69.2096 431.87 Q71.0152 432.218 72.9596 432.912 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M83.3994 465.991 L83.3994 461.731 Q85.1586 462.565 86.9642 463.005 Q88.7697 463.444 90.5058 463.444 Q95.1354 463.444 97.566 460.342 Q100.02 457.217 100.367 450.875 Q99.0243 452.866 96.9641 453.93 Q94.9039 454.995 92.4039 454.995 Q87.2188 454.995 84.1864 451.87 Q81.1771 448.722 81.1771 443.282 Q81.1771 437.958 84.3253 434.741 Q87.4734 431.523 92.7049 431.523 Q98.7002 431.523 101.848 436.13 Q105.02 440.713 105.02 449.463 Q105.02 457.634 101.131 462.518 Q97.265 467.379 90.7141 467.379 Q88.9549 467.379 87.1493 467.032 Q85.3438 466.685 83.3994 465.991 M92.7049 451.338 Q95.853 451.338 97.6817 449.185 Q99.5335 447.032 99.5335 443.282 Q99.5335 439.556 97.6817 437.403 Q95.853 435.227 92.7049 435.227 Q89.5567 435.227 87.7049 437.403 Q85.8762 439.556 85.8762 443.282 Q85.8762 447.032 87.7049 449.185 Q89.5567 451.338 92.7049 451.338 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M53.3301 169.575 L75.5522 169.575 L75.5522 171.566 L63.006 204.135 L58.1217 204.135 L69.9272 173.51 L53.3301 173.51 L53.3301 169.575 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M88.7003 200.2 L105.02 200.2 L105.02 204.135 L83.0753 204.135 L83.0753 200.2 Q85.7373 197.445 90.3206 192.816 Q94.9271 188.163 96.1076 186.82 Q98.353 184.297 99.2326 182.561 Q100.135 180.802 100.135 179.112 Q100.135 176.358 98.1909 174.621 Q96.2697 172.885 93.1678 172.885 Q90.9688 172.885 88.5151 173.649 Q86.0845 174.413 83.3068 175.964 L83.3068 171.242 Q86.1308 170.108 88.5845 169.529 Q91.0382 168.95 93.0752 168.95 Q98.4456 168.95 101.64 171.635 Q104.834 174.321 104.834 178.811 Q104.834 180.941 104.024 182.862 Q103.237 184.76 101.131 187.353 Q100.552 188.024 97.4502 191.242 Q94.3484 194.436 88.7003 200.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip892)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  203.616,87.9763 224.907,118.618 246.198,180.25 267.49,112.957 288.781,249.568 310.072,290.698 331.363,344.458 352.655,350.744 373.946,481.642 395.237,448.74 \n  416.529,418.418 437.82,523.197 459.111,548.592 480.402,564.375 501.694,640.239 522.985,618.291 544.276,490.983 565.567,599.878 586.859,738.887 608.15,704.958 \n  629.441,754.824 650.732,751.087 672.024,771.273 693.315,834.61 714.606,768.29 735.897,767.032 757.189,754.702 778.48,831.618 799.771,815.769 821.063,697.829 \n  842.354,814.459 863.645,759.594 884.936,790.339 906.228,848.358 927.519,830.063 948.81,857.61 970.101,893.104 991.393,840.383 1012.68,805.568 1033.98,870.588 \n  1055.27,902.558 1076.56,800.636 1097.85,884.474 1119.14,931.048 1140.43,832.144 1161.72,1015.6 1183.01,961.981 1204.31,1018.78 1225.6,958.285 1246.89,1028.76 \n  1268.18,963.786 1289.47,988.956 1310.76,984.314 1332.05,939.373 1353.34,948.796 1374.64,1078.89 1395.93,999.17 1417.22,1106.87 1438.51,999.05 1459.8,1130.34 \n  1481.09,1085.47 1502.38,966.174 1523.67,1150.86 1544.97,1041.42 1566.26,1064.99 1587.55,1162.35 1608.84,1176.4 1630.13,1058.63 1651.42,1193.22 1672.71,1131.79 \n  1694,1169.54 1715.3,1208.58 1736.59,1209.1 1757.88,1170.44 1779.17,1129.78 1800.46,1178.28 1821.75,1191.87 1843.04,1196.75 1864.33,1252.86 1885.63,1274.19 \n  1906.92,1147.02 1928.21,1112.19 1949.5,1242.15 1970.79,1250.01 1992.08,1069.18 2013.37,1313.35 2034.66,1298.5 2055.96,1166.12 2077.25,1269.53 2098.54,1305.33 \n  2119.83,1206.78 2141.12,1402.09 2162.41,1349.48 2183.7,1314.46 2204.99,1445.72 2226.29,1345.31 2247.58,1424.47 2268.87,1377.53 2290.16,1402.46 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"\nM1888.15 198.898 L2279.03 198.898 L2279.03 95.2176 L1888.15 95.2176  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1888.15,198.898 2279.03,198.898 2279.03,95.2176 1888.15,95.2176 1888.15,198.898 \n  \"/>\n<polyline clip-path=\"url(#clip890)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1912.15,147.058 2056.15,147.058 \n  \"/>\n<path clip-path=\"url(#clip890)\" d=\"M2080.15 129.778 L2084.83 129.778 L2084.83 160.402 L2101.66 160.402 L2101.66 164.338 L2080.15 164.338 L2080.15 129.778 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2115.59 141.398 Q2112.17 141.398 2110.18 144.083 Q2108.18 146.745 2108.18 151.398 Q2108.18 156.051 2110.15 158.736 Q2112.14 161.398 2115.59 161.398 Q2118.99 161.398 2120.99 158.713 Q2122.98 156.027 2122.98 151.398 Q2122.98 146.791 2120.99 144.106 Q2118.99 141.398 2115.59 141.398 M2115.59 137.787 Q2121.15 137.787 2124.32 141.398 Q2127.49 145.009 2127.49 151.398 Q2127.49 157.764 2124.32 161.398 Q2121.15 165.009 2115.59 165.009 Q2110.01 165.009 2106.84 161.398 Q2103.69 157.764 2103.69 151.398 Q2103.69 145.009 2106.84 141.398 Q2110.01 137.787 2115.59 137.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2151.08 139.176 L2151.08 143.203 Q2149.27 142.277 2147.33 141.815 Q2145.38 141.352 2143.3 141.352 Q2140.13 141.352 2138.53 142.324 Q2136.96 143.296 2136.96 145.24 Q2136.96 146.722 2138.09 147.578 Q2139.23 148.412 2142.65 149.176 L2144.11 149.5 Q2148.65 150.472 2150.55 152.254 Q2152.47 154.014 2152.47 157.185 Q2152.47 160.796 2149.6 162.902 Q2146.75 165.009 2141.75 165.009 Q2139.67 165.009 2137.4 164.592 Q2135.15 164.199 2132.65 163.388 L2132.65 158.99 Q2135.01 160.217 2137.31 160.842 Q2139.6 161.444 2141.84 161.444 Q2144.85 161.444 2146.47 160.426 Q2148.09 159.384 2148.09 157.509 Q2148.09 155.773 2146.91 154.847 Q2145.75 153.921 2141.8 153.064 L2140.31 152.717 Q2136.36 151.884 2134.6 150.171 Q2132.84 148.435 2132.84 145.426 Q2132.84 141.768 2135.43 139.778 Q2138.02 137.787 2142.79 137.787 Q2145.15 137.787 2147.24 138.134 Q2149.32 138.481 2151.08 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2175.78 139.176 L2175.78 143.203 Q2173.97 142.277 2172.03 141.815 Q2170.08 141.352 2168 141.352 Q2164.83 141.352 2163.23 142.324 Q2161.66 143.296 2161.66 145.24 Q2161.66 146.722 2162.79 147.578 Q2163.93 148.412 2167.35 149.176 L2168.81 149.5 Q2173.35 150.472 2175.24 152.254 Q2177.17 154.014 2177.17 157.185 Q2177.17 160.796 2174.3 162.902 Q2171.45 165.009 2166.45 165.009 Q2164.37 165.009 2162.1 164.592 Q2159.85 164.199 2157.35 163.388 L2157.35 158.99 Q2159.71 160.217 2162 160.842 Q2164.3 161.444 2166.54 161.444 Q2169.55 161.444 2171.17 160.426 Q2172.79 159.384 2172.79 157.509 Q2172.79 155.773 2171.61 154.847 Q2170.45 153.921 2166.49 153.064 L2165.01 152.717 Q2161.05 151.884 2159.3 150.171 Q2157.54 148.435 2157.54 145.426 Q2157.54 141.768 2160.13 139.778 Q2162.72 137.787 2167.49 137.787 Q2169.85 137.787 2171.93 138.134 Q2174.02 138.481 2175.78 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2206.12 150.31 L2206.12 152.393 L2186.54 152.393 Q2186.82 156.791 2189.18 159.106 Q2191.56 161.398 2195.8 161.398 Q2198.25 161.398 2200.55 160.796 Q2202.86 160.194 2205.13 158.99 L2205.13 163.018 Q2202.84 163.99 2200.43 164.5 Q2198.02 165.009 2195.55 165.009 Q2189.34 165.009 2185.71 161.398 Q2182.1 157.787 2182.1 151.629 Q2182.1 145.264 2185.52 141.537 Q2188.97 137.787 2194.8 137.787 Q2200.04 137.787 2203.07 141.166 Q2206.12 144.523 2206.12 150.31 M2201.86 149.06 Q2201.82 145.565 2199.9 143.481 Q2198 141.398 2194.85 141.398 Q2191.29 141.398 2189.13 143.412 Q2187 145.426 2186.68 149.083 L2201.86 149.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2229.64 139.176 L2229.64 143.203 Q2227.84 142.277 2225.89 141.815 Q2223.95 141.352 2221.86 141.352 Q2218.69 141.352 2217.1 142.324 Q2215.52 143.296 2215.52 145.24 Q2215.52 146.722 2216.66 147.578 Q2217.79 148.412 2221.22 149.176 L2222.67 149.5 Q2227.21 150.472 2229.11 152.254 Q2231.03 154.014 2231.03 157.185 Q2231.03 160.796 2228.16 162.902 Q2225.31 165.009 2220.31 165.009 Q2218.23 165.009 2215.96 164.592 Q2213.72 164.199 2211.22 163.388 L2211.22 158.99 Q2213.58 160.217 2215.87 160.842 Q2218.16 161.444 2220.41 161.444 Q2223.42 161.444 2225.04 160.426 Q2226.66 159.384 2226.66 157.509 Q2226.66 155.773 2225.48 154.847 Q2224.32 153.921 2220.36 153.064 L2218.88 152.717 Q2214.92 151.884 2213.16 150.171 Q2211.4 148.435 2211.4 145.426 Q2211.4 141.768 2213.99 139.778 Q2216.59 137.787 2221.36 137.787 Q2223.72 137.787 2225.8 138.134 Q2227.88 138.481 2229.64 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n",
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip940\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip940)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip941\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip940)\" d=\"\n",
       "M141.02 1486.45 L2352.76 1486.45 L2352.76 47.2441 L141.02 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip942\">\n",
       "    <rect x=\"141\" y=\"47\" width=\"2213\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.325,1486.45 182.325,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  714.606,1486.45 714.606,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1246.89,1486.45 1246.89,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1779.17,1486.45 1779.17,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2311.45,1486.45 2311.45,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.02,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.325,1486.45 182.325,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  714.606,1486.45 714.606,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1246.89,1486.45 1246.89,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1779.17,1486.45 1779.17,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2311.45,1486.45 2311.45,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M182.325 1517.37 Q178.714 1517.37 176.885 1520.93 Q175.079 1524.47 175.079 1531.6 Q175.079 1538.71 176.885 1542.27 Q178.714 1545.82 182.325 1545.82 Q185.959 1545.82 187.764 1542.27 Q189.593 1538.71 189.593 1531.6 Q189.593 1524.47 187.764 1520.93 Q185.959 1517.37 182.325 1517.37 M182.325 1513.66 Q188.135 1513.66 191.19 1518.27 Q194.269 1522.85 194.269 1531.6 Q194.269 1540.33 191.19 1544.94 Q188.135 1549.52 182.325 1549.52 Q176.515 1549.52 173.436 1544.94 Q170.38 1540.33 170.38 1531.6 Q170.38 1522.85 173.436 1518.27 Q176.515 1513.66 182.325 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M693.877 1544.91 L710.197 1544.91 L710.197 1548.85 L688.252 1548.85 L688.252 1544.91 Q690.914 1542.16 695.498 1537.53 Q700.104 1532.88 701.285 1531.53 Q703.53 1529.01 704.41 1527.27 Q705.312 1525.51 705.312 1523.82 Q705.312 1521.07 703.368 1519.33 Q701.447 1517.6 698.345 1517.6 Q696.146 1517.6 693.692 1518.36 Q691.261 1519.13 688.484 1520.68 L688.484 1515.95 Q691.308 1514.82 693.761 1514.24 Q696.215 1513.66 698.252 1513.66 Q703.622 1513.66 706.817 1516.35 Q710.011 1519.03 710.011 1523.52 Q710.011 1525.65 709.201 1527.57 Q708.414 1529.47 706.308 1532.07 Q705.729 1532.74 702.627 1535.95 Q699.525 1539.15 693.877 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M720.058 1514.29 L738.414 1514.29 L738.414 1518.22 L724.34 1518.22 L724.34 1526.7 Q725.358 1526.35 726.377 1526.19 Q727.395 1526 728.414 1526 Q734.201 1526 737.581 1529.17 Q740.96 1532.34 740.96 1537.76 Q740.96 1543.34 737.488 1546.44 Q734.016 1549.52 727.696 1549.52 Q725.521 1549.52 723.252 1549.15 Q721.007 1548.78 718.599 1548.04 L718.599 1543.34 Q720.683 1544.47 722.905 1545.03 Q725.127 1545.58 727.604 1545.58 Q731.608 1545.58 733.946 1543.48 Q736.284 1541.37 736.284 1537.76 Q736.284 1534.15 733.946 1532.04 Q731.608 1529.94 727.604 1529.94 Q725.729 1529.94 723.854 1530.35 Q722.002 1530.77 720.058 1531.65 L720.058 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1221.59 1514.29 L1239.94 1514.29 L1239.94 1518.22 L1225.87 1518.22 L1225.87 1526.7 Q1226.89 1526.35 1227.91 1526.19 Q1228.92 1526 1229.94 1526 Q1235.73 1526 1239.11 1529.17 Q1242.49 1532.34 1242.49 1537.76 Q1242.49 1543.34 1239.02 1546.44 Q1235.55 1549.52 1229.23 1549.52 Q1227.05 1549.52 1224.78 1549.15 Q1222.54 1548.78 1220.13 1548.04 L1220.13 1543.34 Q1222.21 1544.47 1224.43 1545.03 Q1226.66 1545.58 1229.13 1545.58 Q1233.14 1545.58 1235.48 1543.48 Q1237.81 1541.37 1237.81 1537.76 Q1237.81 1534.15 1235.48 1532.04 Q1233.14 1529.94 1229.13 1529.94 Q1227.26 1529.94 1225.38 1530.35 Q1223.53 1530.77 1221.59 1531.65 L1221.59 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1261.7 1517.37 Q1258.09 1517.37 1256.26 1520.93 Q1254.46 1524.47 1254.46 1531.6 Q1254.46 1538.71 1256.26 1542.27 Q1258.09 1545.82 1261.7 1545.82 Q1265.34 1545.82 1267.14 1542.27 Q1268.97 1538.71 1268.97 1531.6 Q1268.97 1524.47 1267.14 1520.93 Q1265.34 1517.37 1261.7 1517.37 M1261.7 1513.66 Q1267.51 1513.66 1270.57 1518.27 Q1273.65 1522.85 1273.65 1531.6 Q1273.65 1540.33 1270.57 1544.94 Q1267.51 1549.52 1261.7 1549.52 Q1255.89 1549.52 1252.81 1544.94 Q1249.76 1540.33 1249.76 1531.6 Q1249.76 1522.85 1252.81 1518.27 Q1255.89 1513.66 1261.7 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1753.02 1514.29 L1775.25 1514.29 L1775.25 1516.28 L1762.7 1548.85 L1757.82 1548.85 L1769.62 1518.22 L1753.02 1518.22 L1753.02 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1784.41 1514.29 L1802.77 1514.29 L1802.77 1518.22 L1788.69 1518.22 L1788.69 1526.7 Q1789.71 1526.35 1790.73 1526.19 Q1791.75 1526 1792.77 1526 Q1798.56 1526 1801.94 1529.17 Q1805.32 1532.34 1805.32 1537.76 Q1805.32 1543.34 1801.84 1546.44 Q1798.37 1549.52 1792.05 1549.52 Q1789.88 1549.52 1787.61 1549.15 Q1785.36 1548.78 1782.95 1548.04 L1782.95 1543.34 Q1785.04 1544.47 1787.26 1545.03 Q1789.48 1545.58 1791.96 1545.58 Q1795.96 1545.58 1798.3 1543.48 Q1800.64 1541.37 1800.64 1537.76 Q1800.64 1534.15 1798.3 1532.04 Q1795.96 1529.94 1791.96 1529.94 Q1790.08 1529.94 1788.21 1530.35 Q1786.36 1530.77 1784.41 1531.65 L1784.41 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2271.06 1544.91 L2278.7 1544.91 L2278.7 1518.55 L2270.39 1520.21 L2270.39 1515.95 L2278.65 1514.29 L2283.33 1514.29 L2283.33 1544.91 L2290.96 1544.91 L2290.96 1548.85 L2271.06 1548.85 L2271.06 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2310.41 1517.37 Q2306.8 1517.37 2304.97 1520.93 Q2303.16 1524.47 2303.16 1531.6 Q2303.16 1538.71 2304.97 1542.27 Q2306.8 1545.82 2310.41 1545.82 Q2314.04 1545.82 2315.85 1542.27 Q2317.68 1538.71 2317.68 1531.6 Q2317.68 1524.47 2315.85 1520.93 Q2314.04 1517.37 2310.41 1517.37 M2310.41 1513.66 Q2316.22 1513.66 2319.27 1518.27 Q2322.35 1522.85 2322.35 1531.6 Q2322.35 1540.33 2319.27 1544.94 Q2316.22 1549.52 2310.41 1549.52 Q2304.6 1549.52 2301.52 1544.94 Q2298.46 1540.33 2298.46 1531.6 Q2298.46 1522.85 2301.52 1518.27 Q2304.6 1513.66 2310.41 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2340.57 1517.37 Q2336.96 1517.37 2335.13 1520.93 Q2333.33 1524.47 2333.33 1531.6 Q2333.33 1538.71 2335.13 1542.27 Q2336.96 1545.82 2340.57 1545.82 Q2344.21 1545.82 2346.01 1542.27 Q2347.84 1538.71 2347.84 1531.6 Q2347.84 1524.47 2346.01 1520.93 Q2344.21 1517.37 2340.57 1517.37 M2340.57 1513.66 Q2346.38 1513.66 2349.44 1518.27 Q2352.52 1522.85 2352.52 1531.6 Q2352.52 1540.33 2349.44 1544.94 Q2346.38 1549.52 2340.57 1549.52 Q2334.76 1549.52 2331.68 1544.94 Q2328.63 1540.33 2328.63 1531.6 Q2328.63 1522.85 2331.68 1518.27 Q2334.76 1513.66 2340.57 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.02,1237.15 2352.76,1237.15 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.02,974.574 2352.76,974.574 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.02,712.001 2352.76,712.001 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.02,449.428 2352.76,449.428 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.02,186.855 2352.76,186.855 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.02,1486.45 141.02,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.02,1237.15 159.917,1237.15 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.02,974.574 159.917,974.574 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.02,712.001 159.917,712.001 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.02,449.428 159.917,449.428 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.02,186.855 159.917,186.855 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M63.4921 1235.28 Q60.3439 1235.28 58.4921 1237.44 Q56.6634 1239.59 56.6634 1243.34 Q56.6634 1247.07 58.4921 1249.24 Q60.3439 1251.39 63.4921 1251.39 Q66.6402 1251.39 68.4689 1249.24 Q70.3207 1247.07 70.3207 1243.34 Q70.3207 1239.59 68.4689 1237.44 Q66.6402 1235.28 63.4921 1235.28 M72.7744 1220.63 L72.7744 1224.89 Q71.0152 1224.06 69.2096 1223.62 Q67.4272 1223.18 65.668 1223.18 Q61.0384 1223.18 58.5847 1226.3 Q56.1541 1229.43 55.8069 1235.75 Q57.1726 1233.73 59.2328 1232.67 Q61.293 1231.58 63.7698 1231.58 Q68.9781 1231.58 71.9874 1234.75 Q75.0198 1237.9 75.0198 1243.34 Q75.0198 1248.66 71.8716 1251.88 Q68.7235 1255.1 63.4921 1255.1 Q57.4967 1255.1 54.3254 1250.52 Q51.1542 1245.91 51.1542 1237.18 Q51.1542 1228.99 55.043 1224.13 Q58.9319 1219.24 65.4828 1219.24 Q67.242 1219.24 69.0244 1219.59 Q70.83 1219.94 72.7744 1220.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M93.0752 1222.95 Q89.4641 1222.95 87.6354 1226.51 Q85.8299 1230.05 85.8299 1237.18 Q85.8299 1244.29 87.6354 1247.85 Q89.4641 1251.39 93.0752 1251.39 Q96.7095 1251.39 98.515 1247.85 Q100.344 1244.29 100.344 1237.18 Q100.344 1230.05 98.515 1226.51 Q96.7095 1222.95 93.0752 1222.95 M93.0752 1219.24 Q98.8854 1219.24 101.941 1223.85 Q105.02 1228.43 105.02 1237.18 Q105.02 1245.91 101.941 1250.52 Q98.8854 1255.1 93.0752 1255.1 Q87.2651 1255.1 84.1864 1250.52 Q81.1309 1245.91 81.1309 1237.18 Q81.1309 1228.43 84.1864 1223.85 Q87.2651 1219.24 93.0752 1219.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M64.1402 972.711 Q60.9921 972.711 59.1402 974.864 Q57.3115 977.016 57.3115 980.766 Q57.3115 984.493 59.1402 986.669 Q60.9921 988.822 64.1402 988.822 Q67.2883 988.822 69.117 986.669 Q70.9689 984.493 70.9689 980.766 Q70.9689 977.016 69.117 974.864 Q67.2883 972.711 64.1402 972.711 M73.4226 958.058 L73.4226 962.317 Q71.6633 961.484 69.8578 961.044 Q68.0754 960.604 66.3161 960.604 Q61.6865 960.604 59.2328 963.729 Q56.8023 966.854 56.4551 973.174 Q57.8208 971.16 59.881 970.095 Q61.9411 969.007 64.418 969.007 Q69.6263 969.007 72.6355 972.178 Q75.6679 975.327 75.6679 980.766 Q75.6679 986.09 72.5198 989.308 Q69.3717 992.526 64.1402 992.526 Q58.1449 992.526 54.9736 987.942 Q51.8023 983.336 51.8023 974.609 Q51.8023 966.415 55.6912 961.553 Q59.58 956.669 66.1309 956.669 Q67.8902 956.669 69.6726 957.016 Q71.4781 957.364 73.4226 958.058 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M97.89 973.22 Q101.246 973.938 103.121 976.206 Q105.02 978.475 105.02 981.808 Q105.02 986.924 101.501 989.725 Q97.9826 992.526 91.5012 992.526 Q89.3253 992.526 87.0105 992.086 Q84.7188 991.669 82.2651 990.813 L82.2651 986.299 Q84.2095 987.433 86.5243 988.012 Q88.8391 988.59 91.3623 988.59 Q95.7604 988.59 98.0521 986.854 Q100.367 985.118 100.367 981.808 Q100.367 978.752 98.2141 977.039 Q96.0845 975.303 92.2651 975.303 L88.2373 975.303 L88.2373 971.461 L92.4502 971.461 Q95.8993 971.461 97.728 970.095 Q99.5567 968.706 99.5567 966.114 Q99.5567 963.452 97.6585 962.04 Q95.7836 960.604 92.2651 960.604 Q90.3438 960.604 88.1447 961.021 Q85.9456 961.438 83.3068 962.317 L83.3068 958.151 Q85.9688 957.41 88.2836 957.04 Q90.6215 956.669 92.6817 956.669 Q98.0058 956.669 101.108 959.1 Q104.209 961.507 104.209 965.628 Q104.209 968.498 102.566 970.489 Q100.922 972.456 97.89 973.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M63.33 710.138 Q60.1819 710.138 58.33 712.291 Q56.5014 714.443 56.5014 718.193 Q56.5014 721.92 58.33 724.096 Q60.1819 726.249 63.33 726.249 Q66.4782 726.249 68.3068 724.096 Q70.1587 721.92 70.1587 718.193 Q70.1587 714.443 68.3068 712.291 Q66.4782 710.138 63.33 710.138 M72.6124 695.485 L72.6124 699.744 Q70.8531 698.911 69.0476 698.471 Q67.2652 698.031 65.5059 698.031 Q60.8763 698.031 58.4226 701.156 Q55.9921 704.281 55.6449 710.601 Q57.0106 708.587 59.0708 707.522 Q61.131 706.434 63.6078 706.434 Q68.8161 706.434 71.8253 709.605 Q74.8577 712.754 74.8577 718.193 Q74.8577 723.517 71.7096 726.735 Q68.5615 729.952 63.33 729.952 Q57.3347 729.952 54.1634 725.369 Q50.9921 720.763 50.9921 712.036 Q50.9921 703.842 54.881 698.98 Q58.7699 694.096 65.3208 694.096 Q67.08 694.096 68.8624 694.443 Q70.6679 694.791 72.6124 695.485 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M93.4919 710.138 Q90.3438 710.138 88.4919 712.291 Q86.6632 714.443 86.6632 718.193 Q86.6632 721.92 88.4919 724.096 Q90.3438 726.249 93.4919 726.249 Q96.64 726.249 98.4687 724.096 Q100.321 721.92 100.321 718.193 Q100.321 714.443 98.4687 712.291 Q96.64 710.138 93.4919 710.138 M102.774 695.485 L102.774 699.744 Q101.015 698.911 99.2095 698.471 Q97.4271 698.031 95.6678 698.031 Q91.0382 698.031 88.5845 701.156 Q86.154 704.281 85.8068 710.601 Q87.1725 708.587 89.2327 707.522 Q91.2928 706.434 93.7697 706.434 Q98.978 706.434 101.987 709.605 Q105.02 712.754 105.02 718.193 Q105.02 723.517 101.871 726.735 Q98.7234 729.952 93.4919 729.952 Q87.4966 729.952 84.3253 725.369 Q81.154 720.763 81.154 712.036 Q81.154 703.842 85.0429 698.98 Q88.9317 694.096 95.4826 694.096 Q97.2419 694.096 99.0243 694.443 Q100.83 694.791 102.774 695.485 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M63.6772 447.565 Q60.5291 447.565 58.6773 449.718 Q56.8486 451.87 56.8486 455.62 Q56.8486 459.347 58.6773 461.523 Q60.5291 463.676 63.6772 463.676 Q66.8254 463.676 68.6541 461.523 Q70.5059 459.347 70.5059 455.62 Q70.5059 451.87 68.6541 449.718 Q66.8254 447.565 63.6772 447.565 M72.9596 432.912 L72.9596 437.171 Q71.2004 436.338 69.3948 435.898 Q67.6124 435.458 65.8532 435.458 Q61.2236 435.458 58.7699 438.583 Q56.3393 441.708 55.9921 448.028 Q57.3578 446.014 59.418 444.949 Q61.4782 443.861 63.955 443.861 Q69.1633 443.861 72.1726 447.032 Q75.205 450.18 75.205 455.62 Q75.205 460.944 72.0568 464.162 Q68.9087 467.379 63.6772 467.379 Q57.6819 467.379 54.5106 462.796 Q51.3393 458.19 51.3393 449.463 Q51.3393 441.269 55.2282 436.407 Q59.1171 431.523 65.668 431.523 Q67.4272 431.523 69.2096 431.87 Q71.0152 432.218 72.9596 432.912 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M83.3994 465.991 L83.3994 461.731 Q85.1586 462.565 86.9642 463.005 Q88.7697 463.444 90.5058 463.444 Q95.1354 463.444 97.566 460.342 Q100.02 457.217 100.367 450.875 Q99.0243 452.866 96.9641 453.93 Q94.9039 454.995 92.4039 454.995 Q87.2188 454.995 84.1864 451.87 Q81.1771 448.722 81.1771 443.282 Q81.1771 437.958 84.3253 434.741 Q87.4734 431.523 92.7049 431.523 Q98.7002 431.523 101.848 436.13 Q105.02 440.713 105.02 449.463 Q105.02 457.634 101.131 462.518 Q97.265 467.379 90.7141 467.379 Q88.9549 467.379 87.1493 467.032 Q85.3438 466.685 83.3994 465.991 M92.7049 451.338 Q95.853 451.338 97.6817 449.185 Q99.5335 447.032 99.5335 443.282 Q99.5335 439.556 97.6817 437.403 Q95.853 435.227 92.7049 435.227 Q89.5567 435.227 87.7049 437.403 Q85.8762 439.556 85.8762 443.282 Q85.8762 447.032 87.7049 449.185 Q89.5567 451.338 92.7049 451.338 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M53.3301 169.575 L75.5522 169.575 L75.5522 171.566 L63.006 204.135 L58.1217 204.135 L69.9272 173.51 L53.3301 173.51 L53.3301 169.575 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M88.7003 200.2 L105.02 200.2 L105.02 204.135 L83.0753 204.135 L83.0753 200.2 Q85.7373 197.445 90.3206 192.816 Q94.9271 188.163 96.1076 186.82 Q98.353 184.297 99.2326 182.561 Q100.135 180.802 100.135 179.112 Q100.135 176.358 98.1909 174.621 Q96.2697 172.885 93.1678 172.885 Q90.9688 172.885 88.5151 173.649 Q86.0845 174.413 83.3068 175.964 L83.3068 171.242 Q86.1308 170.108 88.5845 169.529 Q91.0382 168.95 93.0752 168.95 Q98.4456 168.95 101.64 171.635 Q104.834 174.321 104.834 178.811 Q104.834 180.941 104.024 182.862 Q103.237 184.76 101.131 187.353 Q100.552 188.024 97.4502 191.242 Q94.3484 194.436 88.7003 200.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip942)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  203.616,87.9763 224.907,118.618 246.198,180.25 267.49,112.957 288.781,249.568 310.072,290.698 331.363,344.458 352.655,350.744 373.946,481.642 395.237,448.74 \n",
       "  416.529,418.418 437.82,523.197 459.111,548.592 480.402,564.375 501.694,640.239 522.985,618.291 544.276,490.983 565.567,599.878 586.859,738.887 608.15,704.958 \n",
       "  629.441,754.824 650.732,751.087 672.024,771.273 693.315,834.61 714.606,768.29 735.897,767.032 757.189,754.702 778.48,831.618 799.771,815.769 821.063,697.829 \n",
       "  842.354,814.459 863.645,759.594 884.936,790.339 906.228,848.358 927.519,830.063 948.81,857.61 970.101,893.104 991.393,840.383 1012.68,805.568 1033.98,870.588 \n",
       "  1055.27,902.558 1076.56,800.636 1097.85,884.474 1119.14,931.048 1140.43,832.144 1161.72,1015.6 1183.01,961.981 1204.31,1018.78 1225.6,958.285 1246.89,1028.76 \n",
       "  1268.18,963.786 1289.47,988.956 1310.76,984.314 1332.05,939.373 1353.34,948.796 1374.64,1078.89 1395.93,999.17 1417.22,1106.87 1438.51,999.05 1459.8,1130.34 \n",
       "  1481.09,1085.47 1502.38,966.174 1523.67,1150.86 1544.97,1041.42 1566.26,1064.99 1587.55,1162.35 1608.84,1176.4 1630.13,1058.63 1651.42,1193.22 1672.71,1131.79 \n",
       "  1694,1169.54 1715.3,1208.58 1736.59,1209.1 1757.88,1170.44 1779.17,1129.78 1800.46,1178.28 1821.75,1191.87 1843.04,1196.75 1864.33,1252.86 1885.63,1274.19 \n",
       "  1906.92,1147.02 1928.21,1112.19 1949.5,1242.15 1970.79,1250.01 1992.08,1069.18 2013.37,1313.35 2034.66,1298.5 2055.96,1166.12 2077.25,1269.53 2098.54,1305.33 \n",
       "  2119.83,1206.78 2141.12,1402.09 2162.41,1349.48 2183.7,1314.46 2204.99,1445.72 2226.29,1345.31 2247.58,1424.47 2268.87,1377.53 2290.16,1402.46 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"\n",
       "M1888.15 198.898 L2279.03 198.898 L2279.03 95.2176 L1888.15 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1888.15,198.898 2279.03,198.898 2279.03,95.2176 1888.15,95.2176 1888.15,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1912.15,147.058 2056.15,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M2080.15 129.778 L2084.83 129.778 L2084.83 160.402 L2101.66 160.402 L2101.66 164.338 L2080.15 164.338 L2080.15 129.778 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2115.59 141.398 Q2112.17 141.398 2110.18 144.083 Q2108.18 146.745 2108.18 151.398 Q2108.18 156.051 2110.15 158.736 Q2112.14 161.398 2115.59 161.398 Q2118.99 161.398 2120.99 158.713 Q2122.98 156.027 2122.98 151.398 Q2122.98 146.791 2120.99 144.106 Q2118.99 141.398 2115.59 141.398 M2115.59 137.787 Q2121.15 137.787 2124.32 141.398 Q2127.49 145.009 2127.49 151.398 Q2127.49 157.764 2124.32 161.398 Q2121.15 165.009 2115.59 165.009 Q2110.01 165.009 2106.84 161.398 Q2103.69 157.764 2103.69 151.398 Q2103.69 145.009 2106.84 141.398 Q2110.01 137.787 2115.59 137.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2151.08 139.176 L2151.08 143.203 Q2149.27 142.277 2147.33 141.815 Q2145.38 141.352 2143.3 141.352 Q2140.13 141.352 2138.53 142.324 Q2136.96 143.296 2136.96 145.24 Q2136.96 146.722 2138.09 147.578 Q2139.23 148.412 2142.65 149.176 L2144.11 149.5 Q2148.65 150.472 2150.55 152.254 Q2152.47 154.014 2152.47 157.185 Q2152.47 160.796 2149.6 162.902 Q2146.75 165.009 2141.75 165.009 Q2139.67 165.009 2137.4 164.592 Q2135.15 164.199 2132.65 163.388 L2132.65 158.99 Q2135.01 160.217 2137.31 160.842 Q2139.6 161.444 2141.84 161.444 Q2144.85 161.444 2146.47 160.426 Q2148.09 159.384 2148.09 157.509 Q2148.09 155.773 2146.91 154.847 Q2145.75 153.921 2141.8 153.064 L2140.31 152.717 Q2136.36 151.884 2134.6 150.171 Q2132.84 148.435 2132.84 145.426 Q2132.84 141.768 2135.43 139.778 Q2138.02 137.787 2142.79 137.787 Q2145.15 137.787 2147.24 138.134 Q2149.32 138.481 2151.08 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2175.78 139.176 L2175.78 143.203 Q2173.97 142.277 2172.03 141.815 Q2170.08 141.352 2168 141.352 Q2164.83 141.352 2163.23 142.324 Q2161.66 143.296 2161.66 145.24 Q2161.66 146.722 2162.79 147.578 Q2163.93 148.412 2167.35 149.176 L2168.81 149.5 Q2173.35 150.472 2175.24 152.254 Q2177.17 154.014 2177.17 157.185 Q2177.17 160.796 2174.3 162.902 Q2171.45 165.009 2166.45 165.009 Q2164.37 165.009 2162.1 164.592 Q2159.85 164.199 2157.35 163.388 L2157.35 158.99 Q2159.71 160.217 2162 160.842 Q2164.3 161.444 2166.54 161.444 Q2169.55 161.444 2171.17 160.426 Q2172.79 159.384 2172.79 157.509 Q2172.79 155.773 2171.61 154.847 Q2170.45 153.921 2166.49 153.064 L2165.01 152.717 Q2161.05 151.884 2159.3 150.171 Q2157.54 148.435 2157.54 145.426 Q2157.54 141.768 2160.13 139.778 Q2162.72 137.787 2167.49 137.787 Q2169.85 137.787 2171.93 138.134 Q2174.02 138.481 2175.78 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2206.12 150.31 L2206.12 152.393 L2186.54 152.393 Q2186.82 156.791 2189.18 159.106 Q2191.56 161.398 2195.8 161.398 Q2198.25 161.398 2200.55 160.796 Q2202.86 160.194 2205.13 158.99 L2205.13 163.018 Q2202.84 163.99 2200.43 164.5 Q2198.02 165.009 2195.55 165.009 Q2189.34 165.009 2185.71 161.398 Q2182.1 157.787 2182.1 151.629 Q2182.1 145.264 2185.52 141.537 Q2188.97 137.787 2194.8 137.787 Q2200.04 137.787 2203.07 141.166 Q2206.12 144.523 2206.12 150.31 M2201.86 149.06 Q2201.82 145.565 2199.9 143.481 Q2198 141.398 2194.85 141.398 Q2191.29 141.398 2189.13 143.412 Q2187 145.426 2186.68 149.083 L2201.86 149.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2229.64 139.176 L2229.64 143.203 Q2227.84 142.277 2225.89 141.815 Q2223.95 141.352 2221.86 141.352 Q2218.69 141.352 2217.1 142.324 Q2215.52 143.296 2215.52 145.24 Q2215.52 146.722 2216.66 147.578 Q2217.79 148.412 2221.22 149.176 L2222.67 149.5 Q2227.21 150.472 2229.11 152.254 Q2231.03 154.014 2231.03 157.185 Q2231.03 160.796 2228.16 162.902 Q2225.31 165.009 2220.31 165.009 Q2218.23 165.009 2215.96 164.592 Q2213.72 164.199 2211.22 163.388 L2211.22 158.99 Q2213.58 160.217 2215.87 160.842 Q2218.16 161.444 2220.41 161.444 Q2223.42 161.444 2225.04 160.426 Q2226.66 159.384 2226.66 157.509 Q2226.66 155.773 2225.48 154.847 Q2224.32 153.921 2220.36 153.064 L2218.88 152.717 Q2214.92 151.884 2213.16 150.171 Q2211.4 148.435 2211.4 145.426 Q2211.4 141.768 2213.99 139.778 Q2216.59 137.787 2221.36 137.787 Q2223.72 137.787 2225.8 138.134 Q2227.88 138.481 2229.64 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(vcat(losses[2:100]...), label=\"Losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#erst einmal so lassen, vielleicht einzelne Änderungen\n",
    "function translate(x)\n",
    "    ix = todevice(vocab(preprocess(x)))\n",
    "    seq = [startsym]\n",
    "\n",
    "    enc = encoder_forward(ix)\n",
    "\n",
    "    len = length(ix)\n",
    "    for i = 1:2len\n",
    "        trg = todevice(vocab(seq))\n",
    "        dec = decoder_forward(trg, enc)\n",
    "        #move back to gpu due to argmax wrong result on CuArrays\n",
    "        ntok = onecold(collect(dec), labels)\n",
    "        push!(seq, ntok[end])\n",
    "        ntok[end] == endsym && break\n",
    "    end\n",
    "  seq[2:end-1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"6\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(map(string, [5,5,6,6,1,12,3,4,6]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
