{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\tusing Flux\n",
    "\tusing Flux: onehot\n",
    "\tusing Flux: gradient\n",
    "\tusing Flux.Optimise: update!\n",
    "\tusing Flux: onecold\n",
    "\tusing CUDA\n",
    "\tusing Transformers\n",
    "\tusing Transformers.Basic \n",
    "    using Transformers.Datasets: batched\n",
    "    enable_gpu(false) \n",
    "\tusing Flux: @functor\n",
    "\tusing ..Transformers: Abstract3DTensor, Container, epsilon, batchedmul, batched_triu!\n",
    "\n",
    "end\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary{String}(13, unk=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Labeling: vielleicht ändern\n",
    "\n",
    "begin\n",
    "\tlabels = map(string, 1:10)\n",
    "\tstartsym = \"11\"\n",
    "\tendsym = \"12\"\n",
    "\tunksym = \"0\"\n",
    "\tlabels = [unksym, startsym, endsym, labels...]\n",
    "\tvocab = Vocabulary(labels, unksym)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for generate training datas \n",
    "\n",
    "#nichts ändern\n",
    "sample_data() = (d = map(string, rand(1:10, 10)); (d,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"7\", \"10\", \"9\", \"4\", \"8\", \"7\", \"9\", \"4\", \"9\", \"7\"], [\"7\", \"10\", \"9\", \"4\", \"8\", \"7\", \"9\", \"4\", \"9\", \"7\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "\n",
    "sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for adding start & end symbol\n",
    "\n",
    "#nichts ändern\n",
    "preprocess(x) = [startsym, x..., endsym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_ex = preprocess.(sample_data()) = ([\"11\", \"4\", \"7\", \"6\", \"4\", \"10\", \"9\", \"10\", \"10\", \"5\", \"5\", \"12\"], [\"11\", \"4\", \"7\", \"6\", \"4\", \"10\", \"9\", \"10\", \"10\", \"5\", \"5\", \"12\"])\n",
      "encoded_sample_ex = vocab(sample_ex[1]) = [2, 7, 10, 9, 7, 13, 12, 13, 13, 8, 8, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       "  7\n",
       " 10\n",
       "  9\n",
       "  7\n",
       " 13\n",
       " 12\n",
       " 13\n",
       " 13\n",
       "  8\n",
       "  8\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "\n",
    "begin\n",
    "    @show sample_ex = preprocess.(sample_data())\n",
    "    @show encoded_sample_ex = vocab(sample_ex[1]) #use Vocabulary to encode the training data\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"11\", \"1\", \"10\", \"5\", \"2\", \"1\", \"8\", \"9\", \"2\", \"9\", \"10\", \"12\"], [\"11\", \"1\", \"10\", \"5\", \"2\", \"1\", \"8\", \"9\", \"2\", \"9\", \"10\", \"12\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "sample = preprocess.(sample_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       "  4\n",
       " 13\n",
       "  8\n",
       "  5\n",
       "  4\n",
       " 11\n",
       " 12\n",
       "  5\n",
       " 12\n",
       " 13\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "encoded_sample = vocab(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embed(512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a Word embedding layer which turn word index to word vector\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "embed = Embed(512, length(vocab)) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionEmbedding(512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a position embedding layer metioned above\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "pe = PositionEmbedding(512) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wrapper for get embedding\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "function embedding(x)\n",
    "    we = embed(x, inv(sqrt(512)))\n",
    "    e = we .+ pe(we)\n",
    "    return e\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractAttention end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_atten_mask1 (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_atten_mask1(T::Type, score::AbstractArray, ::Nothing, future) = create_atten_mask1(T, score, fill!(similar(score, size(score,1), size(score, 2), 1), one(T)), future)\n",
    "function create_atten_mask1(T::Type, score::AbstractArray, _mask::AbstractArray, future::Bool=false)\n",
    "  #size(mask) == (q, k, n, b)\n",
    "\n",
    "  # ql, kl = size(mask)\n",
    "  mask = copy(_mask)\n",
    "\n",
    "  maskval = convert(T, -1e9)\n",
    "  !future && batched_triu!(mask, 0)\n",
    "  mask .= (1 .- mask) .* maskval\n",
    "  return mask\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd create_atten_mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MultiheadAttention_1{Q<:Dense, K<:Dense, V<:Dense, O<:Dense, DP<:Dropout} <: AbstractAttention\n",
    "    head::Int\n",
    "    future::Bool\n",
    "    iqproj::Q\n",
    "    ikproj::K\n",
    "    ivproj::V\n",
    "    oproj::O\n",
    "    drop::DP\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.functor(mh::MultiheadAttention_1) = (mh.iqproj, mh.ikproj, mh.ivproj, mh.oproj), m -> MultiheadAttention_1(mh.head, mh.future, m..., mh.drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;\\n                       future::Bool=true, pdrop = 0.1)\\nMultihead dot product Attention Layer, `head` is the number of head, \\n`is` is the input size, `hs` is the hidden size of input projection layer of each head, \\n`os` is the output size. When `future` is `false`, the k-th token can't see tokens at > k. \\n`pdrop` is the dropout rate.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;\n",
    "                       future::Bool=true, pdrop = 0.1)\n",
    "Multihead dot product Attention Layer, `head` is the number of head, \n",
    "`is` is the input size, `hs` is the hidden size of input projection layer of each head, \n",
    "`os` is the output size. When `future` is `false`, the k-th token can't see tokens at > k. \n",
    "`pdrop` is the dropout rate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiheadAttention_1(head::Int,\n",
    "                   is::Int,\n",
    "                   hs::Int,\n",
    "                   os::Int;\n",
    "                   future::Bool=true, pdrop = 0.1) = MultiheadAttention_1(head,\n",
    "                                                                        future,\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(hs*head, os),\n",
    "                                                                        Dropout(pdrop),\n",
    "                                                                        )\n",
    "\n",
    "\n",
    "function Base.show(io::IO, mh::MultiheadAttention_1)\n",
    "    hs = div(size(mh.iqproj.weight)[1], mh.head)\n",
    "    is = size(mh.iqproj.weight)[end]\n",
    "    os = size(mh.oproj.weight)[1]\n",
    "\n",
    "    print(io, \"MultiheadAttention(\")\n",
    "    print(io, \"head=$(mh.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"$(is)=>$(os)\")\n",
    "\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(mh.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (mh::MultiheadAttention_1)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: Abstract3DTensor{T},\n",
    "                         A2 <: Abstract3DTensor{T},\n",
    "                         A3 <: Abstract3DTensor{T}}\n",
    "qs = size(query)\n",
    "ks = size(key)\n",
    "vs = size(value)\n",
    "\n",
    "#size(ipq) == (h, q_seq_len, batch)\n",
    "ipq = @toNd mh.iqproj(query)\n",
    "ipk = @toNd mh.ikproj(key)\n",
    "ipv = @toNd mh.ivproj(value)\n",
    "\n",
    "h = size(ipq, 1)\n",
    "hs = div(h, mh.head)\n",
    "\n",
    "#size(ipq) == (hs, q_seq_len, head, batch)\n",
    "ipq = permutedims(reshape(ipq, hs, mh.head, qs[2], qs[3]), [1, 3, 2, 4])\n",
    "ipk = permutedims(reshape(ipk, hs, mh.head, ks[2], ks[3]), [1, 3, 2, 4])\n",
    "ipv = permutedims(reshape(ipv, hs, mh.head, vs[2], vs[3]), [1, 3, 2, 4])\n",
    "\n",
    "#size(ipq) == (hs, q_seq_len, head * batch)\n",
    "ipq = reshape(ipq, hs, qs[2], :)\n",
    "ipk = reshape(ipk, hs, ks[2], :)\n",
    "ipv = reshape(ipv, hs, vs[2], :)\n",
    "\n",
    "atten = attention(ipq,ipk,ipv,\n",
    "mask,\n",
    "mh.future,\n",
    "mh.drop)\n",
    "\n",
    "atten = permutedims(reshape(atten, hs, qs[2], mh.head, qs[3]), [1, 3, 2, 4]) #size(atten) == (hs, head, ql, b)\n",
    "atten = reshape(atten, h, qs[2], qs[3]) #size(atten) == (h, ql, b)\n",
    "\n",
    "out = @toNd mh.oproj(atten)\n",
    "out #size(out) == (h, q_seq_len, batch)\n",
    "end\n",
    "\n",
    "function (mh::MultiheadAttention_1)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: AbstractMatrix{T},\n",
    "                         A2 <: AbstractMatrix{T},\n",
    "                         A3 <: AbstractMatrix{T}}\n",
    "\n",
    "# size(query) == (dims, seq_len)\n",
    "ipq = mh.iqproj(query)\n",
    "ipk = mh.ikproj(key)\n",
    "ipv = mh.ivproj(value)\n",
    "\n",
    "h = size(ipq)[1] #h == hs * head\n",
    "hs = div(h, mh.head)\n",
    "\n",
    "#size(hq) == (hs, seq_len, head)\n",
    "hq = permutedims(reshape(ipq, hs, mh.head, :), [1, 3, 2])\n",
    "hk = permutedims(reshape(ipk, hs, mh.head, :), [1, 3, 2])\n",
    "hv = permutedims(reshape(ipv, hs, mh.head, :), [1, 3, 2])\n",
    "\n",
    "atten = attention(hq, hk, hv,\n",
    "mask,\n",
    "mh.future,\n",
    "mh.drop)\n",
    "\n",
    "# size(atten) == (head*hs, seq_len)\n",
    "atten = reshape(permutedims(atten, [1, 3, 2]), h, :)\n",
    "\n",
    "mh.oproj(atten)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_mask1 (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apply_mask1(score, mask)\n",
    "    s = size(score)\n",
    "    ms = size(mask)\n",
    "    bxn = s[end]\n",
    "    b = ms[end]\n",
    "    if bxn == b || b == 1\n",
    "      return score .+ mask\n",
    "    else\n",
    "      return reshape(reshape(score, s[1:end-1]..., :, b) .+\n",
    "                     reshape(mask, ms[1:end-1]..., 1, b), s)\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  apply_mask1(score::AbstractArray{T}, ::Nothing, future) where T = future ? score : apply_mask1(score, create_atten_mask1(T, score, nothing, future))\n",
    "  apply_mask1(score::AbstractArray{T}, mask, future) where T = apply_mask(score, create_atten_mask1(T, score, mask, future))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function attention(query::A1,\n",
    "    key::A2,\n",
    "    value::A3,\n",
    "    mask, future::Bool,\n",
    "    dropout) where {T,\n",
    "                    A1 <: Abstract3DTensor{T},\n",
    "                    A2 <: Abstract3DTensor{T},\n",
    "                    A3 <: Abstract3DTensor{T}}\n",
    "#size(query) == (dims, {q,k}_seq_len, batch) == size(key) == size(value)\n",
    "#size(score) == (k_seq_len, q_seq_len, batch)\n",
    "dk = size(key, 1)\n",
    "score = batchedmul(key, query; transA = true)\n",
    "score = score ./ convert(T, sqrt(dk))\n",
    "\n",
    "score = apply_mask1(score, mask, future)\n",
    "score = softmax(score; dims=1)\n",
    "dropout !== nothing && (score = dropout(score))\n",
    "batchedmul(value, score) #size(return) == (dims, q_seq_len, batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractTransformer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PwFFN{Di<:Dense, Do<:Dense}\n",
    "    din::Di\n",
    "    dout::Do\n",
    "end\n",
    "\n",
    "@functor PwFFN\n",
    "\n",
    "\n",
    "\"just a wrapper for two dense layer.\"\n",
    "PwFFN(size::Int, h::Int, act = relu) = PwFFN(  #relu vielleicht auf elu + 1 oder nur elu ändern\n",
    "    Dense(size, h, act),\n",
    "    Dense(h, size)\n",
    ")\n",
    "\n",
    "function (pw::PwFFN)(x::AbstractMatrix)\n",
    "  # size(x) == (dims, seq_len)\n",
    "  pw.dout(pw.din(x))\n",
    "end\n",
    "\n",
    "function (pw::PwFFN)(x::A) where {T, N, A<:AbstractArray{T, N}}\n",
    "  new_x = reshape(x, size(x, 1), :)\n",
    "  y = pw(new_x)\n",
    "  return reshape(y, Base.setindex(size(x), size(y, 1), 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Transformer1{MA<:MultiheadAttention_1, LA<:LayerNorm, P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "    mh::MA\n",
    "    mhn::LA\n",
    "    pw::P\n",
    "    pwn::LP\n",
    "    drop::DP\n",
    "end\n",
    "\n",
    "@functor Transformer1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Transformer(size::Int, head::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)\n",
    "    Transformer(size::Int, head::Int, hs::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)  \n",
    "\n",
    "Transformer layer.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "When `future` is `false`, the k-th token can't see the j-th tokens where j > k. `pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "\n",
    "########\n",
    "#ganze Funktion von Transformer ändern\n",
    "########\n",
    "function Transformer1(size::Int, head::Int, ps::Int; future::Bool = true, act = relu, pdrop = 0.1)  #relu vielleicht wieder auf elu ändern\n",
    "    rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "    Transformer1(size, head, div(size, head), ps;future=future, act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "Transformer1(size::Int, head::Int, hs::Int, ps::Int; future::Bool = true, act = relu, pdrop = 0.1) = Transformer1(\n",
    "    MultiheadAttention_1(head, size, hs, size; future=future, pdrop=pdrop),\n",
    "    LayerNorm(size),\n",
    "    PwFFN(size, ps, act),\n",
    "    LayerNorm(size),\n",
    "    Dropout(pdrop),     #braucht man das? vielleicht nicht\n",
    ")\n",
    "\n",
    "function (t::Transformer1)(x::A, mask=nothing) where {T, N, A<:AbstractArray{T, N}}\n",
    "    dropout = t.drop\n",
    "    a = t.mh(x, x, x; mask=mask)\n",
    "    a = dropout(a)\n",
    "    res_a = x + a\n",
    "    res_a = t.mhn(res_a)\n",
    "    pwffn = t.pw(res_a)\n",
    "    pwffn = dropout(pwffn)\n",
    "    res_pwffn = res_a + pwffn\n",
    "    res_pwffn = t.pwn(res_pwffn)\n",
    "    res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, t::Transformer1) #zum visualisieren --> nice to have\n",
    "    hs = div(size(t.mh.iqproj.weight)[1], t.mh.head)\n",
    "    h, ps = size(t.pw.dout.weight)\n",
    "\n",
    "    print(io, \"Transformer(\")\n",
    "    print(io, \"head=$(t.mh.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"pwffn_size=$(ps), \")\n",
    "    print(io, \"size=$(h)\")\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(t.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "    #auch diese Funktion ändern\n",
    "    ###############\n",
    "    struct TransformerDecoder1{MA<:MultiheadAttention_1, LA<:LayerNorm,\n",
    "        IMA<:MultiheadAttention_1, ILA<:LayerNorm,\n",
    "        P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "mh::MA\n",
    "mhn::LA\n",
    "imh::IMA\n",
    "imhn::ILA\n",
    "pw::P\n",
    "pwn::LP\n",
    "drop::DP\n",
    "end\n",
    "\n",
    "@functor TransformerDecoder1\n",
    "\n",
    "\"\"\"\n",
    "TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "TransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "\n",
    "TransformerDecoder layer. Decode the value from a Encoder.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "`pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "function TransformerDecoder1(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "TransformerDecoder1(size, head, div(size, head), ps; act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "TransformerDecoder1(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1) = TransformerDecoder1(\n",
    "MultiheadAttention_1(head, size, hs, size; future=false, pdrop=pdrop),\n",
    "LayerNorm(size),\n",
    "MultiheadAttention_1(head, size, hs, size; future=true, pdrop=pdrop), #future = true --> Unterschied zu oben??\n",
    "LayerNorm(size),\n",
    "PwFFN(size, ps, act),\n",
    "LayerNorm(size),\n",
    "Dropout(pdrop),\n",
    ")\n",
    "\n",
    "function (td::TransformerDecoder1)(x::AbstractArray{T,N}, m, mask=nothing) where {T,N}\n",
    "dropout = td.drop\n",
    "a = td.mh(x,x,x)\n",
    "a = dropout(a)\n",
    "res_a = x + a\n",
    "res_a = td.mhn(res_a)\n",
    "\n",
    "ia = td.imh(res_a, m, m, mask=mask)\n",
    "ia = dropout(ia)\n",
    "res_ia = res_a + ia\n",
    "res_ia = td.imhn(res_ia)\n",
    "\n",
    "pwffn = td.pw(res_ia)\n",
    "pwffn = dropout(pwffn)\n",
    "res_pwffn = res_ia + pwffn\n",
    "res_pwffn = td.pwn(res_pwffn)\n",
    "res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, td::TransformerDecoder1)\n",
    "hs = div(size(td.imh.iqproj.weight)[1], td.imh.head)\n",
    "h, ps = size(td.pw.dout.weight)\n",
    "\n",
    "print(io, \"TransformerDecoder(\")\n",
    "print(io, \"head=$(td.mh.head), \")\n",
    "print(io, \"head_size=$(hs), \")\n",
    "print(io, \"pwffn_size=$(ps), \")\n",
    "print(io, \"size=$(h)\")\n",
    "if Flux.istraining()\n",
    "print(io, \", dropout=$(td.drop.p))\")\n",
    "else\n",
    "print(io, \")\")\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer\n",
    "\n",
    "#layer auf jeden Fall ändern\n",
    "encode_t1 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t2 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t3 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t4 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer decoder\n",
    "\n",
    "#layer auf jeden Fall ändern\n",
    "decode_t1 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t2 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t3 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t4 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positionwise(Dense(512, 13), logsoftmax)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define the layer to get the final output probabilities\n",
    "\n",
    "#ÄNDERN\n",
    "linear = Positionwise(Dense(512, length(vocab)), logsoftmax) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neue Layer einfügen\n",
    "function encoder_forward(x)\n",
    "    e = embedding(x)\n",
    "    t1 = encode_t1(e)\n",
    "    t2 = encode_t2(t1)\n",
    "    t3 = encode_t2(t2)\n",
    "    t4 = encode_t2(t3)\n",
    "    return t2\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neue Layer einfügen\n",
    "function decoder_forward(x, m)\n",
    "    e = embedding(x)\n",
    "    t1 = decode_t1(e, m)\n",
    "    t2 = decode_t2(t1, m)\n",
    "    t3 = decode_t3(t2, m)\n",
    "    t4 = decode_t4(t3, m)\n",
    "    p = linear(t4)\n",
    "    return p\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512×12 Matrix{Float32}:\n",
       "  0.468389  -0.239751   -0.781162  …  -0.105302   0.371754   0.732726\n",
       "  0.625913   0.814828    0.276023      0.26693   -0.221515  -0.239351\n",
       "  0.786205   0.0389965  -0.453369     -0.109634   0.207264   0.981548\n",
       "  0.558399   0.457303   -0.205462      0.203294  -0.330861  -0.301439\n",
       " -0.499682  -1.24878    -1.74873      -1.56921   -1.52691   -0.971043\n",
       " -1.37978   -1.29783    -1.72479   …  -1.22446   -1.82067   -2.08222\n",
       " -0.627585  -1.32785    -1.76623      -1.43402   -1.37365   -0.915869\n",
       "  1.21342    1.18821     0.703239      1.48738    1.01276    0.644496\n",
       " -0.612075  -1.04587    -1.44484      -1.47799   -1.45016   -1.08645\n",
       "  3.12137    3.34021     2.92903       3.13671    2.57095    2.10821\n",
       "  ⋮                                ⋱              ⋮         \n",
       " -0.377055  -0.409071   -0.31353       0.100499   0.174365   0.170537\n",
       "  0.754467   0.782147    0.839845      1.12181    0.96634    1.1155\n",
       " -0.729594  -0.631009   -0.49565   …  -0.550436  -0.411324  -0.634024\n",
       " -0.732477  -0.47502    -0.248162     -0.185565  -0.136537  -0.0995348\n",
       " -1.90935   -2.03356    -1.94953      -1.60958   -1.49708   -1.41854\n",
       "  0.406964   0.319053    0.221853      0.416019   0.415823   0.415704\n",
       " -0.072691  -0.1456     -0.230929      0.310315   0.351012   0.438661\n",
       "  1.85316    1.86122     1.90724   …   2.30478    2.28354    2.27798\n",
       "  0.288759   0.344584    0.42298       0.510007   0.477909   0.549764"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bleibt so\n",
    "enc = encoder_forward(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×12 Matrix{Float32}:\n",
       " -3.44137   -3.44499   -3.45885   -3.48929   …  -3.53358  -3.60441  -3.65591\n",
       " -5.03886   -5.02966   -4.96638   -4.93876      -4.99748  -5.0417   -5.04876\n",
       " -4.72285   -4.70291   -4.71861   -4.69051      -4.41509  -4.45054  -4.43333\n",
       " -2.95765   -2.98813   -2.99307   -2.99243      -2.85001  -2.89214  -2.92647\n",
       " -3.3846    -3.44186   -3.44122   -3.48171      -3.14828  -3.172    -3.21538\n",
       " -2.32813   -2.30568   -2.24178   -2.17152   …  -2.01842  -1.95773  -1.87893\n",
       " -2.40015   -2.46888   -2.53813   -2.59019      -2.65999  -2.6486   -2.64615\n",
       " -3.1859    -3.2162    -3.23506   -3.26898      -3.14254  -3.0641   -3.00239\n",
       " -3.11701   -3.13662   -3.12898   -3.10877      -2.96538  -3.01154  -3.02196\n",
       " -2.81244   -2.81068   -2.78788   -2.74565      -2.7592   -2.71611  -2.64822\n",
       " -2.33343   -2.36884   -2.45145   -2.50322   …  -2.37291  -2.43201  -2.48044\n",
       " -2.89845   -2.87283   -2.80676   -2.76297      -2.54701  -2.54185  -2.55631\n",
       " -0.965391  -0.936751  -0.931517  -0.938103     -1.14324  -1.1497   -1.1776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bleibt so\n",
    "probs = decoder_forward(encoded_sample, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smooth (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hier vielleicht etwas ändern --> herausfinden, was diese funktion macht\n",
    "function smooth(et)\n",
    "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
    "    p = sm .* (1 .+ -et)\n",
    "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
    "    label\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define loss function\n",
    "\n",
    "#ÄNDERN\n",
    "function loss(x, y)\n",
    "    label = onehot(vocab, y) #turn the index to one-hot encoding\n",
    "    label = smooth(label) #perform label smoothing\n",
    "    enc = encoder_forward(x)\n",
    "    probs = decoder_forward(y, enc)\n",
    "    l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])  #erstmal so lassen\n",
    "    return l\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[0.19031276 -1.2930133 … -0.735448 0.57854205; -1.6189741 -1.0117838 … -0.41122213 0.6526874; … ; -0.47337812 -1.1268125 … -0.69679254 0.024595367; 0.27807164 0.569649 … 0.73205245 0.4195867], Float32[0.5403023 -0.41614684 … 0.4000682 0.9873536; 0.8218562 0.9364147 … 0.37902638 0.97646356; … ; 1.0 1.0 … 0.9943822 0.99437124; 0.0001 0.0002 … 0.10212166 0.10222114], Float32[0.042281516 -0.01930578 … 0.017422624 -0.03029542; -0.028011655 0.035027128 … 0.06337698 0.071344815; … ; -0.014897446 -0.076423936 … 0.003734632 -0.064617805; 0.016447447 0.005082075 … 0.05951089 -0.023200577], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.06712648 -0.006214203 … 0.031238861 0.025760138; 0.07226734 0.041060966 … -0.057337847 -0.0021938835; … ; 0.030360281 0.012778735 … 0.073901385 -0.057870988; 0.002712333 0.06016846 … -0.03696688 0.062364936], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.049399722 -0.057966437 … -0.045454208 -0.005138486; 0.030790126 0.025551211 … -0.056444414 0.004533385; … ; 0.02505977 -0.040762503 … -0.047532294 -0.052541204; 0.0014039818 -0.05856935 … -0.073594764 0.015241972], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.009893008 0.046507403 … -0.05679443 0.052726902; 0.06233349 0.015046295 … 0.05966804 -0.038100358; … ; -0.042844314 -0.026526077 … -0.06767857 0.04354822; 0.056635655 -0.02304556 … 0.013355147 0.01640237], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.03782194 -0.014893215 … 0.028293699 -0.02152202; -0.020805262 -0.0025507298 … -0.01827075 0.041065227; … ; 0.0058114436 0.03960045 … 0.02297814 -0.018718364; -0.022600496 -0.041941687 … -0.039010175 0.013131789], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.04757892 0.04615472 … -0.006312256 0.033575173; 0.011585559 0.024308054 … 0.0013064716 0.021434275; … ; -0.013590541 -0.013753589 … -0.0128760785 0.014673656; 0.005344877 0.023978565 … -0.004586969 -0.011545564], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.026717156 -0.05571105 … 0.03278368 -0.049950473; 0.037378713 0.0026654666 … -0.0073700193 -0.022488475; … ; 0.0104339225 -0.030471243 … -0.00030656555 -0.00083147554; -0.020745497 -0.044096526 … -0.01860087 0.00286943], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.05305323 0.03500074 … 0.069298886 0.017675955; 0.05371518 0.060311925 … 0.07644217 0.0014532025; … ; -0.06728748 0.06961364 … -0.03730918 -0.07182817; -0.031112825 0.02321213 … 0.041065656 0.017779816], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.058472313 0.062096167 … -0.051941633 -0.05885876; 0.0021902153 -0.073072776 … 0.07277134 -0.021066353; … ; 0.060019758 0.007793678 … -0.05473967 -0.04787662; 0.040263582 0.06037122 … 0.026588457 -0.05095671], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.05478599 0.06108812 … -0.039833006 0.053156726; 0.028035253 0.062769085 … 0.046628732 0.069196194; … ; 0.047688153 0.062114872 … 0.019291747 -0.057612456; 0.031069353 -0.03992785 … -0.049426712 -0.01922581], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.0386561 0.016353754 … -0.021291496 -0.03343168; 0.038859062 0.002318416 … 0.026124261 0.028706778; … ; -0.019696465 0.010745734 … -0.011486329 -0.010769408; 0.028721737 -0.01601031 … 0.03028347 -0.038584743], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.023689464 -0.031095766 … -0.040158942 0.028562753; 0.0052547194 0.030889364 … 0.012575734 -0.037241; … ; 0.01617898 -0.035344623 … 0.026277255 -0.03285308; -0.024262704 -0.020248838 … 0.015891667 -0.03891428], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.031497866 -0.051547725 … 0.040320195 -0.041749306; -0.07542102 0.039770737 … -0.07638572 -0.050979726; … ; -0.045581758 0.05195742 … -0.031994235 0.031264227; -0.009051074 0.04066901 … 0.04985535 -0.004163656], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.023445766 0.04731417 … 0.023708513 0.05421184; -0.06733793 -0.04367194 … -0.015177805 -0.0032247235; … ; 0.0041800626 0.057233073 … 0.009183261 0.045283295; -0.039556865 0.006202541 … -0.028198883 -0.01957309], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.07145926 0.001876368 … 0.053850595 0.03119621; -0.04445715 0.012786127 … -0.061300587 -0.052775063; … ; -0.07467793 -0.0303774 … -0.037702654 0.028286647; 0.02285625 0.026247198 … -0.012267604 0.06990534], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.047669884 0.028384048 … -0.020014033 0.017665826; -3.8325256f-6 0.07344234 … 0.01887309 0.05853358; … ; -0.060953964 0.018563367 … 0.07240756 0.054808564; 0.049254615 0.04421331 … -0.0022522474 -0.008005525], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.003725041 -0.012479125 … 0.048406947 -0.030695647; -0.023076609 0.016502894 … 0.025590505 0.009723356; … ; -0.009037691 -0.038253732 … -0.00420532 -0.04739873; -0.011568753 -0.032200612 … -0.033463445 -0.008315092], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.038781483 0.0077357213 … 0.0036195552 -0.005710101; 0.03445588 -0.0024403844 … 0.02472635 -0.046585098; … ; 0.0061146044 -0.0028178669 … 0.007843066 0.01001292; -0.0036958503 0.03571879 … -0.021697396 0.0400769], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.031530168 0.0029629802 … 0.053599477 -0.0054769525; -0.0024191996 0.04294533 … 0.021854576 -0.021274148; … ; -0.044489488 -0.0047484445 … 0.03112248 0.0048857946; 0.07397708 -0.036320042 … -0.05798859 -0.024929775], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.028891256 -0.05302726 … -0.044364695 -0.07408989; 0.035616226 -0.033308953 … -0.07296457 -0.016544484; … ; 0.016184282 -0.03828286 … -0.057194877 0.04388711; 0.011875299 -0.036683436 … -0.04450175 -0.025237089], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.00726322 -0.00066999847 … 0.04329345 -0.045348268; 0.001238727 0.041969165 … -0.04051284 -0.0023330774; … ; -0.020167133 0.019689947 … -0.049123287 -0.07280214; 0.026409732 0.05140645 … 0.042802557 -0.0028033464], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.02878776 -0.04512691 … -0.042064104 -0.063114375; -0.045034345 0.008465519 … 0.021037681 -0.044101417; … ; 0.055430766 -0.02715764 … -0.014785537 -0.0021637161; 0.06429109 -0.021054417 … 0.031152172 -0.05657452], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.021669775 -0.035165843 … -0.037566718 -0.0057501188; 0.016632989 -0.028253151 … -0.016765367 -0.012133637; … ; 0.03275631 0.012116001 … 0.0028644404 0.0071961493; 0.010005786 -0.037253607 … -0.008975859 0.007026222], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.00073127967 -0.042379778 … -0.013714125 0.0053124772; 0.044482097 -0.04689494 … -0.037957266 -0.0024119325; … ; 0.021013867 -0.02563423 … 0.011492193 -0.0133436145; -0.016192796 0.04022396 … 0.041406557 0.027024105], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.04636478 0.00897092 … 0.02545266 -0.059444807; -0.011725064 0.07212725 … 0.035219267 -0.07182456; … ; 0.07421248 0.047311213 … -0.014228105 0.0111938575; 0.03410796 0.028332036 … -0.05595407 0.006660528], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.022661814 -0.03619125 … -0.013118698 -0.0011867324; -0.011159256 0.06557391 … 0.049304783 -0.053569216; … ; -0.03408431 -0.051919058 … 0.011446512 0.07030737; -0.0056450544 0.038259793 … -0.0022562626 -0.056436583], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.05884943 0.047062155 … 0.067638256 -0.03148312; -0.06452486 -0.068336174 … 0.07407378 -0.0686901; … ; 0.027556425 0.05541547 … 0.06754045 -0.05743337; 0.046045788 0.06355694 … -0.04898681 0.034135703], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.021920623 0.0135195255 … 0.035857912 0.06471998; -0.030627353 0.0675157 … 0.032088898 -0.010183604; … ; -0.03734778 0.07103743 … 0.041168004 -0.035638437; -0.072496384 0.042022273 … -0.0495188 0.06572633], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.06821965 0.0761869 … -0.04411854 0.05600015; 0.047592923 0.04055186 … -0.06499226 -0.06403172; … ; -0.03231248 0.017987976 … 0.060100954 -0.056388676; -0.07236487 -0.002202516 … 0.054514848 0.01583291], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.0061573354 -0.050040994 … 0.016632887 -0.043240286; 0.0047204304 -0.022918757 … 0.045550823 -0.028419144; … ; 0.0018578623 0.026189435 … 0.017739227 -0.028187769; 0.008399381 0.07117985 … 0.021130046 0.05625224], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.047356438 0.03881903 … 0.07236916 0.06390755; -0.031430613 0.0750245 … -0.044879183 0.0036874735; … ; -0.06087848 0.051618006 … 0.057410594 -0.07304033; -0.025948716 0.052978862 … -0.03454757 0.057581507], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.056589484 0.029782975 … 0.065740846 0.052964132; 0.024387728 -0.050720263 … -0.045989685 -0.043062933; … ; 0.048799146 0.053887825 … -0.07551851 -0.014062175; -0.047202334 0.027242376 … 0.037492193 -0.029763447], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.010000789 -0.045400735 … -0.002624428 0.019352756; -0.02630071 -0.004032761 … -0.031168759 0.015526488; … ; -0.0084034605 0.026063826 … 0.034797974 -0.046701655; -0.046322554 0.022601558 … 0.013095592 -0.032696765], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.036722876 -0.03012356 … -0.04450285 -0.016984824; -0.039892703 -0.021012621 … 0.019258155 0.01855624; … ; -0.036581285 -0.010297532 … 0.034716543 -0.010763209; 0.0063562673 0.017755533 … -0.02923415 0.03068616], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.035825208 -0.020351622 … 0.04619628 -0.053359833; -0.021643147 0.016114766 … -0.049283046 0.050756726; … ; -0.015199486 0.022112286 … 0.041256826 0.045493573; -0.028123163 -0.07216536 … 0.060030308 -0.0525617], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.052236155 0.018124579 … 0.028726932 -0.038935173; -0.06671483 -0.054018937 … 0.06292335 -0.01751216; … ; -0.061541982 -0.015072265 … -0.03714343 0.051261432; -0.04147247 -0.052672315 … 0.065364726 -0.018060923], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.065282255 0.046160616 … -0.062308542 0.0087714465; -0.06638869 0.0043735136 … -0.036842816 -0.0036660295; … ; -0.063721776 -0.07609461 … 0.06652228 -0.06721963; 0.010432646 0.06802873 … 0.021911168 -0.06109117], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.031083424 -0.012869693 … 0.04645813 0.040895674; -0.008850853 0.031270288 … -0.011321335 0.014906042; … ; 0.009427392 -0.056932714 … 0.047960114 0.052316584; 0.047899086 0.011551852 … 0.01807532 -0.028689701], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.032284062 -0.0560211 … -0.02750441 0.05931676; 0.069698125 -0.0046845507 … -0.0023910943 0.048601843; … ; 0.054951243 -0.011471826 … -0.00062503014 -0.068202525; -0.05633555 -0.0077878926 … -0.03602282 0.044683196], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.064031936 -0.025774008 … -0.061410893 0.026224859; -0.009499316 0.010540942 … -0.047782633 0.03483377; … ; -0.052719217 0.045010824 … 0.006387013 -0.06432735; 0.026748272 0.04990589 … -0.026410919 -0.0027773948], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.052260228 0.003218482 … 0.050404772 -0.020700347; -0.0038669088 -0.021900218 … 0.0036855026 -0.022085968; … ; -0.052924912 0.05489474 … -0.073732555 -0.03875678; -0.032141384 -0.05182668 … 0.07545308 -0.0334716], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.012573914 0.041036017 … 0.05069566 0.06317978; -0.01971358 0.0064799613 … 0.027794844 0.029507106; … ; 0.058481473 0.056018874 … 0.050799467 -0.0010567368; 0.034124807 0.07539476 … 0.0335229 0.06062818], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0079307305 0.024916893 … -0.013305825 -0.044360846; -0.0034319912 0.039237097 … 0.043966383 0.030377597; … ; -0.039311778 -0.0113079185 … -0.010090739 -0.04091466; 0.00990673 -0.01133248 … -0.008650548 -0.022293538], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.020817993 -0.03043802 … -0.0058989115 -0.030855244; -0.0013688698 -0.014379879 … 0.019017357 0.009888989; … ; 0.0052741915 0.04251825 … -0.029433845 -0.0005301997; 0.045153104 0.017068401 … -0.010235964 -0.010461434], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.03203544 -0.0447816 … 0.05828534 0.035103854; -0.010381564 -0.024780288 … -0.0029216437 0.0293111; … ; 0.07228114 -0.013503812 … 0.05913857 0.022085348; 0.03478864 0.006648136 … 0.068539806 0.071168706], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.023331266 -0.02714795 … -0.018511627 -0.027471269; -0.005159309 -0.0005143979 … 0.016056292 0.040994044; … ; -0.014153352 0.011579812 … 0.060145628 0.034540728; -0.033704232 0.04157416 … 0.0652121 0.00431703], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0610554 0.048539482 … -0.012286566 -0.0735549; -0.047448363 -0.06782322 … -0.07302813 -0.04057075; … ; -0.022720598 -0.046687953 … -0.026815323 -0.068988286; 0.049149822 -0.037845023 … 0.0611578 0.057660475], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.044700697 0.0053780917 … -0.021273455 0.020498555; 0.002000195 0.06931369 … -0.052526202 -0.041836962; … ; -0.062306736 -0.017809618 … -0.017348455 0.061670903; -0.034988675 -0.07453408 … 0.06294241 -0.065343134], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.038558144 0.006307552 … -0.054054286 0.04584166; 0.027381241 -0.020806855 … -0.056477025 0.06263471; … ; -0.0012831113 0.05232799 … 0.044502154 -0.040558886; -0.027089586 -0.033634827 … 0.054141086 -0.041321926], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.016110422 -0.0023122355 … -0.023829037 -0.009969348; 0.022028372 -0.03753351 … -0.059733104 -0.07344816; … ; -0.03379457 -0.052311417 … -0.054456882 0.05341867; 0.03674923 0.05851825 … 0.071109556 -0.01887099], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.019698031 -0.010558389 … 0.03373515 -0.0498772; 0.060344774 -0.037522376 … 0.035064943 -0.051287696; … ; -0.06928296 -0.04407063 … -0.04057929 0.015322126; 0.030598136 0.0042633745 … -0.020141363 0.06099185], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.03275265 -0.03088866 … 0.07494365 0.015302343; -0.07535371 0.049465988 … 0.0354667 0.031339858; … ; -0.029466536 0.05776399 … -0.05012436 -0.07352777; -0.03190519 -0.015628144 … -0.05965346 0.030155733], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.020745253 -0.0072026593 … -0.012114869 -0.0026425957; -0.022968872 -0.009098577 … -0.009369732 0.039005857; … ; 0.018704697 -0.0032533035 … -0.04320911 -0.008782616; -0.025111832 0.022385992 … 0.005430452 -0.016100965], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.022166213 -0.0016728501 … 0.031111082 0.013764254; -0.041684777 0.0076054884 … -0.03803934 -0.00022549213; … ; -0.04744009 -0.025528535 … 0.011839307 0.006505014; -0.04425034 -0.018758716 … -0.02267971 0.004444178], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.015693152 0.022628836 … -0.01859576 -0.057295654; 0.0031801749 -0.037250288 … -0.04408574 -0.02774566; … ; -0.018161006 0.035063885 … -0.0513865 0.04623997; 0.04243896 0.064367555 … -0.044941198 -0.07613202], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.037216358 -0.04197296 … -0.039793424 0.023660626; -0.067897275 0.00055186544 … -0.037447535 -0.06414905; … ; -0.05972281 0.05468616 … -0.038296144 -0.04604263; -0.03545557 0.068540536 … -0.0013219293 -0.013228491], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.021721294 0.057613242 … -0.014666965 0.014518264; -0.029300697 0.0028764382 … -0.0014020473 -0.005817591; … ; -0.038938332 -0.041692894 … 0.07354741 0.061051566; -0.066197075 -0.04162172 … 0.06895609 0.03525447], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.03472896 0.037572473 … 0.01919378 0.023384154; 0.018751215 -0.0764742 … 0.045752306 -0.062558055; … ; -0.043024443 0.042392112 … 0.006449812 -0.03783732; 0.049078226 0.0650344 … -0.03932432 0.040655997], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.031155493 0.011167231 … 0.0022065493 -0.02079032; 0.06717945 -0.07185759 … 0.027670853 -0.047588285; … ; -0.046381168 0.07100854 … 0.07552749 0.032543216; -0.032252874 -0.046945736 … -0.05143596 0.055919085], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.06054722 0.0025367122 … -0.062742695 0.008318077; -0.0067856326 0.054204796 … 0.0080608595 -0.048343293; … ; -0.023108851 -0.008960572 … 0.07485955 0.06459799; 0.00026990104 -0.06205501 … 0.011625437 0.07370049], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0674688 0.017490752 … -0.074464366 0.06973711; 0.062867545 -0.06305599 … 0.04255401 -0.039826382; … ; -0.031814158 0.02893743 … 0.028730253 -0.06400153; -0.06708579 -0.0070329765 … -0.065201804 -0.019347154], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.021847896 -0.029384283 … -0.014978549 -0.0643183; 0.019522958 -0.07124054 … 0.056697596 0.023495827; … ; -0.022753375 0.0619715 … -0.005276822 -0.0005422658; 0.069171116 -0.012116384 … -0.06394277 -0.04867742], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.02556958 0.015744882 … 0.02931901 -0.041119143; 0.026261304 0.046036016 … -0.046310194 0.027564393; … ; 0.038411666 0.03187924 … 0.03279994 0.0015008454; -0.018862598 0.040039696 … 0.0237755 -0.0391569], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.043115098 0.02227904 … -0.039389342 -0.008720991; -0.019753924 -0.0042850547 … -0.02894005 -0.0061308905; … ; -0.014248077 0.03509593 … -0.045013614 -0.0448533; -0.0361704 0.0326282 … -0.013817442 -0.038581606], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.025521765 -0.00012091516 … -0.029346956 0.07646196; -0.019508911 -0.0066512004 … -0.048645005 -0.008195698; … ; 0.060631536 0.06796046 … 0.09972221 0.04188915; -0.050385125 -0.09321325 … -0.0058955317 -0.00787149], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collect all the parameters\n",
    "\n",
    "#Layer neu einfügen\n",
    "ps = params(embed, pe, encode_t1, encode_t2, encode_t3, encode_t4, decode_t1, decode_t2, decode_t3, decode_t4, linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = ADAM(1e-4) #bleibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define training loop\n",
    "\n",
    "#ÄNDERN siehe Paper Kapitel 4.1\n",
    "function train!()\n",
    "    @info \"start training\"\n",
    "    for i = 1:1000\n",
    "      data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
    "      x, y = preprocess.(data[1]), preprocess.(data[2])\n",
    "      x, y = vocab(x), vocab(y) #encode the data\n",
    "      x, y = todevice(x, y) #move to gpu\n",
    "      grad = gradient(()->loss(x, y), ps)\n",
    "      if i % 8 == 0\n",
    "          l = loss(x, y)\n",
    "          println(\"loss = $l\")\n",
    "      end\n",
    "      update!(opt, ps, grad)\n",
    "    end\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: start training\n",
      "└ @ Main /Users/johannes/Documents/GitHub/DM2022-LinearTransformers/test/Duplicat_Exercise11.ipynb:5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 98.160194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 82.9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 80.39799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.87157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.95355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.987305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.83693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.097336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 74.490425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 70.03486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 81.51522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.16063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 69.51718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.902504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.145164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.7446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.04689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 69.38445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.55533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.04718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.77383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.90678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.714554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.290764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.637146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.02257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.77636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.34296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.960815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.92556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.03175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.51673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.63211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.717575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.811134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.47161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.70263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.44575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.06803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.18078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.03054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.63311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.84321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.1288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 65.22694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 65.533554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 63.550385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 65.496376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 63.701496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 63.271893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 62.85361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 63.98697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 61.631947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.7772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 63.062912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 62.18087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 61.933285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.46203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.311237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.86598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.990395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 59.949135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 56.63542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.036133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.409225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.94593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 56.399536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.872807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.368874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 55.935432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 54.17845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 53.758957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 52.44097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 49.527534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 48.014885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 46.931023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 42.00409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 35.82827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 26.470001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 20.075684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 26.06121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 10.524798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 6.6960444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 23.615046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 3.5812197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.9342576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.23604898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.101361446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.042464577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.022816483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.03392332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.060574565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.3739128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.1695309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.26118833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.07519782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.03608935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.020990444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0063413493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0034085002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0024671757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.001659455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0015080053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0020414505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0011876135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0009857521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0014335534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.000995112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0015654472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0007677208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0013412336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.00088303536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.00086750847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0045710197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.000773423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 11.905792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.321671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.37864628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.09742916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.018595252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.009129214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.009761279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0046296967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0031574233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0061669056\n"
     ]
    }
   ],
   "source": [
    "train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#erst einmal so lassen, vielleicht einzelne Änderungen\n",
    "function translate(x)\n",
    "    ix = todevice(vocab(preprocess(x)))\n",
    "    seq = [startsym]\n",
    "\n",
    "    enc = encoder_forward(ix)\n",
    "\n",
    "    len = length(ix)\n",
    "    for i = 1:2len\n",
    "        trg = todevice(vocab(seq))\n",
    "        dec = decoder_forward(trg, enc)\n",
    "        #move back to gpu due to argmax wrong result on CuArrays\n",
    "        ntok = onecold(collect(dec), labels)\n",
    "        push!(seq, ntok[end])\n",
    "        ntok[end] == endsym && break\n",
    "    end\n",
    "  seq[2:end-1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"1\"\n",
       " \"7\"\n",
       " \"3\"\n",
       " \"4\"\n",
       " \"6\"\n",
       " \"6\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(map(string, [5,5,6,6,1,12,3,4,6]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
