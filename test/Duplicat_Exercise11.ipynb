{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\tusing Flux\n",
    "\tusing Flux: onehot\n",
    "\tusing Flux: gradient\n",
    "\tusing Flux.Optimise: update!\n",
    "\tusing Flux: onecold\n",
    "\tusing CUDA\n",
    "\tusing Transformers\n",
    "\tusing Transformers.Basic \n",
    "    using Transformers.Datasets: batched\n",
    "    enable_gpu(false) \n",
    "\tusing Flux: @functor\n",
    "\tusing ..Transformers: Abstract3DTensor, Container, epsilon, batchedmul, batched_triu!\n",
    "end\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary{String}(13, unk=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Labeling: vielleicht ändern\n",
    "\n",
    "begin\n",
    "\tlabels = map(string, 1:10)\n",
    "\tstartsym = \"11\"\n",
    "\tendsym = \"12\"\n",
    "\tunksym = \"0\"\n",
    "\tlabels = [unksym, startsym, endsym, labels...]\n",
    "\tvocab = Vocabulary(labels, unksym)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for generate training datas \n",
    "\n",
    "#nichts ändern\n",
    "sample_data() = (d = map(string, rand(1:10, 10)); (d,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"3\", \"3\", \"6\", \"1\", \"8\", \"1\", \"7\", \"2\", \"8\", \"7\"], [\"3\", \"3\", \"6\", \"1\", \"8\", \"1\", \"7\", \"2\", \"8\", \"7\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "\n",
    "sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for adding start & end symbol\n",
    "\n",
    "#nichts ändern\n",
    "preprocess(x) = [startsym, x..., endsym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_ex = preprocess.(sample_data()) = ([\"11\", \"4\", \"10\", \"4\", \"10\", \"6\", \"3\", \"10\", \"3\", \"5\", \"2\", \"12\"], [\"11\", \"4\", \"10\", \"4\", \"10\", \"6\", \"3\", \"10\", \"3\", \"5\", \"2\", \"12\"])\n",
      "encoded_sample_ex = vocab(sample_ex[1]) = [2, 7, 13, 7, 13, 9, 6, 13, 6, 8, 5, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       "  7\n",
       " 13\n",
       "  7\n",
       " 13\n",
       "  9\n",
       "  6\n",
       " 13\n",
       "  6\n",
       "  8\n",
       "  5\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "\n",
    "begin\n",
    "    @show sample_ex = preprocess.(sample_data())\n",
    "    @show encoded_sample_ex = vocab(sample_ex[1]) #use Vocabulary to encode the training data\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"11\", \"2\", \"3\", \"4\", \"3\", \"9\", \"7\", \"1\", \"8\", \"2\", \"10\", \"12\"], [\"11\", \"2\", \"3\", \"4\", \"3\", \"9\", \"7\", \"1\", \"8\", \"2\", \"10\", \"12\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "sample = preprocess.(sample_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       "  5\n",
       "  6\n",
       "  7\n",
       "  6\n",
       " 12\n",
       " 10\n",
       "  4\n",
       " 11\n",
       "  5\n",
       " 13\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern\n",
    "encoded_sample = vocab(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embed(512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a Word embedding layer which turn word index to word vector\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "embed = Embed(512, length(vocab)) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionEmbedding(512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a position embedding layer metioned above\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "pe = PositionEmbedding(512) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wrapper for get embedding\n",
    "\n",
    "#embeding vielleicht ändern\n",
    "function embedding(x)\n",
    "    we = embed(x, inv(sqrt(512)))\n",
    "    e = we .+ pe(we)\n",
    "    return e\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractAttention end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_atten_mask1 (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_atten_mask1(T::Type, score::AbstractArray, ::Nothing, future) = create_atten_mask1(T, score, fill!(similar(score, size(score,1), size(score, 2), 1), one(T)), future)\n",
    "function create_atten_mask1(T::Type, score::AbstractArray, _mask::AbstractArray, future::Bool=false)\n",
    "  #size(mask) == (q, k, n, b)\n",
    "\n",
    "  # ql, kl = size(mask)\n",
    "  mask = copy(_mask)\n",
    "\n",
    "  maskval = convert(T, -1e9)\n",
    "  !future && batched_triu!(mask, 0)\n",
    "  mask .= (1 .- mask) .* maskval\n",
    "  return mask\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd create_atten_mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MultiheadAttention_1{Q<:Dense, K<:Dense, V<:Dense, O<:Dense, DP<:Dropout} <: AbstractAttention\n",
    "    head::Int\n",
    "    future::Bool\n",
    "    iqproj::Q\n",
    "    ikproj::K\n",
    "    ivproj::V\n",
    "    oproj::O\n",
    "    drop::DP\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.functor(mh::MultiheadAttention_1) = (mh.iqproj, mh.ikproj, mh.ivproj, mh.oproj), m -> MultiheadAttention_1(mh.head, mh.future, m..., mh.drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;\\n                       future::Bool=true, pdrop = 0.1)\\nMultihead dot product Attention Layer, `head` is the number of head, \\n`is` is the input size, `hs` is the hidden size of input projection layer of each head, \\n`os` is the output size. When `future` is `false`, the k-th token can't see tokens at > k. \\n`pdrop` is the dropout rate.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;\n",
    "                       future::Bool=true, pdrop = 0.1)\n",
    "Multihead dot product Attention Layer, `head` is the number of head, \n",
    "`is` is the input size, `hs` is the hidden size of input projection layer of each head, \n",
    "`os` is the output size. When `future` is `false`, the k-th token can't see tokens at > k. \n",
    "`pdrop` is the dropout rate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiheadAttention_1(head::Int,\n",
    "                   is::Int,\n",
    "                   hs::Int,\n",
    "                   os::Int;\n",
    "                   future::Bool=true, pdrop = 0.1) = MultiheadAttention_1(head,\n",
    "                                                                        future,\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(hs*head, os),\n",
    "                                                                        Dropout(pdrop),\n",
    "                                                                        )\n",
    "\n",
    "\n",
    "function Base.show(io::IO, mh::MultiheadAttention_1)\n",
    "    hs = div(size(mh.iqproj.weight)[1], mh.head)\n",
    "    is = size(mh.iqproj.weight)[end]\n",
    "    os = size(mh.oproj.weight)[1]\n",
    "\n",
    "    print(io, \"MultiheadAttention(\")\n",
    "    print(io, \"head=$(mh.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"$(is)=>$(os)\")\n",
    "\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(mh.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (mh::MultiheadAttention_1)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: Abstract3DTensor{T},\n",
    "                         A2 <: Abstract3DTensor{T},\n",
    "                         A3 <: Abstract3DTensor{T}}\n",
    "qs = size(query)\n",
    "ks = size(key)\n",
    "vs = size(value)\n",
    "\n",
    "#size(ipq) == (h, q_seq_len, batch)\n",
    "ipq = @toNd mh.iqproj(query)\n",
    "ipk = @toNd mh.ikproj(key)\n",
    "ipv = @toNd mh.ivproj(value)\n",
    "\n",
    "h = size(ipq, 1)\n",
    "hs = div(h, mh.head)\n",
    "\n",
    "#size(ipq) == (hs, q_seq_len, head, batch)\n",
    "ipq = permutedims(reshape(ipq, hs, mh.head, qs[2], qs[3]), [1, 3, 2, 4])\n",
    "ipk = permutedims(reshape(ipk, hs, mh.head, ks[2], ks[3]), [1, 3, 2, 4])\n",
    "ipv = permutedims(reshape(ipv, hs, mh.head, vs[2], vs[3]), [1, 3, 2, 4])\n",
    "\n",
    "#size(ipq) == (hs, q_seq_len, head * batch)\n",
    "ipq = reshape(ipq, hs, qs[2], :)\n",
    "ipk = reshape(ipk, hs, ks[2], :)\n",
    "ipv = reshape(ipv, hs, vs[2], :)\n",
    "\n",
    "atten = attention(ipq,ipk,ipv,\n",
    "mask,\n",
    "mh.future,\n",
    "mh.drop)\n",
    "\n",
    "atten = permutedims(reshape(atten, hs, qs[2], mh.head, qs[3]), [1, 3, 2, 4]) #size(atten) == (hs, head, ql, b)\n",
    "atten = reshape(atten, h, qs[2], qs[3]) #size(atten) == (h, ql, b)\n",
    "\n",
    "out = @toNd mh.oproj(atten)\n",
    "out #size(out) == (h, q_seq_len, batch)\n",
    "end\n",
    "\n",
    "function (mh::MultiheadAttention_1)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: AbstractMatrix{T},\n",
    "                         A2 <: AbstractMatrix{T},\n",
    "                         A3 <: AbstractMatrix{T}}\n",
    "\n",
    "# size(query) == (dims, seq_len)\n",
    "ipq = mh.iqproj(query)\n",
    "ipk = mh.ikproj(key)\n",
    "ipv = mh.ivproj(value)\n",
    "\n",
    "h = size(ipq)[1] #h == hs * head\n",
    "hs = div(h, mh.head)\n",
    "\n",
    "#size(hq) == (hs, seq_len, head)\n",
    "hq = permutedims(reshape(ipq, hs, mh.head, :), [1, 3, 2])\n",
    "hk = permutedims(reshape(ipk, hs, mh.head, :), [1, 3, 2])\n",
    "hv = permutedims(reshape(ipv, hs, mh.head, :), [1, 3, 2])\n",
    "\n",
    "atten = attention(hq, hk, hv,\n",
    "mask,\n",
    "mh.future,\n",
    "mh.drop)\n",
    "\n",
    "# size(atten) == (head*hs, seq_len)\n",
    "atten = reshape(permutedims(atten, [1, 3, 2]), h, :)\n",
    "\n",
    "mh.oproj(atten)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function CausalLinearAttention(queryDimensions)\n",
    "    feature_map = nelu(queryDimensions)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_sizes_compatible (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function make_sizes_compatible(Q, K)\n",
    "    N, L, H, E = Q.shape\n",
    "    _, S, _, _ = K.shape\n",
    "        if L == S\n",
    "            return Q, K\n",
    "        end\n",
    "        if L < S\n",
    "            return Q, K[:, :L, :, :]\n",
    "        end\n",
    "        if L > S\n",
    "            return Q, torch.cat([K, K.new_zeros(N, L-S, H, E)], dim=1)\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward(queries, keys, values, attn_mask, query_lengths, key_lengths)\n",
    "    Q = nelu(queries)\n",
    "    K = nelu(keys)\n",
    "    K = K * key_lengths.float_matrix[:, :, None, None]\n",
    "\n",
    "    Q, K = make_sizes_compatible(Q, K)\n",
    "\n",
    "    Z = 1/(torch.einsum(\"nlhi,nlhi->nlh\", Q, K.cumsum(1)) + self.eps)  # diese Funktionen und Packages gehen nur mit Torch und CUDA GPU!!\n",
    "\n",
    "    # Compute the unnormalized result\n",
    "    V = causal_linear(\n",
    "        Q,\n",
    "        K,\n",
    "        values\n",
    "    )\n",
    "\n",
    "    return V * Z[:, :, :, None]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_mask1 (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apply_mask1(score, mask)\n",
    "    s = size(score)\n",
    "    ms = size(mask)\n",
    "    bxn = s[end]\n",
    "    b = ms[end]\n",
    "    if bxn == b || b == 1\n",
    "      return score .+ mask\n",
    "    else\n",
    "      return reshape(reshape(score, s[1:end-1]..., :, b) .+\n",
    "                     reshape(mask, ms[1:end-1]..., 1, b), s)\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  apply_mask1(score::AbstractArray{T}, ::Nothing, future) where T = future ? score : apply_mask1(score, create_atten_mask1(T, score, nothing, future))\n",
    "  apply_mask1(score::AbstractArray{T}, mask, future) where T = apply_mask(score, create_atten_mask1(T, score, mask, future))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function attention(query::A1,\n",
    "    key::A2,\n",
    "    value::A3,\n",
    "    mask, future::Bool,\n",
    "    dropout) where {T,\n",
    "                    A1 <: Abstract3DTensor{T},\n",
    "                    A2 <: Abstract3DTensor{T},\n",
    "                    A3 <: Abstract3DTensor{T}}\n",
    "#size(query) == (dims, {q,k}_seq_len, batch) == size(key) == size(value)\n",
    "#size(score) == (k_seq_len, q_seq_len, batch)\n",
    "dk = size(key, 1)\n",
    "score = batchedmul(key, query; transA = true)\n",
    "score = score ./ convert(T, sqrt(dk))\n",
    "\n",
    "score = apply_mask1(score, mask, future)\n",
    "score = softmax(score; dims=1)\n",
    "dropout !== nothing && (score = dropout(score))\n",
    "batchedmul(value, score) #size(return) == (dims, q_seq_len, batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "causal_linear (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function causal_linear(query, key, value)\n",
    "    query = query.permute(0,2,1,3).contiguous()\n",
    "    key = key.permute(0,2,1,3).contiguous()\n",
    "    value = value.permute(0,2,1,3).contiguous()\n",
    "    value_new = causal_dot_product(Q, K, V)\n",
    "    return value_new.permute(0,2,1,3).contiguous()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractTransformer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nelu (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oftf(x, y) = oftype(float(x), y)\n",
    "\n",
    "\n",
    "nelu(x, α=1) = ifelse(x ≥ 0, float(x)+1, @fastmath oftf(x, α) * (exp(x) - 1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PwFFN{Di<:Dense, Do<:Dense}\n",
    "    din::Di\n",
    "    dout::Do\n",
    "end\n",
    "\n",
    "@functor PwFFN\n",
    "\n",
    "\n",
    "\"just a wrapper for two dense layer.\"\n",
    "PwFFN(size::Int, h::Int, act = elu) = PwFFN(  #relu vielleicht auf elu + 1 oder nur elu ändern\n",
    "    Dense(size, h, act),\n",
    "    Dense(h, size)\n",
    ")\n",
    "\n",
    "function (pw::PwFFN)(x::AbstractMatrix)\n",
    "  # size(x) == (dims, seq_len)\n",
    "  pw.dout(pw.din(x))\n",
    "end\n",
    "\n",
    "function (pw::PwFFN)(x::A) where {T, N, A<:AbstractArray{T, N}}\n",
    "  new_x = reshape(x, size(x, 1), :)\n",
    "  y = pw(new_x)\n",
    "  return reshape(y, Base.setindex(size(x), size(y, 1), 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Transformer1{MA<:MultiheadAttention_1, LA<:LayerNorm, P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "    mh::MA\n",
    "    mhn::LA\n",
    "    pw::P\n",
    "    pwn::LP\n",
    "    drop::DP\n",
    "end\n",
    "\n",
    "@functor Transformer1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Transformer(size::Int, head::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)\n",
    "    Transformer(size::Int, head::Int, hs::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)  \n",
    "\n",
    "Transformer layer.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "When `future` is `false`, the k-th token can't see the j-th tokens where j > k. `pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "\n",
    "########\n",
    "#ganze Funktion von Transformer ändern\n",
    "########\n",
    "function Transformer1(size::Int, head::Int, ps::Int; future::Bool = true, act = elu, pdrop = 0.1)  #relu vielleicht wieder auf elu ändern\n",
    "    rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "    Transformer1(size, head, div(size, head), ps;future=future, act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "Transformer1(size::Int, head::Int, hs::Int, ps::Int; future::Bool = true, act = elu, pdrop = 0.1) = Transformer1(\n",
    "    MultiheadAttention_1(head, size, hs, size; future=future, pdrop=pdrop),\n",
    "    LayerNorm(size),\n",
    "    PwFFN(size, ps, act),\n",
    "    LayerNorm(size),\n",
    "    Dropout(pdrop),     #braucht man das? vielleicht nicht\n",
    ")\n",
    "\n",
    "function (t::Transformer1)(x::A, mask=nothing) where {T, N, A<:AbstractArray{T, N}}\n",
    "    dropout = t.drop\n",
    "    a = t.mh(x, x, x; mask=mask)\n",
    "    a = dropout(a)\n",
    "    res_a = x + a\n",
    "    res_a = t.mhn(res_a)\n",
    "    pwffn = t.pw(res_a)\n",
    "    pwffn = dropout(pwffn)\n",
    "    res_pwffn = res_a + pwffn\n",
    "    res_pwffn = t.pwn(res_pwffn)\n",
    "    res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, t::Transformer1) #zum visualisieren --> nice to have\n",
    "    hs = div(size(t.mh.iqproj.weight)[1], t.mh.head)\n",
    "    h, ps = size(t.pw.dout.weight)\n",
    "\n",
    "    print(io, \"Transformer(\")\n",
    "    print(io, \"head=$(t.mh.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"pwffn_size=$(ps), \")\n",
    "    print(io, \"size=$(h)\")\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(t.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "    #auch diese Funktion ändern\n",
    "    ###############\n",
    "    struct TransformerDecoder1{MA<:MultiheadAttention_1, LA<:LayerNorm,\n",
    "        IMA<:MultiheadAttention_1, ILA<:LayerNorm,\n",
    "        P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "mh::MA\n",
    "mhn::LA\n",
    "imh::IMA\n",
    "imhn::ILA\n",
    "pw::P\n",
    "pwn::LP\n",
    "drop::DP\n",
    "end\n",
    "\n",
    "@functor TransformerDecoder1\n",
    "\n",
    "\"\"\"\n",
    "TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "TransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "\n",
    "TransformerDecoder layer. Decode the value from a Encoder.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "`pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "function TransformerDecoder1(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "TransformerDecoder1(size, head, div(size, head), ps; act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "TransformerDecoder1(size::Int, head::Int, hs::Int, ps::Int; act = elu, pdrop = 0.1) = TransformerDecoder1(\n",
    "MultiheadAttention_1(head, size, hs, size; future=false, pdrop=pdrop),\n",
    "LayerNorm(size),\n",
    "MultiheadAttention_1(head, size, hs, size; future=true, pdrop=pdrop), #future = true --> Unterschied zu oben??\n",
    "LayerNorm(size),\n",
    "PwFFN(size, ps, act),\n",
    "LayerNorm(size),\n",
    "Dropout(pdrop),\n",
    ")\n",
    "\n",
    "function (td::TransformerDecoder1)(x::AbstractArray{T,N}, m, mask=nothing) where {T,N}\n",
    "dropout = td.drop\n",
    "a = td.mh(x,x,x)\n",
    "a = dropout(a)\n",
    "res_a = x + a\n",
    "res_a = td.mhn(res_a)\n",
    "\n",
    "ia = td.imh(res_a, m, m, mask=mask)\n",
    "ia = dropout(ia)\n",
    "res_ia = res_a + ia\n",
    "res_ia = td.imhn(res_ia)\n",
    "\n",
    "pwffn = td.pw(res_ia)\n",
    "pwffn = dropout(pwffn)\n",
    "res_pwffn = res_ia + pwffn\n",
    "res_pwffn = td.pwn(res_pwffn)\n",
    "res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, td::TransformerDecoder1)\n",
    "hs = div(size(td.imh.iqproj.weight)[1], td.imh.head)\n",
    "h, ps = size(td.pw.dout.weight)\n",
    "\n",
    "print(io, \"TransformerDecoder(\")\n",
    "print(io, \"head=$(td.mh.head), \")\n",
    "print(io, \"head_size=$(hs), \")\n",
    "print(io, \"pwffn_size=$(ps), \")\n",
    "print(io, \"size=$(h)\")\n",
    "if Flux.istraining()\n",
    "print(io, \", dropout=$(td.drop.p))\")\n",
    "else\n",
    "print(io, \")\")\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer\n",
    "\n",
    "#layer auf jeden Fall ändern\n",
    "encode_t1 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t2 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t3 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t4 = Transformer1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer decoder\n",
    "\n",
    "#layer auf jeden Fall ändern\n",
    "decode_t1 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t2 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t3 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t4 = TransformerDecoder1(512, 8, 64, 2048) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positionwise(Dense(512, 13), logsoftmax)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define the layer to get the final output probabilities\n",
    "\n",
    "#ÄNDERN\n",
    "linear = Positionwise(Dense(512, length(vocab)), logsoftmax) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neue Layer einfügen\n",
    "function encoder_forward(x)\n",
    "    e = embedding(x)\n",
    "    t1 = encode_t1(e)\n",
    "    t2 = encode_t2(t1)\n",
    "    t3 = encode_t2(t2)\n",
    "    t4 = encode_t2(t3)\n",
    "    return t2\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neue Layer einfügen\n",
    "function decoder_forward(x, m)\n",
    "    e = embedding(x)\n",
    "    t1 = decode_t1(e, m)\n",
    "    t2 = decode_t2(t1, m)\n",
    "    t3 = decode_t3(t2, m)\n",
    "    t4 = decode_t4(t3, m)\n",
    "    p = linear(t4)\n",
    "    return p\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512×12 Matrix{Float32}:\n",
       " -1.13408     -1.84392   -2.32267     …  -1.64319    -1.0968     -0.520561\n",
       " -1.08517     -1.09036   -1.56736        -1.85067    -2.38069    -2.34225\n",
       "  0.831087     0.302583   0.0664692      -0.060868    0.324584    0.749434\n",
       "  0.848626     0.897475   0.506241        0.0141686  -0.394741   -0.433558\n",
       "  0.00865521  -0.487076  -0.876659       -1.46744    -1.19967    -0.635452\n",
       " -0.794482    -0.379881  -0.706728    …  -0.990828   -1.63664    -2.15277\n",
       " -1.38286     -1.79868   -2.20306        -1.89379    -1.91283    -1.45226\n",
       "  0.334699     0.482382   0.3129          0.588992   -0.0460867  -0.523754\n",
       "  0.0969949   -0.413367  -0.763434       -1.34569    -1.56875    -1.212\n",
       "  0.623114     0.414014  -0.15226        -0.0594968  -0.455226   -1.12402\n",
       "  ⋮                                   ⋱               ⋮          \n",
       " -0.588119    -0.585887  -0.739805       -0.835556   -0.710021   -0.569935\n",
       " -2.18158     -2.07123   -1.91992        -1.90608    -1.81531    -1.84507\n",
       " -1.09231     -1.01188   -0.96697     …  -0.452416   -0.333199   -0.413925\n",
       "  0.409372     0.173914  -0.00738613      0.192476    0.221628    0.374135\n",
       "  0.327394     0.430698   0.373068        0.633986    0.579961    0.599894\n",
       "  1.57985      1.5656     1.61315         1.43788     1.32576     1.14642\n",
       " -0.314143    -0.37087   -0.529618        0.218023    0.194591    0.249183\n",
       "  1.91684      1.9169     1.84901     …   1.84655     1.79024     1.81745\n",
       "  0.858809     0.832051   0.906097        0.724039    0.689871    0.55632"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bleibt so\n",
    "enc = encoder_forward(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×12 Matrix{Float32}:\n",
       " -3.33083  -3.37172  -3.37812  -3.40815  …  -3.67683  -3.58109  -3.50331\n",
       " -4.31708  -4.26506  -4.19566  -4.16887     -4.01353  -3.97492  -3.96483\n",
       " -3.38153  -3.38715  -3.43217  -3.45597     -3.43957  -3.42064  -3.41785\n",
       " -2.56638  -2.58689  -2.60332  -2.61612     -2.48629  -2.49056  -2.48591\n",
       " -3.96521  -3.97472  -3.97081  -4.01278     -4.14631  -4.16042  -4.16306\n",
       " -1.99491  -1.91175  -1.83148  -1.77007  …  -1.71046  -1.71809  -1.76508\n",
       " -1.72637  -1.72586  -1.7042   -1.67745     -1.54591  -1.52572  -1.52684\n",
       " -2.16321  -2.16004  -2.13376  -2.16662     -2.47786  -2.42628  -2.39656\n",
       " -4.40395  -4.45754  -4.53692  -4.62177     -4.37682  -4.41079  -4.45209\n",
       " -1.35455  -1.37756  -1.43772  -1.46047     -1.54932  -1.58851  -1.56803\n",
       " -2.76795  -2.81047  -2.82594  -2.84114  …  -2.76693  -2.8033   -2.8192\n",
       " -3.80526  -3.79245  -3.76785  -3.78733     -3.6266   -3.60324  -3.56655\n",
       " -3.30018  -3.32112  -3.36363  -3.38169     -3.40323  -3.41862  -3.44518"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bleibt so\n",
    "probs = decoder_forward(encoded_sample, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smooth (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hier vielleicht etwas ändern --> herausfinden, was diese funktion macht\n",
    "function smooth(et)\n",
    "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
    "    p = sm .* (1 .+ -et)\n",
    "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
    "    label\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define loss function\n",
    "\n",
    "#ÄNDERN\n",
    "function loss(x, y)\n",
    "    label = onehot(vocab, y) #turn the index to one-hot encoding\n",
    "    label = smooth(label) #perform label smoothing\n",
    "    enc = encoder_forward(x)\n",
    "    probs = decoder_forward(y, enc)\n",
    "    l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])  #erstmal so lassen\n",
    "    return l\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[0.44517747 -1.2352791 … -0.5580842 0.61990374; -0.86038107 0.9852316 … 0.8646372 -1.6544943; … ; -0.4268172 -0.24769828 … -1.0369782 -0.66909796; -0.59597033 0.7646327 … 1.0684395 -0.24535699], Float32[0.5403023 -0.41614684 … 0.4000682 0.9873536; 0.8218562 0.9364147 … 0.37902638 0.97646356; … ; 1.0 1.0 … 0.9943822 0.99437124; 0.0001 0.0002 … 0.10212166 0.10222114], Float32[-0.041287433 -0.048739977 … 0.040234454 0.030739756; -0.046966214 0.054721765 … -0.017682707 -0.028461229; … ; 0.017833525 -0.022631519 … -0.024375683 -0.03949702; 0.07064739 0.021828623 … 0.076225206 -0.032440905], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.04466606 0.023011195 … -0.010626206 0.029662123; -0.020044418 0.058562506 … -0.0024756473 -0.038418476; … ; -0.030274872 -0.008325687 … -0.038550917 0.061701123; 0.032254297 0.06236364 … 0.031859256 -0.060116887], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0044737617 0.019246597 … 0.03217748 0.029396802; 0.048310023 -0.007064385 … -0.006776562 0.05967547; … ; 0.004640586 -0.023608375 … 0.04625827 0.05263711; -0.0417425 -0.00978285 … -0.041127615 0.061147727], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.05853436 -0.022454439 … 0.0139403185 0.03852637; 0.07649325 -0.035758156 … 0.054511525 -0.07485472; … ; -0.06564676 -0.061945457 … -0.03720393 -0.071540765; 0.04785775 -0.06905846 … 0.0566692 0.033603437], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.014679139 0.009689651 … -0.038403623 -0.039540652; 0.045792412 -0.024234219 … -0.035651673 0.028175171; … ; 0.009961613 0.037956897 … -0.028994322 0.017446842; -0.021702833 0.01140649 … 0.022954281 0.011500872], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.004882685 -0.03971792 … 0.01504397 0.040472224; -0.0048727356 -0.033871673 … -0.019472312 0.022109795; … ; -0.025109027 -0.04322326 … -0.010875552 -0.040438198; 0.03214378 0.024826724 … -0.019132182 0.007935093], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.07627047 0.006438734 … 0.026141949 0.002633748; 0.052883614 -0.04111112 … -0.061503988 0.053349357; … ; 0.024266383 0.059783347 … -0.041753523 -0.039162006; 0.030871358 0.07413314 … 0.047567554 -0.06766475], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.029938577 0.013489231 … 0.06384608 0.04057254; -0.048033517 -0.0033785538 … 0.021338845 -0.01981193; … ; -0.054345503 -0.0376421 … -0.027261976 0.034259073; 0.06657803 0.06311965 … -0.03733597 0.06924833], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.059307255 -0.00844716 … -0.058536097 0.037313357; -0.009162675 -0.0032457293 … -0.045518395 -0.049111534; … ; -0.009966701 -0.040879104 … 0.056166008 0.018633155; 0.027272599 0.027984718 … -0.014515398 -0.022223957], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.02380146 0.02935293 … 0.0741121 0.049437497; -0.036621206 0.02808263 … 0.05888369 -0.058965486; … ; 0.0017737475 0.050412547 … 0.07083728 0.004965876; -0.052474756 0.046407357 … 0.019421104 0.03048117], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.012386543 0.0050432165 … -0.033784483 -0.034836352; 0.041152198 -0.026400771 … -0.002280326 -0.04303578; … ; -0.0057197046 -0.010133122 … -0.01915702 0.020130806; 0.00073009083 0.02122529 … -0.014305696 0.015001852], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.023421703 -0.04238682 … 0.00089957926 0.043635175; 0.014157792 -0.022017121 … -0.045562927 0.027622532; … ; 0.040636186 -0.023845885 … -0.044187292 -0.009124687; 0.03704964 0.025643509 … -0.0020366893 0.005260975], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.067087024 0.07116619 … 0.012979067 -0.06114557; -0.056969725 -0.011169603 … -0.04045335 0.029171797; … ; 0.07573087 0.014155944 … 0.05748867 -0.04750098; -0.064166985 -0.015971959 … -0.058597364 0.04979084], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.031553328 0.01027354 … 0.029247224 -0.06532562; 0.019110743 -0.050882835 … -0.052454993 0.0047012493; … ; 0.03552563 0.06496983 … -0.05254971 0.020077286; -0.05289659 0.06972849 … -0.06530026 0.04170224], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.0369456 0.06367966 … -0.025494324 0.027054965; -0.07481144 -0.004513383 … 0.021619933 -0.058309942; … ; 0.06007743 -0.03004335 … 0.0724789 -0.0028371273; -0.0763181 -0.041253358 … 0.008857751 0.024759483], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.06953146 0.060121942 … 0.048879173 -0.013166823; -0.04260062 -0.06754963 … -0.021445572 0.012187377; … ; 0.028418468 0.016415693 … -0.07059592 -0.009816193; -0.03486392 0.0029042512 … 0.071435206 0.056054335], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.036262218 0.00025130092 … -0.042636503 0.016691659; 0.031772334 -0.020702835 … 0.0392021 -0.0141718155; … ; -0.019520847 -0.029046044 … -0.0058136364 -0.008690855; -0.021617904 -0.032329656 … 0.03926989 -0.046377406], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.012376674 -0.024366897 … -0.036731858 -0.027124628; 0.0075691417 -0.02758622 … 0.019008666 -0.023786915; … ; 0.02916552 -0.033697132 … 0.043282624 -0.039746094; 0.03530255 -0.03169312 … -0.010437425 -0.03892955], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.020009834 -0.048534904 … -0.03774817 -0.0036252954; 0.035405893 0.055596933 … 0.0077320472 -0.07543472; … ; -0.01665725 0.022331396 … -0.008087632 0.056146096; 0.040837474 0.051838465 … -0.02988669 -0.06771308], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.065925516 -0.047337785 … -0.004344606 0.06547501; 0.019919332 -0.044855386 … -0.023440292 -0.06490422; … ; -0.012369038 0.0683568 … 0.012535114 0.009710707; 0.06844934 -0.041153513 … 0.009190195 0.00850359], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.012218676 -0.034489043 … -0.0633797 -0.05364282; 0.054061495 0.07419574 … 0.011070962 0.053815026; … ; -0.0030782297 0.07271936 … -0.01492787 -0.02541198; 0.03358601 -0.04936906 … -0.0037214917 -0.064093694], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.058685 -0.025510658 … 0.018351136 0.06894263; 0.00420032 0.048935328 … -0.054811373 0.037365682; … ; -0.046268348 0.07339927 … -0.06555891 0.038464047; 0.004720923 -0.046297073 … -0.03996698 0.060148183], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.004556624 -0.04690175 … 0.018973796 0.04362293; -0.028595176 0.012199163 … -0.02876927 -0.022652367; … ; -0.02748437 -0.010813765 … 0.03780024 -0.043608073; 0.0030016794 0.009432372 … 0.0062556867 -0.03799157], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.021936001 0.026768984 … 0.021654159 0.0045695747; -0.021208104 -0.044417415 … 0.04114525 0.013516636; … ; 0.025355792 0.024023812 … -0.003947601 -0.03360223; 0.016231036 0.0255493 … -0.033563215 0.027482327], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.03687081 -0.05572304 … 0.0392678 -0.022626976; 0.06290893 -0.024853745 … 0.058935955 0.025589352; … ; 0.029751895 0.013297422 … 0.0041915965 -0.06579826; 0.041039977 0.035908867 … -0.030904902 0.021414911], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.014847332 -0.0027431028 … -0.03838893 0.060130756; -0.02147811 -0.06269407 … 0.024187397 0.07225019; … ; 0.0036196378 -0.057150036 … 0.009980499 0.018582657; -0.057203237 0.026887247 … 0.05782604 -0.01954683], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.021637581 -0.07401512 … -0.06433761 -0.050616585; 0.028178114 -0.04632295 … 0.024159037 0.04027276; … ; 0.030850152 0.04654989 … -0.009211146 -0.0043891543; -0.046763163 0.018600214 … -0.036304984 0.018442368], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.063333996 0.023602042 … 0.070453756 -0.013903909; -0.009985627 0.0677719 … -0.042872034 -0.020509943; … ; 0.058672298 0.024860935 … -0.054335576 0.044464175; 0.063202634 0.024987664 … 0.0074944673 -0.008487858], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.053984698 0.026850527 … -0.043442518 0.015011382; -0.067411676 -0.020464463 … 0.06636996 0.008265681; … ; 0.06979527 0.06377424 … 0.022013625 0.07406987; 0.05027998 0.022461502 … 0.04185461 -0.015396386], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.050707962 -0.05671543 … 0.0049205245 -0.059175655; -0.06916398 -0.047502857 … 0.031107679 -0.065091215; … ; 0.031681553 -0.020789973 … 0.069907166 -0.021050567; -0.05556954 -0.0044903876 … 0.047306284 -0.0038509215], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0249871 0.05622461 … -0.04879365 -0.06614481; -0.07338726 -0.04256591 … -0.0391556 0.050803393; … ; 0.014520928 -0.0034388157 … 0.04733317 -0.07097069; 0.027346164 0.05800051 … -0.049741674 0.051414605], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0034062392 -0.03991349 … -0.024484929 0.0019139085; 0.03545192 -0.0060750092 … 0.02430146 -0.054582737; … ; -0.06634576 -0.033675287 … 0.069881305 0.07052559; 0.01693062 0.036745086 … 0.03711312 0.04100923], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.040174603 -0.037953537 … 0.0018369136 -0.044310786; -0.020475369 -0.04105687 … -0.027808411 -0.034304455; … ; 0.01462871 0.00077287847 … -0.034395147 -0.0024675669; -0.017800165 0.011797384 … -0.031985417 -0.008434729], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.040679872 -0.019384867 … 0.048171263 -0.012175271; 0.018304292 0.01700096 … -0.00029294586 -0.0347547; … ; -0.005684858 0.036193702 … -0.025521483 -0.0314158; -0.035112157 0.011500503 … 0.047866732 0.008302592], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.012843723 0.05222391 … -0.022584835 -0.028153477; 0.057402126 0.03665172 … 0.04783897 0.03252217; … ; 0.04926266 -0.07121723 … 0.007057906 -0.07151494; 0.024926435 0.03539589 … -0.027621724 -0.0013846185], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.03668183 0.000746284 … -0.03578845 0.040491927; -0.054107357 -0.041728612 … 0.06557139 0.06951152; … ; 0.06343094 0.040859867 … 0.032448296 0.074415475; 0.05287852 0.010405453 … 0.02848835 -0.030370483], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.07633394 -0.03266109 … -0.07395814 -0.055167854; 0.04449761 0.014387136 … 0.024506573 -0.010074778; … ; 0.06459629 -0.02018104 … -0.011605635 0.011650239; -0.010532254 0.054051146 … -0.015039888 -0.018194076], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.021019084 -0.031323284 … 0.012069499 0.047876693; -0.028100735 0.06408001 … 0.06331998 0.027180454; … ; 0.035054762 0.011387145 … -0.072984464 0.021536002; -0.033739857 -0.008606027 … 0.03501118 -0.065215096], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.06534044 0.0040174904 … 0.025611345 -0.0647127; 0.0011543202 -0.028496068 … -0.018975053 -0.041724652; … ; -0.03640472 0.055184077 … 0.020922378 -0.039934278; 0.0016156468 -0.059436902 … 0.025996951 0.016912734], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.04059913 -0.051764935 … 0.03398085 -0.04806874; -0.06238021 0.061448414 … 0.051134996 0.07410893; … ; -0.07061508 0.021409655 … -0.028275771 -0.03601167; -0.04271658 0.027238362 … 0.0017307503 -0.016756222], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.051955286 -0.026673209 … 0.04118735 -0.06329467; 0.061402608 0.010641354 … -0.03992685 0.03970385; … ; -0.068231456 -0.016212678 … 0.032139577 -0.030923024; 0.010106315 0.07444814 … -0.053377643 0.057634596], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.04711391 -0.02820255 … 0.0640117 -0.023213115; 0.062145095 0.043942824 … -0.015854865 0.0068412405; … ; 0.03339911 0.023172762 … 0.033870272 -0.016590547; 0.021718083 -0.031634487 … -0.04161721 -0.00753261], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.04077143 0.028810244 … -0.026638279 -0.017836766; -0.03017909 -0.010481413 … -0.011489664 -0.047851264; … ; 0.02128217 -0.041789245 … 0.021130353 -0.03987964; 0.035258874 0.048358966 … -0.045305427 -0.00039366476], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.032557633 -0.0006880765 … -0.046896614 0.027534464; -0.036367834 0.014165953 … -0.04667312 0.039627265; … ; 0.028560294 0.011998188 … -0.0081759365 0.009040958; -0.033194818 -0.0055671833 … 0.036107205 0.0033905886], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.01903285 0.053724706 … -0.029669167 -0.0017252205; -0.009430495 -0.06606692 … 0.035217587 -0.032686517; … ; -0.064169414 -0.02522079 … 0.04908664 0.020519834; 0.073210195 -0.053886715 … 0.0120684765 0.019686114], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.015764035 -0.068792224 … 0.06852798 0.043617606; -0.06510172 -0.026475761 … -0.036869332 0.061253082; … ; 0.07412495 0.017556123 … 0.03629055 0.05355478; -0.044053365 -0.04244876 … 0.025524765 -0.07345714], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.049804945 0.07507645 … 0.02255111 0.06309547; 0.05045788 -0.058921482 … 0.051693596 0.07083805; … ; -0.04372671 0.022374613 … 0.0030054483 0.026724821; 0.013488428 -0.06751074 … 0.052305505 0.051404003], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.05145215 0.035490483 … -0.0028101536 0.06022217; 0.040588655 -0.0030666227 … 0.015400894 -0.041957248; … ; -0.06948873 0.0029564283 … 0.012645053 -0.03019395; -0.039147533 -0.041293584 … -0.02616071 -0.014765097], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.066049054 -0.015796246 … 0.040588763 -0.0065462636; -0.06059252 0.07572554 … 0.027676474 0.02000217; … ; 0.047229983 -0.07573751 … -0.037996918 0.04487632; 0.06252888 -0.04005086 … -0.015366018 -0.019740919], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.04930097 0.044202816 … -0.040441155 0.052156676; 0.059536897 -0.002850742 … 0.026384072 -0.03232717; … ; 0.005178052 -0.031133905 … -0.049821973 -0.032943696; 0.03873683 0.005441657 … 0.035196617 0.043757275], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.055354115 -0.009862548 … 0.033926025 -0.061053902; -0.03126607 0.011337121 … 0.065634295 0.022150993; … ; 0.074987285 -0.06835388 … 0.009831286 0.07467525; -0.0147183575 -0.038452752 … -0.076301605 0.017974691], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.03423624 0.0148188975 … -0.018989945 0.039086778; -0.01465054 0.038438827 … -0.062992886 -0.047959495; … ; 0.044900846 0.028168187 … -0.042755708 -0.024947716; 0.043828506 -0.054397296 … -0.055212986 0.008109806], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.023219042 0.03472189 … 0.030415803 0.031618323; 8.039275f-5 0.00020062983 … 0.038933832 0.0014267779; … ; 0.013041839 -0.034155328 … -0.023850549 0.038718957; -0.009226052 -0.034947112 … 0.03551104 0.027940918], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.03468525 0.021997696 … -0.030869072 0.033478577; -0.021044513 0.02523941 … -0.025766252 -0.0095535675; … ; 0.027855469 -0.0038689398 … 0.015680637 0.026943032; -0.025305524 -0.04545519 … 0.0037709796 -0.045441825], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.024451897 -0.03899348 … -0.000107420215 -0.069267206; -0.039116617 -0.018685844 … -0.058381245 0.040485468; … ; -0.021654554 -0.07143539 … 0.05725136 0.051113937; 0.029941842 0.012591726 … -0.010550432 0.049510736], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.039280996 -0.028229179 … -0.02927011 0.06624546; 0.038353305 -0.011640329 … 0.058954917 0.030915359; … ; -0.028227864 0.018375773 … 0.038254008 0.016531197; -0.010412771 0.013602709 … 0.041613434 -0.021725474], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.012665585 -0.051471 … -0.03075817 0.012749115; 0.053838678 0.016898079 … 0.04578129 0.027782341; … ; 0.04557017 0.03942373 … 0.0059072906 0.01538248; -0.0078280615 0.059036188 … -0.04145455 0.018166024], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.044127624 0.02479009 … -0.07438399 0.055023532; 0.045768693 -0.011833142 … -0.0015695834 -0.033377208; … ; -0.02113908 0.035181124 … 0.0652502 -0.036805786; -0.05851666 0.031806532 … 0.01606102 -0.013981929], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.0112821525 -0.0209747 … -0.009278179 -0.0047598504; -0.03811474 -0.048591588 … 0.068033755 0.037315037; … ; -0.0015525196 0.026415134 … -0.0009651577 -0.062277902; 0.012426034 -0.06992813 … -0.06458382 0.009521819], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.023605546 -0.07060629 … 0.04957576 -0.027139172; -0.06724555 -0.00023945984 … 0.07394137 0.03635189; … ; 0.011575139 0.009176526 … -0.031983245 -0.043952864; -0.050498288 -0.06197876 … -0.022480791 -0.07404454], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.036820844 0.05914423 … 0.04860469 -0.033751026; -0.058314923 0.020804117 … 0.010097555 -0.010314439; … ; 0.05683369 -0.013689325 … -0.030308196 -0.050509293; -0.015624586 0.008118786 … -0.06415764 0.0050896853], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.02140951 0.04229721 … -0.037624724 -0.038768716; 0.04557873 -0.07638276 … 0.049857907 0.0066613127; … ; 0.06402146 -0.0068511683 … 0.06496662 -0.015450534; 0.036150936 0.00087613356 … 0.03200551 -0.07627071], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.026830228 0.006904173 … -0.03057151 0.024011508; -0.029966561 -0.040809527 … 0.04568575 -0.010860247; … ; -0.0015553832 0.022982458 … -0.0057955724 -0.035797417; -0.040310193 0.017015526 … 0.046468925 -0.002789657], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.008631538 -0.046721112 … 0.038547035 -0.019374168; 0.019304786 0.0017021563 … 0.033886578 0.04464779; … ; -0.004780477 -0.026723104 … -0.007750115 0.018790504; -0.012744807 -0.044963658 … -0.016051551 -0.030690107], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.005240999 -0.015799513 … -0.100541845 -0.10302395; 0.061768353 0.024352936 … -0.07238544 0.058388513; … ; -0.03856406 -0.07736597 … -0.013648222 -0.039427593; -0.0830681 0.009024364 … -0.04705435 -0.087431885], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collect all the parameters\n",
    "\n",
    "#Layer neu einfügen\n",
    "ps = params(embed, pe, encode_t1, encode_t2, encode_t3, encode_t4, decode_t1, decode_t2, decode_t3, decode_t4, linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = ADAM(1e-4) #bleibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define training loop\n",
    "\n",
    "#ÄNDERN siehe Paper Kapitel 4.1\n",
    "function train!()\n",
    "    @info \"start training\"\n",
    "    for i = 1:1000\n",
    "      data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
    "      x, y = preprocess.(data[1]), preprocess.(data[2])\n",
    "      x, y = vocab(x), vocab(y) #encode the data\n",
    "      x, y = todevice(x, y) #move to gpu\n",
    "      grad = gradient(()->loss(x, y), ps)\n",
    "      if i % 8 == 0\n",
    "          l = loss(x, y)\n",
    "          println(\"loss = $l\")\n",
    "      end\n",
    "      update!(opt, ps, grad)\n",
    "    end\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: start training\n",
      "└ @ Main /Users/johannes/Documents/GitHub/DM2022-LinearTransformers/test/Duplicat_Exercise11.ipynb:5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 105.71132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 82.529884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 81.73267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 79.40291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 78.55067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.61925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.81436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.17047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.32717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.91512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.149956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.23068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 75.62789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 69.1194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 71.352615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.05735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.713974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.814156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.71426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.227066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.50553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.48499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.48952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.634544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.96221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.815346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.157715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.55225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.27672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.392624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.72232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.227806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.28221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.39281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.828636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.582214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.18309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.23629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.357414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.42819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.06248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.591415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.27017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.88831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.0318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.85909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.530754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.29395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.139114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.101204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.49786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.77928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.316345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 68.04356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.05968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.97719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.93164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.25173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.31709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.81495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.635796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.04542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.91476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.54921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.775246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 65.56332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.374825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 65.73842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 64.94812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 66.31008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 64.354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.01174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 63.872875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 60.35215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 61.649662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 62.953842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 60.4351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.679398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.631424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 60.3711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.037006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 60.79925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.52569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 81.54691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.56976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 71.552635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 65.65068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.243687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.1189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.422203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 56.210464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.47223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 56.307182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 55.30398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 55.284782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 54.443913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 50.08449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 50.241737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 45.332897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 39.41431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 35.79455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 28.793844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 19.113693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 31.802711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 19.278803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 7.489004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 4.3143783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 8.495479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 17.53115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.9089817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.19637007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.18695503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.066513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.034344178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 30.42926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5388842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.6861704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.460068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.07975506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.03670506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.010820839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.017511562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0071364627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0067240065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0069205435\n"
     ]
    }
   ],
   "source": [
    "train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#erst einmal so lassen, vielleicht einzelne Änderungen\n",
    "function translate(x)\n",
    "    ix = todevice(vocab(preprocess(x)))\n",
    "    seq = [startsym]\n",
    "\n",
    "    enc = encoder_forward(ix)\n",
    "\n",
    "    len = length(ix)\n",
    "    for i = 1:2len\n",
    "        trg = todevice(vocab(seq))\n",
    "        dec = decoder_forward(trg, enc)\n",
    "        #move back to gpu due to argmax wrong result on CuArrays\n",
    "        ntok = onecold(collect(dec), labels)\n",
    "        push!(seq, ntok[end])\n",
    "        ntok[end] == endsym && break\n",
    "    end\n",
    "  seq[2:end-1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"6\"\n",
       " \"6\"\n",
       " \"1\"\n",
       " \"1\"\n",
       " \"3\"\n",
       " \"4\"\n",
       " \"6\"\n",
       " \"6\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(map(string, [5,5,6,6,1,12,3,4,6]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
