{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "\tusing Flux\n",
    "\tusing Flux: onehot\n",
    "\tusing Flux: gradient\n",
    "\tusing Flux.Optimise: update!\n",
    "\tusing Flux: onecold\n",
    "\tusing CUDA\n",
    "\tusing Transformers\n",
    "\tusing Transformers.Basic \n",
    "    using Transformers.Datasets: batched\n",
    "\tusing Flux: @functor\n",
    "\tusing ..Transformers: Abstract3DTensor, Container, epsilon, batchedmul, batched_triu!\n",
    "\tusing NNlib\n",
    "\tusing Tullio\n",
    "\tusing Plots\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "Number of layers: 8\n",
    "Number of heads: 8\n",
    "Feed forward dimensions: 1024\n",
    "d_model = 64\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary{String}(13, unk=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Labeling: vielleicht ändern\n",
    "\n",
    "begin\n",
    "\tlabels = map(string, 1:10)\n",
    "\tstartsym = \"11\"\n",
    "\tendsym = \"12\"\n",
    "\tunksym = \"0\"\n",
    "\tlabels = [unksym, startsym, endsym, labels...]\n",
    "\tvocab = Vocabulary(labels, unksym)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for generate training datas \n",
    "\n",
    "#nichts ändern\n",
    "sample_data() = (d = map(string, rand(1:10, 10)); (d,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"2\", \"3\", \"5\", \"9\", \"6\", \"3\", \"4\", \"9\", \"3\", \"1\"], [\"2\", \"3\", \"5\", \"9\", \"6\", \"3\", \"4\", \"9\", \"3\", \"1\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nichts ändern, generieren 10 string token, nutze 1-10\n",
    "\n",
    "sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for adding start & end symbol\n",
    "\n",
    "preprocess(x) = [startsym, x..., endsym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_ex = preprocess.(sample_data()) = ([\"11\", \"7\", \"7\", \"4\", \"6\", \"8\", \"1\", \"4\", \"8\", \"3\", \"5\", \"12\"], [\"11\", \"7\", \"7\", \"4\", \"6\", \"8\", \"1\", \"4\", \"8\", \"3\", \"5\", \"12\"])\n",
      "encoded_sample_ex = vocab(sample_ex[1]) = [2, 10, 10, 7, 9, 11, 4, 7, 11, 6, 8, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       " 10\n",
       " 10\n",
       "  7\n",
       "  9\n",
       " 11\n",
       "  4\n",
       "  7\n",
       " 11\n",
       "  6\n",
       "  8\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "begin\n",
    "    @show sample_ex = preprocess.(sample_data())\n",
    "    @show encoded_sample_ex = vocab(sample_ex[1]) #use Vocabulary to encode the training data, +3\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"11\", \"4\", \"3\", \"5\", \"9\", \"3\", \"8\", \"6\", \"9\", \"8\", \"1\", \"12\"], [\"11\", \"4\", \"3\", \"5\", \"9\", \"3\", \"8\", \"6\", \"9\", \"8\", \"1\", \"12\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = preprocess.(sample_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{Int64}:\n",
       "  2\n",
       "  7\n",
       "  6\n",
       "  8\n",
       " 12\n",
       "  6\n",
       " 11\n",
       "  9\n",
       " 12\n",
       " 11\n",
       "  4\n",
       "  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sample = vocab(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embed(128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a Word embedding layer which turn word index to word vector\n",
    "\n",
    "embed = Embed(128, length(vocab)) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionEmbedding(128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a position embedding layer mentioned above\n",
    "\n",
    "pe = PositionEmbedding(128) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wrapper for get embedding\n",
    "\n",
    "function embedding(x)\n",
    "    we = embed(x, inv(sqrt(128)))\n",
    "    e = we .+ pe(we)\n",
    "    return e\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractAttention end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_atten_mask1 (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_atten_mask1(T::Type, score::AbstractArray, ::Nothing, future) = create_atten_mask1(T, score, fill!(similar(score, size(score,1), size(score, 2), 1), one(T)), future)\n",
    "function create_atten_mask1(T::Type, score::AbstractArray, _mask::AbstractArray, future::Bool=false)\n",
    "  #size(mask) == (q, k, n, b)\n",
    "\n",
    "  # ql, kl = size(mask)\n",
    "  mask = copy(_mask)\n",
    "\n",
    "  maskval = convert(T, -1e9)\n",
    "  !future && batched_triu!(mask, 0)\n",
    "  mask .= (1 .- mask) .* maskval\n",
    "  return mask\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd create_atten_mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MultiheadAttention_1{Q<:Dense, K<:Dense, V<:Dense, O<:Dense, DP<:Dropout} <: AbstractAttention\n",
    "    head::Int\n",
    "    future::Bool\n",
    "    iqproj::Q\n",
    "    ikproj::K\n",
    "    ivproj::V\n",
    "    oproj::O\n",
    "    drop::DP\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.functor(mh::MultiheadAttention_1) = (mh.iqproj, mh.ikproj, mh.ivproj, mh.oproj), m -> MultiheadAttention_1(mh.head, mh.future, m..., mh.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiheadAttention_1(head::Int,\n",
    "                   is::Int,\n",
    "                   hs::Int,\n",
    "                   os::Int;\n",
    "                   future::Bool=true, pdrop = 0.1) = MultiheadAttention_1(head,\n",
    "                                                                        future,\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(is, hs*head),\n",
    "                                                                        Dense(hs*head, os),\n",
    "                                                                        Dropout(pdrop),\n",
    "                                                                        )\n",
    "\n",
    "\n",
    "function Base.show(io::IO, mh::MultiheadAttention_1)\n",
    "    hs = div(size(mh.iqproj.weight)[1], mh.head)\n",
    "    is = size(mh.iqproj.weight)[end]\n",
    "    os = size(mh.oproj.weight)[1]\n",
    "\n",
    "    print(io, \"MultiheadAttention(\")\n",
    "    print(io, \"head=$(mh.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"$(is)=>$(os)\")\n",
    "\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(mh.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (mh::MultiheadAttention_1)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: Abstract3DTensor{T},\n",
    "                         A2 <: Abstract3DTensor{T},\n",
    "                         A3 <: Abstract3DTensor{T}}\n",
    "qs = size(query)\n",
    "ks = size(key)\n",
    "vs = size(value)\n",
    "\n",
    "#size(ipq) == (h, q_seq_len, batch)\n",
    "ipq = @toNd mh.iqproj(query)\n",
    "ipk = @toNd mh.ikproj(key)\n",
    "ipv = @toNd mh.ivproj(value)\n",
    "\n",
    "h = size(ipq, 1)\n",
    "hs = div(h, mh.head)\n",
    "\n",
    "#size(ipq) == (hs, q_seq_len, head, batch)\n",
    "ipq = permutedims(reshape(ipq, hs, mh.head, qs[2], qs[3]), [1, 3, 2, 4])\n",
    "ipk = permutedims(reshape(ipk, hs, mh.head, ks[2], ks[3]), [1, 3, 2, 4])\n",
    "ipv = permutedims(reshape(ipv, hs, mh.head, vs[2], vs[3]), [1, 3, 2, 4])\n",
    "\n",
    "#size(ipq) == (hs, q_seq_len, head * batch)\n",
    "ipq = reshape(ipq, hs, qs[2], :)\n",
    "ipk = reshape(ipk, hs, ks[2], :)\n",
    "ipv = reshape(ipv, hs, vs[2], :)\n",
    "\n",
    "atten = attention1(ipq,ipk,ipv,\n",
    "mask,\n",
    "mh.future,\n",
    "mh.drop,mh.head)\n",
    "\n",
    "atten = permutedims(reshape(atten, hs, qs[2], mh.head, qs[3]), [1, 3, 2, 4]) #size(atten) == (hs, head, ql, b)\n",
    "atten = reshape(atten, h, qs[2], qs[3]) #size(atten) == (h, ql, b)\n",
    "\n",
    "out = @toNd mh.oproj(atten)\n",
    "out #size(out) == (h, q_seq_len, batch)\n",
    "end\n",
    "\n",
    "function (mh::MultiheadAttention_1)(query::A1,\n",
    "    key::A2,\n",
    "    value::A3;\n",
    "    mask=nothing) where {T,\n",
    "                         A1 <: AbstractMatrix{T},\n",
    "                         A2 <: AbstractMatrix{T},\n",
    "                         A3 <: AbstractMatrix{T}}\n",
    "\n",
    "# size(query) == (dims, seq_len)\n",
    "ipq = mh.iqproj(query)\n",
    "ipk = mh.ikproj(key)\n",
    "ipv = mh.ivproj(value)\n",
    "\n",
    "h = size(ipq)[1] #h == hs * head\n",
    "hs = div(h, mh.head)\n",
    "\n",
    "#size(hq) == (hs, seq_len, head)\n",
    "hq = permutedims(reshape(ipq, hs, mh.head, :), [1, 3, 2])\n",
    "hk = permutedims(reshape(ipk, hs, mh.head, :), [1, 3, 2])\n",
    "hv = permutedims(reshape(ipv, hs, mh.head, :), [1, 3, 2])\n",
    "\n",
    "atten = attention1(hq, hk, hv,\n",
    "mask,\n",
    "mh.future,\n",
    "mh.drop,mh.head)\n",
    "\n",
    "# size(atten) == (head*hs, seq_len)\n",
    "atten = reshape(permutedims(atten, [1, 3, 2]), h, :)\n",
    "\n",
    "mh.oproj(atten)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_mask1 (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apply_mask1(score, mask)\n",
    "    s = size(score)\n",
    "    ms = size(mask)\n",
    "    bxn = s[end]\n",
    "    b = ms[end]\n",
    "    if bxn == b || b == 1\n",
    "      return score .+ mask\n",
    "    else\n",
    "      return reshape(reshape(score, s[1:end-1]..., :, b) .+\n",
    "                     reshape(mask, ms[1:end-1]..., 1, b), s)\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  apply_mask1(score::AbstractArray{T}, ::Nothing, future) where T = future ? score : apply_mask1(score, create_atten_mask1(T, score, nothing, future))\n",
    "  apply_mask1(score::AbstractArray{T}, mask, future) where T = apply_mask(score, create_atten_mask1(T, score, mask, future))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nelu (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# of type float (to allow for integer inputs)\n",
    "function oftf(x, y)\n",
    "    oftype(float(x), y)\n",
    "end\n",
    "function nelu(x, α=1)\n",
    "    ifelse(x ≥ 0, float(x)+1, @fastmath oftf(x, α) * (exp(x) - 1)+1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×12 Matrix{Float32}:\n",
       "  0.492742    -0.488629    -0.970747   …  -0.00201693    0.958541\n",
       "  0.797864     0.914671     0.360138       0.04893      -0.833579\n",
       "  0.707373    -0.00888045  -0.866527      -1.07054      -0.530788\n",
       "  0.658058     0.832071     0.603502       1.03786       0.438578\n",
       "  0.67227      0.0931084   -0.529596      -0.224848     -0.817858\n",
       "  0.433296     0.98964      0.850181   …   0.811493      1.10168\n",
       "  0.800429     0.202702    -0.467546       0.74379       0.020749\n",
       "  0.619195     0.982063     0.978251      -0.098461      0.466958\n",
       "  0.866877     0.506233    -0.194401       1.04758       0.996295\n",
       "  0.428513     0.866789     0.959077      -0.703733     -0.542097\n",
       "  ⋮                                    ⋱   ⋮            \n",
       "  0.0338934    0.0222793    0.190211       0.18267       0.0372519\n",
       "  0.96112      1.07388      1.09958    …   1.05093       0.884489\n",
       "  0.0601462    0.101847     0.0083902      0.000913331   0.00414561\n",
       "  0.995797     1.01953      1.10602        0.930685      0.953327\n",
       " -0.00306131  -0.048961     0.0635349     -0.0229469     0.0511478\n",
       "  1.09933      0.86242      1.07449        1.03988       0.898667\n",
       " -0.041164    -0.138463     0.099154   …  -0.062335      0.00552095\n",
       "  1.05308      0.987934     1.02133        1.12944       1.15555\n",
       "  0.0331143    0.11171     -0.0456175     -0.000810463  -0.11406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = embedding(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splitHeads (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function splitHeads(x, batch_size, head, depth)\n",
    "    x = reshape(x, (batch_size, :, head, depth))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "padding_mask (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function padding_mask(seq)\n",
    "    # Returns (batch, 1, seq_len, 1) tensor with 1's where the sequence is padded, 0 where it is not\n",
    "    mask = isequal.(seq, 0)\n",
    "    mask = mask[:,1,:,1]\n",
    "    na = [CartesianIndex()]\n",
    "return Float32.(mask[:,na,:,na])  # (batch, 1, seq_len, 1) m l j h  <- j gets masked\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention1 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hier anstatt softmax linear\n",
    "function attention1(query::A1,\n",
    "    key::A2,\n",
    "    value::A3,\n",
    "    mask, future::Bool,\n",
    "    dropout, head) where {T,\n",
    "                    A1 <: Abstract3DTensor{T},\n",
    "                    A2 <: Abstract3DTensor{T},\n",
    "                    A3 <: Abstract3DTensor{T}}\n",
    "#size(query) == (dims, {q,k}_seq_len, batch) == size(key) == size(value)\n",
    "#size(ipq) == (hs, q_seq_len, head * batch)\n",
    "#size(score) == (k_seq_len, q_seq_len, batch)\n",
    "#=\n",
    "dk = size(key, 1)\n",
    "score = batchedmul(key, query; transA = true) \n",
    "score = score ./ convert(T, sqrt(dk))\n",
    "\n",
    "score = apply_mask1(score, mask, future)\n",
    "score = softmax(score; dims=1)\n",
    "dropout !== nothing && (score = dropout(score))\n",
    "batchedmul(value, score) #size(return) == (dims, q_seq_len, batch)\n",
    "=#\n",
    "#size(query) == (batch, {q,k}_seq_len, dims) == size(key) == size(value)\n",
    "\n",
    "query = permutedims(query,(3,2,1))\n",
    "key = permutedims(key,(3,2,1))\n",
    "value = permutedims(value,(3,2,1))\n",
    "\n",
    "qs = size(query)\n",
    "#batch = 8, seq_len = 12, d_model = 64\n",
    "ks = size(key)\n",
    "vs = size(value)\n",
    "\n",
    "d_model = qs[3]\n",
    "\n",
    "#depth = div(d_model, n_heads)\n",
    "depth = div(d_model, head)   \n",
    "\n",
    "# size(key) == (batch_size,  seq_len_k, num_heads, depth_k) (m,j,h,d)\n",
    "query = nelu.(splitHeads(query, qs[1], head, depth))\n",
    "key = nelu.(splitHeads(key, ks[1], head, depth))\n",
    "value = splitHeads(value, vs[1], head, depth)\n",
    "\n",
    "# size(k_v) == (batch_size, depth_k, depth_v, seq_len_v)\n",
    "@tullio k_v[m,d,e,h] := key[m,j,h,d]*value[m,j,h,e]\n",
    "\n",
    "#padmask = padding_mask(key)\n",
    "#key = key .* padmask\n",
    "\n",
    "k_reduced = dropdims(sum(key, dims=2), dims=2)\n",
    "\n",
    "k_reduced = k_reduced .+ 1e-8\n",
    "\n",
    "@tullio z_1[m,l,h] := query[m,l,h,d]*k_reduced[m,h,d]\n",
    "# size(z) == (batch_size, num_heads, seq_len_q)\n",
    "z = 1 ./ z_1 # ...\n",
    "\n",
    "# size(output) == (batch_size,len_q, heads, depth_v)\n",
    "@tullio output[m,l,h,e] := query[m,l,h,d]*k_v[m,d,e,h]*z[m,l,h]\n",
    "\n",
    "#output = reshape(output, (qs[1], qs[2], mh.head*depth))\n",
    "# size(output) == (batch_size,len_q, d_model)\n",
    "output = reshape(output, (qs[1], :, head*depth))\n",
    "\n",
    "output = permutedims(output,(3,2,1))\n",
    "#(dims, q_seq_len, batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractTransformer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PwFFN{Di<:Dense, Do<:Dense}\n",
    "    din::Di\n",
    "    dout::Do\n",
    "end\n",
    "\n",
    "@functor PwFFN\n",
    "\n",
    "\n",
    "\"just a wrapper for two dense layer.\"\n",
    "function PwFFN(size::Int, h::Int, act = nelu)\n",
    "    PwFFN(\n",
    "    Dense(size, h, act),\n",
    "    Dense(h, size)\n",
    ")\n",
    "end\n",
    "function (pw::PwFFN)(x::AbstractMatrix)\n",
    "  #size(x) == (dims, seq_len)\n",
    "  pw.dout(pw.din(x))\n",
    "end\n",
    "\n",
    "function (pw::PwFFN)(x::A) where {T, N, A<:AbstractArray{T, N}}\n",
    "  new_x = reshape(x, size(x, 1), :)\n",
    "  y = pw(new_x)\n",
    "  return reshape(y, Base.setindex(size(x), size(y, 1), 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Transformer1{MA<:MultiheadAttention_1, LA<:LayerNorm, P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "    mh::MA\n",
    "    mhn::LA\n",
    "    pw::P\n",
    "    pwn::LP\n",
    "    drop::DP\n",
    "end\n",
    "\n",
    "@functor Transformer1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Transformer(size::Int, head::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)\n",
    "    Transformer(size::Int, head::Int, hs::Int, ps::Int;\n",
    "                future::Bool = true, act = relu, pdrop = 0.1)  \n",
    "\n",
    "Transformer layer.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "When `future` is `false`, the k-th token can't see the j-th tokens where j > k. `pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "\n",
    "########\n",
    "#ganze Funktion von Transformer ändern\n",
    "########\n",
    "function Transformer1(size::Int, head::Int, ps::Int; future::Bool = true, act = relu, pdrop = 0.1)  #relu vielleicht wieder auf elu ändern\n",
    "    rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "    Transformer1(size, head, div(size, head), ps;future=future, act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "Transformer1(size::Int, head::Int, hs::Int, ps::Int; future::Bool = true, act = relu, pdrop = 0.1) = Transformer1(\n",
    "    MultiheadAttention_1(head, size, hs, size; future=future, pdrop=pdrop),\n",
    "    LayerNorm(size),\n",
    "    PwFFN(size, ps, act),\n",
    "    LayerNorm(size),\n",
    "    Dropout(pdrop),     #braucht man das? vielleicht nicht\n",
    ")\n",
    "\n",
    "function (t::Transformer1)(x::A, mask=nothing) where {T, N, A<:AbstractArray{T, N}}\n",
    "    dropout = t.drop\n",
    "    a = t.mh(x, x, x; mask=mask)\n",
    "    a = dropout(a)\n",
    "    res_a = x + a\n",
    "    res_a = t.mhn(res_a)\n",
    "    pwffn = t.pw(res_a)\n",
    "    pwffn = dropout(pwffn)\n",
    "    res_pwffn = res_a + pwffn\n",
    "    res_pwffn = t.pwn(res_pwffn)\n",
    "    res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, t::Transformer1) \n",
    "    hs = div(size(t.mh.iqproj.weight)[1], t.mh.head)\n",
    "    h, ps = size(t.pw.dout.weight)\n",
    "\n",
    "    print(io, \"Transformer(\")\n",
    "    print(io, \"head=$(t.mh.head), \")\n",
    "    print(io, \"head_size=$(hs), \")\n",
    "    print(io, \"pwffn_size=$(ps), \")\n",
    "    print(io, \"size=$(h)\")\n",
    "    if Flux.istraining()\n",
    "        print(io, \", dropout=$(t.drop.p))\")\n",
    "    else\n",
    "        print(io, \")\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TransformerDecoder1{MA<:MultiheadAttention_1, LA<:LayerNorm,\n",
    "        IMA<:MultiheadAttention_1, ILA<:LayerNorm,\n",
    "        P<:PwFFN, LP<:LayerNorm, DP<:Dropout} <: AbstractTransformer\n",
    "                mh::MA\n",
    "                mhn::LA\n",
    "                imh::IMA\n",
    "                imhn::ILA\n",
    "                pw::P\n",
    "                pwn::LP\n",
    "                drop::DP\n",
    "end\n",
    "\n",
    "@functor TransformerDecoder1\n",
    "\n",
    "\"\"\"\n",
    "TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "TransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "\n",
    "TransformerDecoder layer. Decode the value from a Encoder.\n",
    "\n",
    "`size` is the input size. if `hs` is not specify, use `div(size, head)` as the hidden size of multi-head attention. \n",
    "`ps` is the hidden size & `act` is the activation function of the positionwise feedforward layer. \n",
    "`pdrop` is the dropout rate.\n",
    "\"\"\"\n",
    "function TransformerDecoder1(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\n",
    "rem(size, head) != 0 && error(\"size not divisible by head\")\n",
    "TransformerDecoder1(size, head, div(size, head), ps; act=act, pdrop=pdrop)\n",
    "end\n",
    "\n",
    "TransformerDecoder1(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1) = TransformerDecoder1(\n",
    "MultiheadAttention_1(head, size, hs, size; future=false, pdrop=pdrop),\n",
    "LayerNorm(size),\n",
    "MultiheadAttention_1(head, size, hs, size; future=true, pdrop=pdrop), \n",
    "LayerNorm(size),\n",
    "PwFFN(size, ps, act),\n",
    "LayerNorm(size),\n",
    "Dropout(pdrop),\n",
    ")\n",
    "\n",
    "function (td::TransformerDecoder1)(x::AbstractArray{T,N}, m, mask=nothing) where {T,N}\n",
    "dropout = td.drop\n",
    "a = td.mh(x,x,x)\n",
    "a = dropout(a)\n",
    "res_a = x + a\n",
    "res_a = td.mhn(res_a)\n",
    "\n",
    "ia = td.imh(res_a, m, m, mask=mask)\n",
    "ia = dropout(ia)\n",
    "res_ia = res_a + ia\n",
    "res_ia = td.imhn(res_ia)\n",
    "\n",
    "pwffn = td.pw(res_ia)\n",
    "pwffn = dropout(pwffn)\n",
    "res_pwffn = res_ia + pwffn\n",
    "res_pwffn = td.pwn(res_pwffn)\n",
    "res_pwffn\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, td::TransformerDecoder1)\n",
    "hs = div(size(td.imh.iqproj.weight)[1], td.imh.head)\n",
    "h, ps = size(td.pw.dout.weight)\n",
    "\n",
    "print(io, \"TransformerDecoder(\")\n",
    "print(io, \"head=$(td.mh.head), \")\n",
    "print(io, \"head_size=$(hs), \")\n",
    "print(io, \"pwffn_size=$(ps), \")\n",
    "print(io, \"size=$(h)\")\n",
    "if Flux.istraining()\n",
    "print(io, \", dropout=$(td.drop.p))\")\n",
    "else\n",
    "print(io, \")\")\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer\n",
    "\n",
    "encode_t1 = Transformer1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t2 = Transformer1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t3 = Transformer1(128, 8, 64, 1024) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_t4 = Transformer1(128, 8, 64, 1024) |> gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define 2 layer of transformer decoder\n",
    "\n",
    "decode_t1 = TransformerDecoder1(128, 8, 64, 1024, act=nelu) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t2 = TransformerDecoder1(128, 8, 64, 1024, act=nelu) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t3 = TransformerDecoder1(128, 8, 64, 1024, act=nelu) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(head=8, head_size=64, pwffn_size=1024, size=128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_t4 = TransformerDecoder1(128, 8, 64, 1024, act=nelu) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positionwise(Dense(128, 13), logsoftmax)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define the layer to get the final output probabilities\n",
    "\n",
    "linear = Positionwise(Dense(128, length(vocab)), logsoftmax) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function encoder_forward(x)\n",
    "    e = embedding(x)\n",
    "    t1 = encode_t1(e)\n",
    "    t2 = encode_t2(t1)\n",
    "    t3 = encode_t2(t2)\n",
    "    t4 = encode_t2(t3)\n",
    "    return t2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder_forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function decoder_forward(x, m)\n",
    "    e = embedding(x)\n",
    "    t1 = decode_t1(e, m)\n",
    "    t2 = decode_t2(t1, m)\n",
    "    t3 = decode_t3(t2, m)\n",
    "    t4 = decode_t4(t3, m)\n",
    "    p = linear(t4)\n",
    "    return p\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder_forward(encoded_sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = decoder_forward(encoded_sample, enc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smooth (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function smooth(et)\n",
    "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
    "    p = sm .* (1 .+ -et)\n",
    "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
    "    label\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@nograd smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define loss function\n",
    "\n",
    "function loss(x, y)\n",
    "  label = onehot(vocab, y) #turn the index to one-hot encoding\n",
    "  label = smooth(label) #perform label smoothing\n",
    "  enc = encoder_forward(x)\n",
    "  probs = decoder_forward(y, enc)\n",
    "  # logcrossentropy used by Katharopoulos et al.\n",
    "  l = logcrossentropy(label[:, 2:end, :], probs[:, 1:end-1, :])  \n",
    "  return l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all the parameters\n",
    "\n",
    "ps = params(embed, pe, encode_t1, encode_t2, encode_t3, encode_t4, decode_t1, decode_t2, decode_t3, decode_t4, linear);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = ADAM(1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define training loop\n",
    "\n",
    "function train!()\n",
    "    @info \"start training\"\n",
    "    losses =[]\n",
    "    for i = 1:100\n",
    "      data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
    "      x, y = preprocess.(data[1]), preprocess.(data[2])\n",
    "      x, y = vocab(x), vocab(y) #encode the data\n",
    "      x, y = todevice(x, y) #move to gpu\n",
    "      grad = gradient(()->loss(x, y), ps)\n",
    "      if i % 8 == 0\n",
    "          l = loss(x, y)\n",
    "          println(\"loss = $l\")\n",
    "      end\n",
    "      update!(opt, ps, grad)\n",
    "      push!(losses, loss(x,y))\n",
    "    end\n",
    "    return losses\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: start training\n",
      "└ @ Main /Users/johannes/Documents/GitHub/DM2022-LinearTransformers/test/LinAt.ipynb:4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.27944545593272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.36003913373465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.02405598345193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.26442343650062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.55338632593747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.24687300611112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 77.00559596883528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.60372571424801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.41622767912942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.59964743826043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.60988572604366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 76.0232659249612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100-element Vector{Any}:\n",
       " 83.18587183973669\n",
       " 81.18755897122291\n",
       " 80.38958137749137\n",
       " 79.37800582695553\n",
       " 79.48831111928693\n",
       " 78.0060780582991\n",
       " 77.63316942691814\n",
       " 77.34876628357966\n",
       " 77.70433247321913\n",
       " 77.74275754125487\n",
       "  ⋮\n",
       " 75.68701454757876\n",
       " 76.1077023955443\n",
       " 75.95226129423138\n",
       " 75.62009502228324\n",
       " 75.77390346025973\n",
       " 75.61004467579836\n",
       " 75.91078513239958\n",
       " 75.12017112688518\n",
       " 75.8064769444809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: start training\n",
      "└ @ Main /Users/johannes/Documents/GitHub/DM2022-LinearTransformers/test/LinAt.ipynb:4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 75.3093925179901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 73.17689909402075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 70.7204534120818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 67.35511613554571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 62.08500335749254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 59.62594237628173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 60.34042382593122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 58.181053394480045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.78369393977786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 56.50356088787359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 57.16098963061923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 56.31002735940224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100-element Vector{Any}:\n",
       " 75.53522175179373\n",
       " 75.56286947213374\n",
       " 75.20936353938775\n",
       " 75.12272364363933\n",
       " 74.95403839886472\n",
       " 75.46368657363344\n",
       " 74.65482647729064\n",
       " 74.75719941542837\n",
       " 74.64050649860845\n",
       " 74.40975265973626\n",
       "  ⋮\n",
       " 54.90250191436421\n",
       " 56.005929305740125\n",
       " 57.11505984001213\n",
       " 57.937886385389724\n",
       " 55.7269991281237\n",
       " 56.96535296613942\n",
       " 57.55589768231686\n",
       " 57.4473819069275\n",
       " 57.069995997453496"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip570\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip570)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip571\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip570)\" d=\"\nM140.858 1486.45 L2352.76 1486.45 L2352.76 47.2441 L140.858 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip572\">\n    <rect x=\"140\" y=\"47\" width=\"2213\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  182.166,1486.45 182.166,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  714.486,1486.45 714.486,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1246.81,1486.45 1246.81,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1779.13,1486.45 1779.13,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2311.45,1486.45 2311.45,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  140.858,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  182.166,1486.45 182.166,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  714.486,1486.45 714.486,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1246.81,1486.45 1246.81,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1779.13,1486.45 1779.13,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2311.45,1486.45 2311.45,1467.55 \n  \"/>\n<path clip-path=\"url(#clip570)\" d=\"M182.166 1517.37 Q178.555 1517.37 176.726 1520.93 Q174.92 1524.47 174.92 1531.6 Q174.92 1538.71 176.726 1542.27 Q178.555 1545.82 182.166 1545.82 Q185.8 1545.82 187.605 1542.27 Q189.434 1538.71 189.434 1531.6 Q189.434 1524.47 187.605 1520.93 Q185.8 1517.37 182.166 1517.37 M182.166 1513.66 Q187.976 1513.66 191.031 1518.27 Q194.11 1522.85 194.11 1531.6 Q194.11 1540.33 191.031 1544.94 Q187.976 1549.52 182.166 1549.52 Q176.355 1549.52 173.277 1544.94 Q170.221 1540.33 170.221 1531.6 Q170.221 1522.85 173.277 1518.27 Q176.355 1513.66 182.166 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M693.757 1544.91 L710.076 1544.91 L710.076 1548.85 L688.132 1548.85 L688.132 1544.91 Q690.794 1542.16 695.378 1537.53 Q699.984 1532.88 701.165 1531.53 Q703.41 1529.01 704.289 1527.27 Q705.192 1525.51 705.192 1523.82 Q705.192 1521.07 703.248 1519.33 Q701.327 1517.6 698.225 1517.6 Q696.026 1517.6 693.572 1518.36 Q691.141 1519.13 688.364 1520.68 L688.364 1515.95 Q691.188 1514.82 693.641 1514.24 Q696.095 1513.66 698.132 1513.66 Q703.502 1513.66 706.697 1516.35 Q709.891 1519.03 709.891 1523.52 Q709.891 1525.65 709.081 1527.57 Q708.294 1529.47 706.188 1532.07 Q705.609 1532.74 702.507 1535.95 Q699.405 1539.15 693.757 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M719.938 1514.29 L738.294 1514.29 L738.294 1518.22 L724.22 1518.22 L724.22 1526.7 Q725.238 1526.35 726.257 1526.19 Q727.275 1526 728.294 1526 Q734.081 1526 737.461 1529.17 Q740.84 1532.34 740.84 1537.76 Q740.84 1543.34 737.368 1546.44 Q733.896 1549.52 727.576 1549.52 Q725.4 1549.52 723.132 1549.15 Q720.887 1548.78 718.479 1548.04 L718.479 1543.34 Q720.563 1544.47 722.785 1545.03 Q725.007 1545.58 727.484 1545.58 Q731.488 1545.58 733.826 1543.48 Q736.164 1541.37 736.164 1537.76 Q736.164 1534.15 733.826 1532.04 Q731.488 1529.94 727.484 1529.94 Q725.609 1529.94 723.734 1530.35 Q721.882 1530.77 719.938 1531.65 L719.938 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M1221.51 1514.29 L1239.86 1514.29 L1239.86 1518.22 L1225.79 1518.22 L1225.79 1526.7 Q1226.81 1526.35 1227.83 1526.19 Q1228.84 1526 1229.86 1526 Q1235.65 1526 1239.03 1529.17 Q1242.41 1532.34 1242.41 1537.76 Q1242.41 1543.34 1238.94 1546.44 Q1235.46 1549.52 1229.14 1549.52 Q1226.97 1549.52 1224.7 1549.15 Q1222.46 1548.78 1220.05 1548.04 L1220.05 1543.34 Q1222.13 1544.47 1224.35 1545.03 Q1226.58 1545.58 1229.05 1545.58 Q1233.06 1545.58 1235.39 1543.48 Q1237.73 1541.37 1237.73 1537.76 Q1237.73 1534.15 1235.39 1532.04 Q1233.06 1529.94 1229.05 1529.94 Q1227.18 1529.94 1225.3 1530.35 Q1223.45 1530.77 1221.51 1531.65 L1221.51 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M1261.62 1517.37 Q1258.01 1517.37 1256.18 1520.93 Q1254.38 1524.47 1254.38 1531.6 Q1254.38 1538.71 1256.18 1542.27 Q1258.01 1545.82 1261.62 1545.82 Q1265.26 1545.82 1267.06 1542.27 Q1268.89 1538.71 1268.89 1531.6 Q1268.89 1524.47 1267.06 1520.93 Q1265.26 1517.37 1261.62 1517.37 M1261.62 1513.66 Q1267.43 1513.66 1270.49 1518.27 Q1273.57 1522.85 1273.57 1531.6 Q1273.57 1540.33 1270.49 1544.94 Q1267.43 1549.52 1261.62 1549.52 Q1255.81 1549.52 1252.73 1544.94 Q1249.68 1540.33 1249.68 1531.6 Q1249.68 1522.85 1252.73 1518.27 Q1255.81 1513.66 1261.62 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M1752.98 1514.29 L1775.2 1514.29 L1775.2 1516.28 L1762.66 1548.85 L1757.77 1548.85 L1769.58 1518.22 L1752.98 1518.22 L1752.98 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M1784.37 1514.29 L1802.73 1514.29 L1802.73 1518.22 L1788.65 1518.22 L1788.65 1526.7 Q1789.67 1526.35 1790.69 1526.19 Q1791.71 1526 1792.73 1526 Q1798.51 1526 1801.89 1529.17 Q1805.27 1532.34 1805.27 1537.76 Q1805.27 1543.34 1801.8 1546.44 Q1798.33 1549.52 1792.01 1549.52 Q1789.83 1549.52 1787.56 1549.15 Q1785.32 1548.78 1782.91 1548.04 L1782.91 1543.34 Q1785 1544.47 1787.22 1545.03 Q1789.44 1545.58 1791.92 1545.58 Q1795.92 1545.58 1798.26 1543.48 Q1800.6 1541.37 1800.6 1537.76 Q1800.6 1534.15 1798.26 1532.04 Q1795.92 1529.94 1791.92 1529.94 Q1790.04 1529.94 1788.17 1530.35 Q1786.31 1530.77 1784.37 1531.65 L1784.37 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M2271.05 1544.91 L2278.69 1544.91 L2278.69 1518.55 L2270.38 1520.21 L2270.38 1515.95 L2278.65 1514.29 L2283.32 1514.29 L2283.32 1544.91 L2290.96 1544.91 L2290.96 1548.85 L2271.05 1548.85 L2271.05 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M2310.41 1517.37 Q2306.8 1517.37 2304.97 1520.93 Q2303.16 1524.47 2303.16 1531.6 Q2303.16 1538.71 2304.97 1542.27 Q2306.8 1545.82 2310.41 1545.82 Q2314.04 1545.82 2315.85 1542.27 Q2317.67 1538.71 2317.67 1531.6 Q2317.67 1524.47 2315.85 1520.93 Q2314.04 1517.37 2310.41 1517.37 M2310.41 1513.66 Q2316.22 1513.66 2319.27 1518.27 Q2322.35 1522.85 2322.35 1531.6 Q2322.35 1540.33 2319.27 1544.94 Q2316.22 1549.52 2310.41 1549.52 Q2304.6 1549.52 2301.52 1544.94 Q2298.46 1540.33 2298.46 1531.6 Q2298.46 1522.85 2301.52 1518.27 Q2304.6 1513.66 2310.41 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M2340.57 1517.37 Q2336.96 1517.37 2335.13 1520.93 Q2333.32 1524.47 2333.32 1531.6 Q2333.32 1538.71 2335.13 1542.27 Q2336.96 1545.82 2340.57 1545.82 Q2344.2 1545.82 2346.01 1542.27 Q2347.84 1538.71 2347.84 1531.6 Q2347.84 1524.47 2346.01 1520.93 Q2344.2 1517.37 2340.57 1517.37 M2340.57 1513.66 Q2346.38 1513.66 2349.43 1518.27 Q2352.51 1522.85 2352.51 1531.6 Q2352.51 1540.33 2349.43 1544.94 Q2346.38 1549.52 2340.57 1549.52 Q2334.76 1549.52 2331.68 1544.94 Q2328.62 1540.33 2328.62 1531.6 Q2328.62 1522.85 2331.68 1518.27 Q2334.76 1513.66 2340.57 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  140.858,1439.31 2352.76,1439.31 \n  \"/>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  140.858,1110.72 2352.76,1110.72 \n  \"/>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  140.858,782.137 2352.76,782.137 \n  \"/>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  140.858,453.552 2352.76,453.552 \n  \"/>\n<polyline clip-path=\"url(#clip572)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  140.858,124.966 2352.76,124.966 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  140.858,1486.45 140.858,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  140.858,1439.31 159.755,1439.31 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  140.858,1110.72 159.755,1110.72 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  140.858,782.137 159.755,782.137 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  140.858,453.552 159.755,453.552 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  140.858,124.966 159.755,124.966 \n  \"/>\n<path clip-path=\"url(#clip570)\" d=\"M53.793 1422.03 L72.1494 1422.03 L72.1494 1425.96 L58.0754 1425.96 L58.0754 1434.44 Q59.0939 1434.09 60.1124 1433.93 Q61.131 1433.74 62.1495 1433.74 Q67.9365 1433.74 71.3161 1436.91 Q74.6957 1440.08 74.6957 1445.5 Q74.6957 1451.08 71.2235 1454.18 Q67.7513 1457.26 61.4319 1457.26 Q59.256 1457.26 56.9875 1456.89 Q54.7421 1456.52 52.3347 1455.78 L52.3347 1451.08 Q54.418 1452.21 56.6402 1452.77 Q58.8625 1453.32 61.3393 1453.32 Q65.3439 1453.32 67.6819 1451.22 Q70.0198 1449.11 70.0198 1445.5 Q70.0198 1441.89 67.6819 1439.78 Q65.3439 1437.68 61.3393 1437.68 Q59.4643 1437.68 57.5893 1438.09 Q55.7375 1438.51 53.793 1439.39 L53.793 1422.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M83.9549 1422.03 L102.311 1422.03 L102.311 1425.96 L88.2373 1425.96 L88.2373 1434.44 Q89.2558 1434.09 90.2743 1433.93 Q91.2928 1433.74 92.3113 1433.74 Q98.0984 1433.74 101.478 1436.91 Q104.858 1440.08 104.858 1445.5 Q104.858 1451.08 101.385 1454.18 Q97.9132 1457.26 91.5938 1457.26 Q89.4178 1457.26 87.1493 1456.89 Q84.904 1456.52 82.4966 1455.78 L82.4966 1451.08 Q84.5799 1452.21 86.8021 1452.77 Q89.0243 1453.32 91.5012 1453.32 Q95.5058 1453.32 97.8437 1451.22 Q100.182 1449.11 100.182 1445.5 Q100.182 1441.89 97.8437 1439.78 Q95.5058 1437.68 91.5012 1437.68 Q89.6262 1437.68 87.7512 1438.09 Q85.8993 1438.51 83.9549 1439.39 L83.9549 1422.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M63.33 1108.86 Q60.1819 1108.86 58.33 1111.01 Q56.5014 1113.17 56.5014 1116.92 Q56.5014 1120.64 58.33 1122.82 Q60.1819 1124.97 63.33 1124.97 Q66.4782 1124.97 68.3068 1122.82 Q70.1587 1120.64 70.1587 1116.92 Q70.1587 1113.17 68.3068 1111.01 Q66.4782 1108.86 63.33 1108.86 M72.6124 1094.21 L72.6124 1098.47 Q70.8531 1097.63 69.0476 1097.19 Q67.2652 1096.75 65.5059 1096.75 Q60.8763 1096.75 58.4226 1099.88 Q55.9921 1103 55.6449 1109.32 Q57.0106 1107.31 59.0708 1106.24 Q61.131 1105.16 63.6078 1105.16 Q68.8161 1105.16 71.8253 1108.33 Q74.8577 1111.48 74.8577 1116.92 Q74.8577 1122.24 71.7096 1125.46 Q68.5615 1128.67 63.33 1128.67 Q57.3347 1128.67 54.1634 1124.09 Q50.9921 1119.48 50.9921 1110.76 Q50.9921 1102.56 54.881 1097.7 Q58.7699 1092.82 65.3208 1092.82 Q67.08 1092.82 68.8624 1093.17 Q70.6679 1093.51 72.6124 1094.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M92.9132 1096.52 Q89.3021 1096.52 87.4734 1100.09 Q85.6679 1103.63 85.6679 1110.76 Q85.6679 1117.86 87.4734 1121.43 Q89.3021 1124.97 92.9132 1124.97 Q96.5474 1124.97 98.353 1121.43 Q100.182 1117.86 100.182 1110.76 Q100.182 1103.63 98.353 1100.09 Q96.5474 1096.52 92.9132 1096.52 M92.9132 1092.82 Q98.7234 1092.82 101.779 1097.42 Q104.858 1102.01 104.858 1110.76 Q104.858 1119.48 101.779 1124.09 Q98.7234 1128.67 92.9132 1128.67 Q87.103 1128.67 84.0244 1124.09 Q80.9688 1119.48 80.9688 1110.76 Q80.9688 1102.01 84.0244 1097.42 Q87.103 1092.82 92.9132 1092.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M64.3254 780.274 Q61.1773 780.274 59.3254 782.427 Q57.4967 784.58 57.4967 788.33 Q57.4967 792.056 59.3254 794.232 Q61.1773 796.385 64.3254 796.385 Q67.4735 796.385 69.3022 794.232 Q71.1541 792.056 71.1541 788.33 Q71.1541 784.58 69.3022 782.427 Q67.4735 780.274 64.3254 780.274 M73.6077 765.621 L73.6077 769.881 Q71.8485 769.047 70.0429 768.607 Q68.2606 768.168 66.5013 768.168 Q61.8717 768.168 59.418 771.293 Q56.9875 774.418 56.6402 780.737 Q58.006 778.723 60.0662 777.658 Q62.1263 776.57 64.6032 776.57 Q69.8115 776.57 72.8207 779.742 Q75.8531 782.89 75.8531 788.33 Q75.8531 793.654 72.705 796.871 Q69.5568 800.089 64.3254 800.089 Q58.33 800.089 55.1588 795.505 Q51.9875 790.899 51.9875 782.172 Q51.9875 773.978 55.8764 769.117 Q59.7652 764.232 66.3161 764.232 Q68.0754 764.232 69.8578 764.58 Q71.6633 764.927 73.6077 765.621 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M83.9549 764.857 L102.311 764.857 L102.311 768.793 L88.2373 768.793 L88.2373 777.265 Q89.2558 776.918 90.2743 776.755 Q91.2928 776.57 92.3113 776.57 Q98.0984 776.57 101.478 779.742 Q104.858 782.913 104.858 788.33 Q104.858 793.908 101.385 797.01 Q97.9132 800.089 91.5938 800.089 Q89.4178 800.089 87.1493 799.718 Q84.904 799.348 82.4966 798.607 L82.4966 793.908 Q84.5799 795.042 86.8021 795.598 Q89.0243 796.154 91.5012 796.154 Q95.5058 796.154 97.8437 794.047 Q100.182 791.941 100.182 788.33 Q100.182 784.718 97.8437 782.612 Q95.5058 780.505 91.5012 780.505 Q89.6262 780.505 87.7512 780.922 Q85.8993 781.339 83.9549 782.218 L83.9549 764.857 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M51.5708 436.272 L73.7929 436.272 L73.7929 438.263 L61.2467 470.832 L56.3625 470.832 L68.168 440.207 L51.5708 440.207 L51.5708 436.272 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M92.9132 439.351 Q89.3021 439.351 87.4734 442.915 Q85.6679 446.457 85.6679 453.587 Q85.6679 460.693 87.4734 464.258 Q89.3021 467.8 92.9132 467.8 Q96.5474 467.8 98.353 464.258 Q100.182 460.693 100.182 453.587 Q100.182 446.457 98.353 442.915 Q96.5474 439.351 92.9132 439.351 M92.9132 435.647 Q98.7234 435.647 101.779 440.253 Q104.858 444.837 104.858 453.587 Q104.858 462.313 101.779 466.92 Q98.7234 471.503 92.9132 471.503 Q87.103 471.503 84.0244 466.92 Q80.9688 462.313 80.9688 453.587 Q80.9688 444.837 84.0244 440.253 Q87.103 435.647 92.9132 435.647 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M52.5662 107.686 L74.7883 107.686 L74.7883 109.677 L62.2421 142.246 L57.3578 142.246 L69.1633 111.622 L52.5662 111.622 L52.5662 107.686 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M83.9549 107.686 L102.311 107.686 L102.311 111.622 L88.2373 111.622 L88.2373 120.094 Q89.2558 119.747 90.2743 119.585 Q91.2928 119.399 92.3113 119.399 Q98.0984 119.399 101.478 122.571 Q104.858 125.742 104.858 131.159 Q104.858 136.737 101.385 139.839 Q97.9132 142.918 91.5938 142.918 Q89.4178 142.918 87.1493 142.547 Q84.904 142.177 82.4966 141.436 L82.4966 136.737 Q84.5799 137.871 86.8021 138.427 Q89.0243 138.983 91.5012 138.983 Q95.5058 138.983 97.8437 136.876 Q100.182 134.77 100.182 131.159 Q100.182 127.547 97.8437 125.441 Q95.5058 123.334 91.5012 123.334 Q89.6262 123.334 87.7512 123.751 Q85.8993 124.168 83.9549 125.047 L83.9549 107.686 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip572)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  203.458,87.9763 224.751,111.208 246.044,116.901 267.337,127.987 288.63,94.4943 309.923,147.65 331.215,140.923 352.508,148.591 373.801,163.756 395.094,161.92 \n  416.387,135.131 437.68,199.751 458.972,206.008 480.265,202.334 501.558,258.164 522.851,262.473 544.144,368.394 565.436,255.61 586.729,438.766 608.022,442.442 \n  629.315,414.816 650.608,490.969 671.901,470.893 693.193,508.193 714.486,691.372 735.779,542.154 757.072,741.546 778.365,773.396 799.657,829.893 820.95,601.755 \n  842.243,629.12 863.536,977.721 884.829,721.952 906.122,1004.15 927.414,903.152 948.707,911.483 970,1051.54 991.293,939.506 1012.59,1157.32 1033.88,1006.93 \n  1055.17,1097.43 1076.46,1030.57 1097.76,1121.67 1119.05,1086.17 1140.34,1085.51 1161.64,1132.22 1182.93,1238.79 1204.22,1174.33 1225.51,1209.46 1246.81,1087.56 \n  1268.1,1204.55 1289.39,1307.32 1310.69,1263.78 1331.98,1231.71 1353.27,1163.02 1374.56,1221.98 1395.86,1299.62 1417.15,1220.43 1438.44,1360.34 1459.73,1240.18 \n  1481.03,1280.32 1502.32,1278.48 1523.61,1267.71 1544.91,1194.71 1566.2,1271.62 1587.49,1309.08 1608.78,1181.12 1630.08,1285.73 1651.37,1282.17 1672.66,1260.6 \n  1693.96,1311.91 1715.25,1349.78 1736.54,1349.34 1757.83,1256.91 1779.13,1336.09 1800.42,1196.76 1821.71,1436.07 1843.01,1408.89 1864.3,1340.4 1885.59,1292.97 \n  1906.88,1312.01 1928.18,1306.73 1949.47,1238.91 1970.76,1314.78 1992.06,1275.45 2013.35,1280.43 2034.64,1320.18 2055.93,1283.31 2077.23,1284.65 2098.52,1342.12 \n  2119.81,1445.72 2141.11,1373.2 2162.4,1300.31 2183.69,1246.24 2204.98,1391.53 2226.28,1310.15 2247.57,1271.34 2268.86,1278.47 2290.16,1303.27 \n  \"/>\n<path clip-path=\"url(#clip570)\" d=\"\nM1888.15 198.898 L2279.03 198.898 L2279.03 95.2176 L1888.15 95.2176  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1888.15,198.898 2279.03,198.898 2279.03,95.2176 1888.15,95.2176 1888.15,198.898 \n  \"/>\n<polyline clip-path=\"url(#clip570)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1912.15,147.058 2056.15,147.058 \n  \"/>\n<path clip-path=\"url(#clip570)\" d=\"M2080.15 129.778 L2084.82 129.778 L2084.82 160.402 L2101.65 160.402 L2101.65 164.338 L2080.15 164.338 L2080.15 129.778 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M2115.59 141.398 Q2112.16 141.398 2110.17 144.083 Q2108.18 146.745 2108.18 151.398 Q2108.18 156.051 2110.15 158.736 Q2112.14 161.398 2115.59 161.398 Q2118.99 161.398 2120.98 158.713 Q2122.97 156.027 2122.97 151.398 Q2122.97 146.791 2120.98 144.106 Q2118.99 141.398 2115.59 141.398 M2115.59 137.787 Q2121.14 137.787 2124.31 141.398 Q2127.48 145.009 2127.48 151.398 Q2127.48 157.764 2124.31 161.398 Q2121.14 165.009 2115.59 165.009 Q2110.01 165.009 2106.84 161.398 Q2103.69 157.764 2103.69 151.398 Q2103.69 145.009 2106.84 141.398 Q2110.01 137.787 2115.59 137.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M2151.07 139.176 L2151.07 143.203 Q2149.27 142.277 2147.32 141.815 Q2145.38 141.352 2143.29 141.352 Q2140.12 141.352 2138.53 142.324 Q2136.95 143.296 2136.95 145.24 Q2136.95 146.722 2138.09 147.578 Q2139.22 148.412 2142.65 149.176 L2144.11 149.5 Q2148.64 150.472 2150.54 152.254 Q2152.46 154.014 2152.46 157.185 Q2152.46 160.796 2149.59 162.902 Q2146.74 165.009 2141.74 165.009 Q2139.66 165.009 2137.39 164.592 Q2135.15 164.199 2132.65 163.388 L2132.65 158.99 Q2135.01 160.217 2137.3 160.842 Q2139.59 161.444 2141.84 161.444 Q2144.85 161.444 2146.47 160.426 Q2148.09 159.384 2148.09 157.509 Q2148.09 155.773 2146.91 154.847 Q2145.75 153.921 2141.79 153.064 L2140.31 152.717 Q2136.35 151.884 2134.59 150.171 Q2132.83 148.435 2132.83 145.426 Q2132.83 141.768 2135.42 139.778 Q2138.02 137.787 2142.79 137.787 Q2145.15 137.787 2147.23 138.134 Q2149.31 138.481 2151.07 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M2175.77 139.176 L2175.77 143.203 Q2173.97 142.277 2172.02 141.815 Q2170.08 141.352 2167.99 141.352 Q2164.82 141.352 2163.23 142.324 Q2161.65 143.296 2161.65 145.24 Q2161.65 146.722 2162.79 147.578 Q2163.92 148.412 2167.35 149.176 L2168.8 149.5 Q2173.34 150.472 2175.24 152.254 Q2177.16 154.014 2177.16 157.185 Q2177.16 160.796 2174.29 162.902 Q2171.44 165.009 2166.44 165.009 Q2164.36 165.009 2162.09 164.592 Q2159.85 164.199 2157.35 163.388 L2157.35 158.99 Q2159.71 160.217 2162 160.842 Q2164.29 161.444 2166.54 161.444 Q2169.54 161.444 2171.17 160.426 Q2172.79 159.384 2172.79 157.509 Q2172.79 155.773 2171.6 154.847 Q2170.45 153.921 2166.49 153.064 L2165.01 152.717 Q2161.05 151.884 2159.29 150.171 Q2157.53 148.435 2157.53 145.426 Q2157.53 141.768 2160.12 139.778 Q2162.72 137.787 2167.48 137.787 Q2169.85 137.787 2171.93 138.134 Q2174.01 138.481 2175.77 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M2206.12 150.31 L2206.12 152.393 L2186.54 152.393 Q2186.81 156.791 2189.17 159.106 Q2191.56 161.398 2195.79 161.398 Q2198.25 161.398 2200.54 160.796 Q2202.85 160.194 2205.12 158.99 L2205.12 163.018 Q2202.83 163.99 2200.42 164.5 Q2198.02 165.009 2195.54 165.009 Q2189.34 165.009 2185.7 161.398 Q2182.09 157.787 2182.09 151.629 Q2182.09 145.264 2185.52 141.537 Q2188.97 137.787 2194.8 137.787 Q2200.03 137.787 2203.06 141.166 Q2206.12 144.523 2206.12 150.31 M2201.86 149.06 Q2201.81 145.565 2199.89 143.481 Q2197.99 141.398 2194.85 141.398 Q2191.28 141.398 2189.13 143.412 Q2187 145.426 2186.67 149.083 L2201.86 149.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip570)\" d=\"M2229.64 139.176 L2229.64 143.203 Q2227.83 142.277 2225.89 141.815 Q2223.94 141.352 2221.86 141.352 Q2218.69 141.352 2217.09 142.324 Q2215.52 143.296 2215.52 145.24 Q2215.52 146.722 2216.65 147.578 Q2217.79 148.412 2221.21 149.176 L2222.67 149.5 Q2227.21 150.472 2229.1 152.254 Q2231.03 154.014 2231.03 157.185 Q2231.03 160.796 2228.16 162.902 Q2225.31 165.009 2220.31 165.009 Q2218.23 165.009 2215.96 164.592 Q2213.71 164.199 2211.21 163.388 L2211.21 158.99 Q2213.57 160.217 2215.86 160.842 Q2218.16 161.444 2220.4 161.444 Q2223.41 161.444 2225.03 160.426 Q2226.65 159.384 2226.65 157.509 Q2226.65 155.773 2225.47 154.847 Q2224.31 153.921 2220.35 153.064 L2218.87 152.717 Q2214.91 151.884 2213.16 150.171 Q2211.4 148.435 2211.4 145.426 Q2211.4 141.768 2213.99 139.778 Q2216.58 137.787 2221.35 137.787 Q2223.71 137.787 2225.79 138.134 Q2227.88 138.481 2229.64 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n",
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip620\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip620)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip621\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip620)\" d=\"\n",
       "M140.858 1486.45 L2352.76 1486.45 L2352.76 47.2441 L140.858 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip622\">\n",
       "    <rect x=\"140\" y=\"47\" width=\"2213\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.166,1486.45 182.166,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  714.486,1486.45 714.486,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1246.81,1486.45 1246.81,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1779.13,1486.45 1779.13,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2311.45,1486.45 2311.45,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  140.858,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.166,1486.45 182.166,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  714.486,1486.45 714.486,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1246.81,1486.45 1246.81,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1779.13,1486.45 1779.13,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2311.45,1486.45 2311.45,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M182.166 1517.37 Q178.555 1517.37 176.726 1520.93 Q174.92 1524.47 174.92 1531.6 Q174.92 1538.71 176.726 1542.27 Q178.555 1545.82 182.166 1545.82 Q185.8 1545.82 187.605 1542.27 Q189.434 1538.71 189.434 1531.6 Q189.434 1524.47 187.605 1520.93 Q185.8 1517.37 182.166 1517.37 M182.166 1513.66 Q187.976 1513.66 191.031 1518.27 Q194.11 1522.85 194.11 1531.6 Q194.11 1540.33 191.031 1544.94 Q187.976 1549.52 182.166 1549.52 Q176.355 1549.52 173.277 1544.94 Q170.221 1540.33 170.221 1531.6 Q170.221 1522.85 173.277 1518.27 Q176.355 1513.66 182.166 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M693.757 1544.91 L710.076 1544.91 L710.076 1548.85 L688.132 1548.85 L688.132 1544.91 Q690.794 1542.16 695.378 1537.53 Q699.984 1532.88 701.165 1531.53 Q703.41 1529.01 704.289 1527.27 Q705.192 1525.51 705.192 1523.82 Q705.192 1521.07 703.248 1519.33 Q701.327 1517.6 698.225 1517.6 Q696.026 1517.6 693.572 1518.36 Q691.141 1519.13 688.364 1520.68 L688.364 1515.95 Q691.188 1514.82 693.641 1514.24 Q696.095 1513.66 698.132 1513.66 Q703.502 1513.66 706.697 1516.35 Q709.891 1519.03 709.891 1523.52 Q709.891 1525.65 709.081 1527.57 Q708.294 1529.47 706.188 1532.07 Q705.609 1532.74 702.507 1535.95 Q699.405 1539.15 693.757 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M719.938 1514.29 L738.294 1514.29 L738.294 1518.22 L724.22 1518.22 L724.22 1526.7 Q725.238 1526.35 726.257 1526.19 Q727.275 1526 728.294 1526 Q734.081 1526 737.461 1529.17 Q740.84 1532.34 740.84 1537.76 Q740.84 1543.34 737.368 1546.44 Q733.896 1549.52 727.576 1549.52 Q725.4 1549.52 723.132 1549.15 Q720.887 1548.78 718.479 1548.04 L718.479 1543.34 Q720.563 1544.47 722.785 1545.03 Q725.007 1545.58 727.484 1545.58 Q731.488 1545.58 733.826 1543.48 Q736.164 1541.37 736.164 1537.76 Q736.164 1534.15 733.826 1532.04 Q731.488 1529.94 727.484 1529.94 Q725.609 1529.94 723.734 1530.35 Q721.882 1530.77 719.938 1531.65 L719.938 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1221.51 1514.29 L1239.86 1514.29 L1239.86 1518.22 L1225.79 1518.22 L1225.79 1526.7 Q1226.81 1526.35 1227.83 1526.19 Q1228.84 1526 1229.86 1526 Q1235.65 1526 1239.03 1529.17 Q1242.41 1532.34 1242.41 1537.76 Q1242.41 1543.34 1238.94 1546.44 Q1235.46 1549.52 1229.14 1549.52 Q1226.97 1549.52 1224.7 1549.15 Q1222.46 1548.78 1220.05 1548.04 L1220.05 1543.34 Q1222.13 1544.47 1224.35 1545.03 Q1226.58 1545.58 1229.05 1545.58 Q1233.06 1545.58 1235.39 1543.48 Q1237.73 1541.37 1237.73 1537.76 Q1237.73 1534.15 1235.39 1532.04 Q1233.06 1529.94 1229.05 1529.94 Q1227.18 1529.94 1225.3 1530.35 Q1223.45 1530.77 1221.51 1531.65 L1221.51 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1261.62 1517.37 Q1258.01 1517.37 1256.18 1520.93 Q1254.38 1524.47 1254.38 1531.6 Q1254.38 1538.71 1256.18 1542.27 Q1258.01 1545.82 1261.62 1545.82 Q1265.26 1545.82 1267.06 1542.27 Q1268.89 1538.71 1268.89 1531.6 Q1268.89 1524.47 1267.06 1520.93 Q1265.26 1517.37 1261.62 1517.37 M1261.62 1513.66 Q1267.43 1513.66 1270.49 1518.27 Q1273.57 1522.85 1273.57 1531.6 Q1273.57 1540.33 1270.49 1544.94 Q1267.43 1549.52 1261.62 1549.52 Q1255.81 1549.52 1252.73 1544.94 Q1249.68 1540.33 1249.68 1531.6 Q1249.68 1522.85 1252.73 1518.27 Q1255.81 1513.66 1261.62 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1752.98 1514.29 L1775.2 1514.29 L1775.2 1516.28 L1762.66 1548.85 L1757.77 1548.85 L1769.58 1518.22 L1752.98 1518.22 L1752.98 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1784.37 1514.29 L1802.73 1514.29 L1802.73 1518.22 L1788.65 1518.22 L1788.65 1526.7 Q1789.67 1526.35 1790.69 1526.19 Q1791.71 1526 1792.73 1526 Q1798.51 1526 1801.89 1529.17 Q1805.27 1532.34 1805.27 1537.76 Q1805.27 1543.34 1801.8 1546.44 Q1798.33 1549.52 1792.01 1549.52 Q1789.83 1549.52 1787.56 1549.15 Q1785.32 1548.78 1782.91 1548.04 L1782.91 1543.34 Q1785 1544.47 1787.22 1545.03 Q1789.44 1545.58 1791.92 1545.58 Q1795.92 1545.58 1798.26 1543.48 Q1800.6 1541.37 1800.6 1537.76 Q1800.6 1534.15 1798.26 1532.04 Q1795.92 1529.94 1791.92 1529.94 Q1790.04 1529.94 1788.17 1530.35 Q1786.31 1530.77 1784.37 1531.65 L1784.37 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2271.05 1544.91 L2278.69 1544.91 L2278.69 1518.55 L2270.38 1520.21 L2270.38 1515.95 L2278.65 1514.29 L2283.32 1514.29 L2283.32 1544.91 L2290.96 1544.91 L2290.96 1548.85 L2271.05 1548.85 L2271.05 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2310.41 1517.37 Q2306.8 1517.37 2304.97 1520.93 Q2303.16 1524.47 2303.16 1531.6 Q2303.16 1538.71 2304.97 1542.27 Q2306.8 1545.82 2310.41 1545.82 Q2314.04 1545.82 2315.85 1542.27 Q2317.67 1538.71 2317.67 1531.6 Q2317.67 1524.47 2315.85 1520.93 Q2314.04 1517.37 2310.41 1517.37 M2310.41 1513.66 Q2316.22 1513.66 2319.27 1518.27 Q2322.35 1522.85 2322.35 1531.6 Q2322.35 1540.33 2319.27 1544.94 Q2316.22 1549.52 2310.41 1549.52 Q2304.6 1549.52 2301.52 1544.94 Q2298.46 1540.33 2298.46 1531.6 Q2298.46 1522.85 2301.52 1518.27 Q2304.6 1513.66 2310.41 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2340.57 1517.37 Q2336.96 1517.37 2335.13 1520.93 Q2333.32 1524.47 2333.32 1531.6 Q2333.32 1538.71 2335.13 1542.27 Q2336.96 1545.82 2340.57 1545.82 Q2344.2 1545.82 2346.01 1542.27 Q2347.84 1538.71 2347.84 1531.6 Q2347.84 1524.47 2346.01 1520.93 Q2344.2 1517.37 2340.57 1517.37 M2340.57 1513.66 Q2346.38 1513.66 2349.43 1518.27 Q2352.51 1522.85 2352.51 1531.6 Q2352.51 1540.33 2349.43 1544.94 Q2346.38 1549.52 2340.57 1549.52 Q2334.76 1549.52 2331.68 1544.94 Q2328.62 1540.33 2328.62 1531.6 Q2328.62 1522.85 2331.68 1518.27 Q2334.76 1513.66 2340.57 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  140.858,1439.31 2352.76,1439.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  140.858,1110.72 2352.76,1110.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  140.858,782.137 2352.76,782.137 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  140.858,453.552 2352.76,453.552 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  140.858,124.966 2352.76,124.966 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  140.858,1486.45 140.858,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  140.858,1439.31 159.755,1439.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  140.858,1110.72 159.755,1110.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  140.858,782.137 159.755,782.137 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  140.858,453.552 159.755,453.552 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  140.858,124.966 159.755,124.966 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M53.793 1422.03 L72.1494 1422.03 L72.1494 1425.96 L58.0754 1425.96 L58.0754 1434.44 Q59.0939 1434.09 60.1124 1433.93 Q61.131 1433.74 62.1495 1433.74 Q67.9365 1433.74 71.3161 1436.91 Q74.6957 1440.08 74.6957 1445.5 Q74.6957 1451.08 71.2235 1454.18 Q67.7513 1457.26 61.4319 1457.26 Q59.256 1457.26 56.9875 1456.89 Q54.7421 1456.52 52.3347 1455.78 L52.3347 1451.08 Q54.418 1452.21 56.6402 1452.77 Q58.8625 1453.32 61.3393 1453.32 Q65.3439 1453.32 67.6819 1451.22 Q70.0198 1449.11 70.0198 1445.5 Q70.0198 1441.89 67.6819 1439.78 Q65.3439 1437.68 61.3393 1437.68 Q59.4643 1437.68 57.5893 1438.09 Q55.7375 1438.51 53.793 1439.39 L53.793 1422.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M83.9549 1422.03 L102.311 1422.03 L102.311 1425.96 L88.2373 1425.96 L88.2373 1434.44 Q89.2558 1434.09 90.2743 1433.93 Q91.2928 1433.74 92.3113 1433.74 Q98.0984 1433.74 101.478 1436.91 Q104.858 1440.08 104.858 1445.5 Q104.858 1451.08 101.385 1454.18 Q97.9132 1457.26 91.5938 1457.26 Q89.4178 1457.26 87.1493 1456.89 Q84.904 1456.52 82.4966 1455.78 L82.4966 1451.08 Q84.5799 1452.21 86.8021 1452.77 Q89.0243 1453.32 91.5012 1453.32 Q95.5058 1453.32 97.8437 1451.22 Q100.182 1449.11 100.182 1445.5 Q100.182 1441.89 97.8437 1439.78 Q95.5058 1437.68 91.5012 1437.68 Q89.6262 1437.68 87.7512 1438.09 Q85.8993 1438.51 83.9549 1439.39 L83.9549 1422.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M63.33 1108.86 Q60.1819 1108.86 58.33 1111.01 Q56.5014 1113.17 56.5014 1116.92 Q56.5014 1120.64 58.33 1122.82 Q60.1819 1124.97 63.33 1124.97 Q66.4782 1124.97 68.3068 1122.82 Q70.1587 1120.64 70.1587 1116.92 Q70.1587 1113.17 68.3068 1111.01 Q66.4782 1108.86 63.33 1108.86 M72.6124 1094.21 L72.6124 1098.47 Q70.8531 1097.63 69.0476 1097.19 Q67.2652 1096.75 65.5059 1096.75 Q60.8763 1096.75 58.4226 1099.88 Q55.9921 1103 55.6449 1109.32 Q57.0106 1107.31 59.0708 1106.24 Q61.131 1105.16 63.6078 1105.16 Q68.8161 1105.16 71.8253 1108.33 Q74.8577 1111.48 74.8577 1116.92 Q74.8577 1122.24 71.7096 1125.46 Q68.5615 1128.67 63.33 1128.67 Q57.3347 1128.67 54.1634 1124.09 Q50.9921 1119.48 50.9921 1110.76 Q50.9921 1102.56 54.881 1097.7 Q58.7699 1092.82 65.3208 1092.82 Q67.08 1092.82 68.8624 1093.17 Q70.6679 1093.51 72.6124 1094.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M92.9132 1096.52 Q89.3021 1096.52 87.4734 1100.09 Q85.6679 1103.63 85.6679 1110.76 Q85.6679 1117.86 87.4734 1121.43 Q89.3021 1124.97 92.9132 1124.97 Q96.5474 1124.97 98.353 1121.43 Q100.182 1117.86 100.182 1110.76 Q100.182 1103.63 98.353 1100.09 Q96.5474 1096.52 92.9132 1096.52 M92.9132 1092.82 Q98.7234 1092.82 101.779 1097.42 Q104.858 1102.01 104.858 1110.76 Q104.858 1119.48 101.779 1124.09 Q98.7234 1128.67 92.9132 1128.67 Q87.103 1128.67 84.0244 1124.09 Q80.9688 1119.48 80.9688 1110.76 Q80.9688 1102.01 84.0244 1097.42 Q87.103 1092.82 92.9132 1092.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M64.3254 780.274 Q61.1773 780.274 59.3254 782.427 Q57.4967 784.58 57.4967 788.33 Q57.4967 792.056 59.3254 794.232 Q61.1773 796.385 64.3254 796.385 Q67.4735 796.385 69.3022 794.232 Q71.1541 792.056 71.1541 788.33 Q71.1541 784.58 69.3022 782.427 Q67.4735 780.274 64.3254 780.274 M73.6077 765.621 L73.6077 769.881 Q71.8485 769.047 70.0429 768.607 Q68.2606 768.168 66.5013 768.168 Q61.8717 768.168 59.418 771.293 Q56.9875 774.418 56.6402 780.737 Q58.006 778.723 60.0662 777.658 Q62.1263 776.57 64.6032 776.57 Q69.8115 776.57 72.8207 779.742 Q75.8531 782.89 75.8531 788.33 Q75.8531 793.654 72.705 796.871 Q69.5568 800.089 64.3254 800.089 Q58.33 800.089 55.1588 795.505 Q51.9875 790.899 51.9875 782.172 Q51.9875 773.978 55.8764 769.117 Q59.7652 764.232 66.3161 764.232 Q68.0754 764.232 69.8578 764.58 Q71.6633 764.927 73.6077 765.621 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M83.9549 764.857 L102.311 764.857 L102.311 768.793 L88.2373 768.793 L88.2373 777.265 Q89.2558 776.918 90.2743 776.755 Q91.2928 776.57 92.3113 776.57 Q98.0984 776.57 101.478 779.742 Q104.858 782.913 104.858 788.33 Q104.858 793.908 101.385 797.01 Q97.9132 800.089 91.5938 800.089 Q89.4178 800.089 87.1493 799.718 Q84.904 799.348 82.4966 798.607 L82.4966 793.908 Q84.5799 795.042 86.8021 795.598 Q89.0243 796.154 91.5012 796.154 Q95.5058 796.154 97.8437 794.047 Q100.182 791.941 100.182 788.33 Q100.182 784.718 97.8437 782.612 Q95.5058 780.505 91.5012 780.505 Q89.6262 780.505 87.7512 780.922 Q85.8993 781.339 83.9549 782.218 L83.9549 764.857 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M51.5708 436.272 L73.7929 436.272 L73.7929 438.263 L61.2467 470.832 L56.3625 470.832 L68.168 440.207 L51.5708 440.207 L51.5708 436.272 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M92.9132 439.351 Q89.3021 439.351 87.4734 442.915 Q85.6679 446.457 85.6679 453.587 Q85.6679 460.693 87.4734 464.258 Q89.3021 467.8 92.9132 467.8 Q96.5474 467.8 98.353 464.258 Q100.182 460.693 100.182 453.587 Q100.182 446.457 98.353 442.915 Q96.5474 439.351 92.9132 439.351 M92.9132 435.647 Q98.7234 435.647 101.779 440.253 Q104.858 444.837 104.858 453.587 Q104.858 462.313 101.779 466.92 Q98.7234 471.503 92.9132 471.503 Q87.103 471.503 84.0244 466.92 Q80.9688 462.313 80.9688 453.587 Q80.9688 444.837 84.0244 440.253 Q87.103 435.647 92.9132 435.647 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M52.5662 107.686 L74.7883 107.686 L74.7883 109.677 L62.2421 142.246 L57.3578 142.246 L69.1633 111.622 L52.5662 111.622 L52.5662 107.686 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M83.9549 107.686 L102.311 107.686 L102.311 111.622 L88.2373 111.622 L88.2373 120.094 Q89.2558 119.747 90.2743 119.585 Q91.2928 119.399 92.3113 119.399 Q98.0984 119.399 101.478 122.571 Q104.858 125.742 104.858 131.159 Q104.858 136.737 101.385 139.839 Q97.9132 142.918 91.5938 142.918 Q89.4178 142.918 87.1493 142.547 Q84.904 142.177 82.4966 141.436 L82.4966 136.737 Q84.5799 137.871 86.8021 138.427 Q89.0243 138.983 91.5012 138.983 Q95.5058 138.983 97.8437 136.876 Q100.182 134.77 100.182 131.159 Q100.182 127.547 97.8437 125.441 Q95.5058 123.334 91.5012 123.334 Q89.6262 123.334 87.7512 123.751 Q85.8993 124.168 83.9549 125.047 L83.9549 107.686 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip622)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  203.458,87.9763 224.751,111.208 246.044,116.901 267.337,127.987 288.63,94.4943 309.923,147.65 331.215,140.923 352.508,148.591 373.801,163.756 395.094,161.92 \n",
       "  416.387,135.131 437.68,199.751 458.972,206.008 480.265,202.334 501.558,258.164 522.851,262.473 544.144,368.394 565.436,255.61 586.729,438.766 608.022,442.442 \n",
       "  629.315,414.816 650.608,490.969 671.901,470.893 693.193,508.193 714.486,691.372 735.779,542.154 757.072,741.546 778.365,773.396 799.657,829.893 820.95,601.755 \n",
       "  842.243,629.12 863.536,977.721 884.829,721.952 906.122,1004.15 927.414,903.152 948.707,911.483 970,1051.54 991.293,939.506 1012.59,1157.32 1033.88,1006.93 \n",
       "  1055.17,1097.43 1076.46,1030.57 1097.76,1121.67 1119.05,1086.17 1140.34,1085.51 1161.64,1132.22 1182.93,1238.79 1204.22,1174.33 1225.51,1209.46 1246.81,1087.56 \n",
       "  1268.1,1204.55 1289.39,1307.32 1310.69,1263.78 1331.98,1231.71 1353.27,1163.02 1374.56,1221.98 1395.86,1299.62 1417.15,1220.43 1438.44,1360.34 1459.73,1240.18 \n",
       "  1481.03,1280.32 1502.32,1278.48 1523.61,1267.71 1544.91,1194.71 1566.2,1271.62 1587.49,1309.08 1608.78,1181.12 1630.08,1285.73 1651.37,1282.17 1672.66,1260.6 \n",
       "  1693.96,1311.91 1715.25,1349.78 1736.54,1349.34 1757.83,1256.91 1779.13,1336.09 1800.42,1196.76 1821.71,1436.07 1843.01,1408.89 1864.3,1340.4 1885.59,1292.97 \n",
       "  1906.88,1312.01 1928.18,1306.73 1949.47,1238.91 1970.76,1314.78 1992.06,1275.45 2013.35,1280.43 2034.64,1320.18 2055.93,1283.31 2077.23,1284.65 2098.52,1342.12 \n",
       "  2119.81,1445.72 2141.11,1373.2 2162.4,1300.31 2183.69,1246.24 2204.98,1391.53 2226.28,1310.15 2247.57,1271.34 2268.86,1278.47 2290.16,1303.27 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip620)\" d=\"\n",
       "M1888.15 198.898 L2279.03 198.898 L2279.03 95.2176 L1888.15 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1888.15,198.898 2279.03,198.898 2279.03,95.2176 1888.15,95.2176 1888.15,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1912.15,147.058 2056.15,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M2080.15 129.778 L2084.82 129.778 L2084.82 160.402 L2101.65 160.402 L2101.65 164.338 L2080.15 164.338 L2080.15 129.778 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2115.59 141.398 Q2112.16 141.398 2110.17 144.083 Q2108.18 146.745 2108.18 151.398 Q2108.18 156.051 2110.15 158.736 Q2112.14 161.398 2115.59 161.398 Q2118.99 161.398 2120.98 158.713 Q2122.97 156.027 2122.97 151.398 Q2122.97 146.791 2120.98 144.106 Q2118.99 141.398 2115.59 141.398 M2115.59 137.787 Q2121.14 137.787 2124.31 141.398 Q2127.48 145.009 2127.48 151.398 Q2127.48 157.764 2124.31 161.398 Q2121.14 165.009 2115.59 165.009 Q2110.01 165.009 2106.84 161.398 Q2103.69 157.764 2103.69 151.398 Q2103.69 145.009 2106.84 141.398 Q2110.01 137.787 2115.59 137.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2151.07 139.176 L2151.07 143.203 Q2149.27 142.277 2147.32 141.815 Q2145.38 141.352 2143.29 141.352 Q2140.12 141.352 2138.53 142.324 Q2136.95 143.296 2136.95 145.24 Q2136.95 146.722 2138.09 147.578 Q2139.22 148.412 2142.65 149.176 L2144.11 149.5 Q2148.64 150.472 2150.54 152.254 Q2152.46 154.014 2152.46 157.185 Q2152.46 160.796 2149.59 162.902 Q2146.74 165.009 2141.74 165.009 Q2139.66 165.009 2137.39 164.592 Q2135.15 164.199 2132.65 163.388 L2132.65 158.99 Q2135.01 160.217 2137.3 160.842 Q2139.59 161.444 2141.84 161.444 Q2144.85 161.444 2146.47 160.426 Q2148.09 159.384 2148.09 157.509 Q2148.09 155.773 2146.91 154.847 Q2145.75 153.921 2141.79 153.064 L2140.31 152.717 Q2136.35 151.884 2134.59 150.171 Q2132.83 148.435 2132.83 145.426 Q2132.83 141.768 2135.42 139.778 Q2138.02 137.787 2142.79 137.787 Q2145.15 137.787 2147.23 138.134 Q2149.31 138.481 2151.07 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2175.77 139.176 L2175.77 143.203 Q2173.97 142.277 2172.02 141.815 Q2170.08 141.352 2167.99 141.352 Q2164.82 141.352 2163.23 142.324 Q2161.65 143.296 2161.65 145.24 Q2161.65 146.722 2162.79 147.578 Q2163.92 148.412 2167.35 149.176 L2168.8 149.5 Q2173.34 150.472 2175.24 152.254 Q2177.16 154.014 2177.16 157.185 Q2177.16 160.796 2174.29 162.902 Q2171.44 165.009 2166.44 165.009 Q2164.36 165.009 2162.09 164.592 Q2159.85 164.199 2157.35 163.388 L2157.35 158.99 Q2159.71 160.217 2162 160.842 Q2164.29 161.444 2166.54 161.444 Q2169.54 161.444 2171.17 160.426 Q2172.79 159.384 2172.79 157.509 Q2172.79 155.773 2171.6 154.847 Q2170.45 153.921 2166.49 153.064 L2165.01 152.717 Q2161.05 151.884 2159.29 150.171 Q2157.53 148.435 2157.53 145.426 Q2157.53 141.768 2160.12 139.778 Q2162.72 137.787 2167.48 137.787 Q2169.85 137.787 2171.93 138.134 Q2174.01 138.481 2175.77 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2206.12 150.31 L2206.12 152.393 L2186.54 152.393 Q2186.81 156.791 2189.17 159.106 Q2191.56 161.398 2195.79 161.398 Q2198.25 161.398 2200.54 160.796 Q2202.85 160.194 2205.12 158.99 L2205.12 163.018 Q2202.83 163.99 2200.42 164.5 Q2198.02 165.009 2195.54 165.009 Q2189.34 165.009 2185.7 161.398 Q2182.09 157.787 2182.09 151.629 Q2182.09 145.264 2185.52 141.537 Q2188.97 137.787 2194.8 137.787 Q2200.03 137.787 2203.06 141.166 Q2206.12 144.523 2206.12 150.31 M2201.86 149.06 Q2201.81 145.565 2199.89 143.481 Q2197.99 141.398 2194.85 141.398 Q2191.28 141.398 2189.13 143.412 Q2187 145.426 2186.67 149.083 L2201.86 149.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2229.64 139.176 L2229.64 143.203 Q2227.83 142.277 2225.89 141.815 Q2223.94 141.352 2221.86 141.352 Q2218.69 141.352 2217.09 142.324 Q2215.52 143.296 2215.52 145.24 Q2215.52 146.722 2216.65 147.578 Q2217.79 148.412 2221.21 149.176 L2222.67 149.5 Q2227.21 150.472 2229.1 152.254 Q2231.03 154.014 2231.03 157.185 Q2231.03 160.796 2228.16 162.902 Q2225.31 165.009 2220.31 165.009 Q2218.23 165.009 2215.96 164.592 Q2213.71 164.199 2211.21 163.388 L2211.21 158.99 Q2213.57 160.217 2215.86 160.842 Q2218.16 161.444 2220.4 161.444 Q2223.41 161.444 2225.03 160.426 Q2226.65 159.384 2226.65 157.509 Q2226.65 155.773 2225.47 154.847 Q2224.31 153.921 2220.35 153.064 L2218.87 152.717 Q2214.91 151.884 2213.16 150.171 Q2211.4 148.435 2211.4 145.426 Q2211.4 141.768 2213.99 139.778 Q2216.58 137.787 2221.35 137.787 Q2223.71 137.787 2225.79 138.134 Q2227.88 138.481 2229.64 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(vcat(losses[2:100]...), label=\"Losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#erst einmal so lassen, vielleicht einzelne Änderungen\n",
    "function translate(x)\n",
    "    ix = todevice(vocab(preprocess(x)))\n",
    "    seq = [startsym]\n",
    "\n",
    "    enc = encoder_forward(ix)\n",
    "\n",
    "    len = length(ix)\n",
    "    for i = 1:2len\n",
    "        trg = todevice(vocab(seq))\n",
    "        dec = decoder_forward(trg, enc)\n",
    "        #move back to gpu due to argmax wrong result on CuArrays\n",
    "        ntok = onecold(collect(dec), labels)\n",
    "        push!(seq, ntok[end])\n",
    "        ntok[end] == endsym && break\n",
    "    end\n",
    "  seq[2:end-1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"5\"\n",
       " \"5\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(map(string, [5,5,6,6,1,12,3,4,6]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
